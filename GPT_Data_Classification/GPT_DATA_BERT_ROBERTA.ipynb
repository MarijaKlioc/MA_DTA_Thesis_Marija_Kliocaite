{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"cQSb4y2-FUSP"},"outputs":[],"source":["%pip install datasets transformers --quiet 2> /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKPJ94b6ApvN"},"outputs":[],"source":["import sklearn\n","import pandas as pd\n","import numpy as np\n","import random\n","import torch\n","import json\n","\n","seed = 42\n","\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"markdown","metadata":{"id":"6Vzmlkc0eFrO"},"source":["Getting the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qlj0cokYFeEY","colab":{"base_uri":"https://localhost:8080/","height":322,"referenced_widgets":["6789a06dd4e6459c9dfd9d4f7348423f","5c5282bc687e4be2a698fd4b2d70fb6e","e2f181dd18824461bac289f1d810f926","457b9d05610d45f681054728123923b1","66b5f310dc4a480bb9fc4774ad4dfefd","b1a3b40b99b943e89c22a2c836ebf056","1603cdf5244f47ab9e468dcbbb2c0393","62bf1c9001634bb68604754954d32e77","3789bf4cf59c40fbb87297803425fc0e","1c39fa26e87b4fd3858c8145d65dd4f6","ceeb699bcf5744e7a95823da1ca719fc","380a88cb4d964393a57f2ec8ba576393","34030bfd299c43f7b0bafc166e4d22f3","45da0a23ecdd4393b5e026bcd5330ae5","4167af55dd6b4c97b8e7b253ee887d15","9472874bd55e4e618ee26e6ff83c2ff6","c3b7a923975c432cbd36637dc35e282c","1ce65a7af59d43ebb2b203b258ad1654","5b7a5e326d094d3e87d171c3c7a51488","3c3e7b15abc94a0da89b76ae376954fb","b73df2a3262e406580fce04dcc0de490","ed9e644b137c4b21a1ebd213e0953d30","47ce7f4bd4e0479a978fcf3d4c94dc8d","a6bb198802df4de7874faa9a5291449b","9933f2e4adad40719e4f68237edd109c","84df407f2364487ea5e45537c0bb775b","8e55f1b7a92e459e8cb5e5a9c10f44e7","62ceb250c08b47bebae6ad10865e88e1","cc75f116f0514ca0b9d4a1e9d47ae234","b8a43148bfaf479dae12f850c81ec685","a1a7efb909d1419496891f61eadf8df8","f271bfcbeacb434d995d7403ec6d3019","94780ae2789243ac80404eb103632dc5","257878e0e07b4e969078cb700e970e80","528c27c967de4e9fb39eb92cea986527","bb8763067394466c9aedb49f25182da7","6551eb2da31b4d04ae1d887b69455b84","85f0e54af57c4acaa1241b295da84a81","a25f3444232646ec98e0b2fe24ceb81a","5eb233454371405a96c03c41849cb176","5bac8e2da8094fdcadfcd1485776fdc2","129161b50b08484aa9c43ff32a49f047","7c857e88b5e14d719ac8f0ef3d661478","8651528f272445339114eacf67f8b289","0d24a6ed90e54af896595898fdad953e","674f011b83184f158d8f132267c44df4","1c730557132949b794684989d6e6856e","7cd755facbbf4734a282ae513d257ddc","9e290699f47444d5b20526243765d0d9","6cd1b59331c3427eb8cfa6d968f627c9","14c4794ee26748908471fed5d26a2f99","e9e24d21cc3146b4a0ac6ff1c58a2b10","07bdffb0a1d84dea921d645f25679716","7210a6d284b042e181a0da4eddea4a32","20b2366495624c0ca637c335ccbd2bdd","225d5e4fbd2142f8b349d5c651fd2b91","5d7ab9c72ba646519738c2c14ffef747","6e6bc71e12934345ad88c8a5764c39d4","1799de1201f54d588ad1a276de48f4d4","318769065b454c31ba3129ae8e4b80ff","d0113ffc04db44799e9e6616e3543d4c","8edd4fd579b54752a1dd63140677cca1","354ac2defe6b415199d862344b592d16","d53ecbeb63c94bc6bf17a61a703eae2d","c8a26ce96d6244049e3287a51d577f06","90b962b75cb04c7db31b5832b8b85538"]},"executionInfo":{"status":"ok","timestamp":1723202502359,"user_tz":-120,"elapsed":9657,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"289350d7-2216-4320-8f09-cfb2ba4a5ccc"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.29k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6789a06dd4e6459c9dfd9d4f7348423f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"380a88cb4d964393a57f2ec8ba576393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/5.98M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ce7f4bd4e0479a978fcf3d4c94dc8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/6838 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"257878e0e07b4e969078cb700e970e80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/3259 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d24a6ed90e54af896595898fdad953e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/886 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"225d5e4fbd2142f8b349d5c651fd2b91"}},"metadata":{}}],"source":["from datasets import load_dataset\n","sem_eval_2018_task_1 = load_dataset('sem_eval_2018_task_1', 'subtask5.english', trust_remote_code=True)"]},{"cell_type":"markdown","metadata":{"id":"iwaGC6VzgDnj"},"source":["#BERT"]},{"cell_type":"markdown","metadata":{"id":"WBI7zhY0b_32"},"source":["BERT + 1/2 gold standard data + 1/2 synthetic data"]},{"cell_type":"markdown","source":["NO ROLE"],"metadata":{"id":"OEpDJO9QNWsN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5n_OKvMVcEYn","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7c6598125fd14388a80a2fc91c591a01","9825fa236f6a497d8011629111f44e37","c81fab7945a24f76b8223aed7d104742","c16be8df05854bd790249a2b9234c70c","a68e934fb7c6417ea5a45ac4fd0ff761","b2e20208d0d64690b5c2a710c1478ba7","273966be0c9b44809e16544b8cc1fc48","76e7700fcbab46c4ac54371709f5faf9","6d89aacea0ab4f63876f6772b61038d2","db533772ff1f4180b8864c615b00f7f3","61cf82ea86c14a73bac7a104c074dc81","7dae49384134478bb2d0f40b0190f69f","50e99765f46d4fd3bb073eb46c41502c","7ce2c48ab3cb4515b577d8c17b3f5483","a025e5dbe52a4498b79e4cc5fdb08d8b","7dd3ee3f4d514971b391ad9792d89dc0","1134dafebdfc47bda1f24572dd878a22","55ea6c6793bf45c69c62c7301d66195c","10d95c2f353144e4a91b1605861b5676","fd3d8a6859a049a3a5d4bf2042ee163a","fcd25c6f756e4f54a29a3c98bdd4eccd","77853a9cc6c4439fbad36ebc92e7afb8","bddaa0b2c826427babd8438bad2e582a","29b9fb12723d4bb683d2fe0b146b3913","5d828a52833d4586bcf07b6b9d7d80bf","44d290b2103f4f3db53bc69f4e14f75f","733f0ec3e1a84b8aaae7f0b1bd47ebe6","a26ec8fd9095491da765253d3093519a","0a564acd7a2548499ca7e0b090b8f5b6","2159d1fbbeac4ba7bd9ebba93cb1588e","706add22b0054aaa9b59dda6d423d808","3a776f5036214fcaa413122ffa66ead3","1840f6d195dc4f63b4c58a9c8ef71ccd","b81faa97a37d4354a6ec304785a76322","4f40a08c26284ce09b03b6940bb408f2","3b0b9679c0cf47249e6d1e4fa09052b7","d5f5d1af7f8f4bcc9ceb45bc622895af","fd39058fbc5f4a23bce2a888e0e83eca","54f311f67b164751a48e981f3995bb5d","b3e51991f0b943a0bc654a00a3d74db9","487fa7f735894dad8106cdd24fb00732","826e70b90cc845efae148230bc23d856","1ee1f7554a2745388990837fa576d1fe","7400cfa4235442b48952aa3a782f0391","ff3b105270fd4f50b4e034fd6a5b5014","57472a9678414521950e09bffbd39cb7","040a58a4ee7345a2ae2fa938595dc272","d6d5cd817f2b4e22ab54ebd5622b2c93","1ab851f2a04649f4b3932c0845eb7acd","c7d214a47de24c9dad0bfe45eb685717","640384c0ef694c92822f1bff96d15559","92abc84017284791b00d67ff66b0e54d","b98897ee698648bb9ff1f103ce07d7c2","c1e6087ab11943b88761b0743fbecbd2","e5918de91219450aae4f9c819ea047a1"]},"executionInfo":{"status":"ok","timestamp":1723023875889,"user_tz":-120,"elapsed":311019,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"78d5a1c7-d62b-4910-b00d-9feffc9b8af6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c6598125fd14388a80a2fc91c591a01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dae49384134478bb2d0f40b0190f69f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bddaa0b2c826427babd8438bad2e582a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b81faa97a37d4354a6ec304785a76322"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff3b105270fd4f50b4e034fd6a5b5014"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5175, Val Loss: 0.4056\n","Epoch 2, Train Loss: 0.3767, Val Loss: 0.3616\n","Epoch 3, Train Loss: 0.3240, Val Loss: 0.3456\n","Epoch 4, Train Loss: 0.2891, Val Loss: 0.3347\n","Epoch 5, Train Loss: 0.2638, Val Loss: 0.3302\n","Test Accuracy: 26.08\n","Test F1-macro: 49.12\n","Test F1-micro: 67.06\n","Confusion Matrix:\n","Label: anticipation\n","[[2769   65]\n"," [ 398   27]]\n","\n","Label: optimism\n","[[1752  364]\n"," [ 271  872]]\n","\n","Label: trust\n","[[3019   87]\n"," [ 143   10]]\n","\n","Label: joy\n","[[1572  245]\n"," [ 269 1173]]\n","\n","Label: love\n","[[2606  137]\n"," [ 285  231]]\n","\n","Label: anger\n","[[1950  208]\n"," [ 276  825]]\n","\n","Label: disgust\n","[[1899  261]\n"," [ 281  818]]\n","\n","Label: pessimism\n","[[2757  127]\n"," [ 266  109]]\n","\n","Label: sadness\n","[[2174  125]\n"," [ 460  500]]\n","\n","Label: fear\n","[[2723   51]\n"," [ 238  247]]\n","\n","Label: surprise\n","[[3088    1]\n"," [ 170    0]]\n","\n"]}],"source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"half_gsd_half_gpt_data.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"]},{"cell_type":"markdown","source":["NO ROLE + NEWS ARTICLES"],"metadata":{"id":"IaeKR9t-NYt1"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"half_gsd_half_gpt_data_no_role_news.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8yO8c3xNIte","executionInfo":{"status":"ok","timestamp":1723024227543,"user_tz":-120,"elapsed":292133,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"52038cfb-5436-445d-a662-247521aa6465"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5420, Val Loss: 0.4218\n","Epoch 2, Train Loss: 0.4236, Val Loss: 0.3728\n","Epoch 3, Train Loss: 0.3648, Val Loss: 0.3560\n","Epoch 4, Train Loss: 0.3241, Val Loss: 0.3351\n","Epoch 5, Train Loss: 0.2938, Val Loss: 0.3263\n","Test Accuracy: 25.71\n","Test F1-macro: 48.50\n","Test F1-micro: 67.39\n","Confusion Matrix:\n","Label: anticipation\n","[[2785   49]\n"," [ 402   23]]\n","\n","Label: optimism\n","[[1764  352]\n"," [ 281  862]]\n","\n","Label: trust\n","[[3079   27]\n"," [ 148    5]]\n","\n","Label: joy\n","[[1599  218]\n"," [ 258 1184]]\n","\n","Label: love\n","[[2655   88]\n"," [ 324  192]]\n","\n","Label: anger\n","[[1932  226]\n"," [ 269  832]]\n","\n","Label: disgust\n","[[1893  267]\n"," [ 284  815]]\n","\n","Label: pessimism\n","[[2767  117]\n"," [ 289   86]]\n","\n","Label: sadness\n","[[2117  182]\n"," [ 417  543]]\n","\n","Label: fear\n","[[2714   60]\n"," [ 224  261]]\n","\n","Label: surprise\n","[[3087    2]\n"," [ 167    3]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 1"],"metadata":{"id":"Gbnykt87Nb1R"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"half_gsd_half_gpt_data_role_1.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7d4b9bc822ee411cbd794fa36e03e979","fef8c2f11fb446c69fa9f10a5f92f55a","b548199952ef4b9fbd5c095002ebe0a2","3c9a03d0c62548e2ba38aff3d46ef140","4e15964de7424e9780e02ae93a505c6e","dfa313fcfd324d52a0bda46013cd744c","eb9272b241b14da4be2d03d0323fae85","650d4c325db44b9e8af23de1eb9810c9","6bb75129aa4145b08e7b94750cb9e782","f0990d9dea484886b9a1031f81dd67df","975bb59131284e189ad05073a226db1d","cf5f9fd288b4438787672ae7df8e33b1","b3f1e6de7cb9494ea88185e81b5a51ee","08006ce0b7814784bbb8c0c4eae7531f","724549cd5c5c4faebb660ca006a09c98","1f761e94fcc4475f8a6a6a4065fd263a","5f7755a60f3b4c2d83c0d89e1176a354","ab292304ee3049568721edf6a6edefe1","c77479897a5d47db86c66ce6d0831d0e","361198cb08ef44b780ec941e262b398f","99b61d0a2b604176a301cbd0fe347ba0","c2a23f833f9a43fa9fb292ce848ac016","c8594c4299f04bf89b9bffab3af5e429","f3d286db8a344ef495464eca438eb44a","05434ba8cbfa4eef90e46375224d1fbc","3070c249ae7a4147bdc934c2ec3183ee","19c98d8090994e69abab9a3a5dcd1db2","b3ac59e4ff3847ee8ff746f621a9c0bd","5a1967a84a8643c8ba1d6d89cce5e0f4","cf0612b4359c4e16bb602ed887c40833","2a7b3defe317404391b58d8ea2e71646","df610d10c2fa450c96135ae91a0d1aaf","7f1e7fa3ee55454db0a3321b3536fc23","02c76b299656463581d31a870277b7f0","3c7bd8707fe34551bd343f9ab2ca49c7","d9acdb86793346e78e69c60557b046b8","dd536fa0f03542b6895facd9b3615509","1ab523b261ca4e18ac79015ad799dfe2","caa616778f4a4dc38d444b4e56594b7d","8bd760aab04747f5af84d7e84633c3d5","aed91a97c61546d68ec604180f531cbf","2a416d504d3c4b27aad80ba25d155d0b","290a62c13dd947288639119ffe6ca208","0c69d3ce08f14d218f3d34fbd9c38a51","4159b3accc9b46cfb13f4ff620ccfc43","a073eb19b3be481f93e0859d8a1c626e","3574a09805b84ce0b5b2b05f7a0f7bde","75969a9b8f164646a3b3305b35f3b248","e2942467348a486d9c1e883b6f299db9","5c94168c6cc14b6bbc929c887641c417","dc6279d4874249709bc81337e1210e8f","869e8cec7ec64613b1fa4c21eec26e7e","ce7edf0c3df24b3b96b2beebf38ed44e","288f9f6ccda24c5fbf36432f4c0f5ea3","ca0f9d92c9aa40c99efee9c34cc8c448"]},"id":"L-dl6i8ONhWe","executionInfo":{"status":"ok","timestamp":1723025655617,"user_tz":-120,"elapsed":315570,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"f4e27896-c41b-4a91-e9d0-6d7446448f4e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d4b9bc822ee411cbd794fa36e03e979"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf5f9fd288b4438787672ae7df8e33b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8594c4299f04bf89b9bffab3af5e429"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c76b299656463581d31a870277b7f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4159b3accc9b46cfb13f4ff620ccfc43"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5240, Val Loss: 0.4115\n","Epoch 2, Train Loss: 0.3870, Val Loss: 0.3671\n","Epoch 3, Train Loss: 0.3314, Val Loss: 0.3440\n","Epoch 4, Train Loss: 0.2970, Val Loss: 0.3279\n","Epoch 5, Train Loss: 0.2680, Val Loss: 0.3223\n","Test Accuracy: 25.31\n","Test F1-macro: 46.32\n","Test F1-micro: 65.67\n","Confusion Matrix:\n","Label: anticipation\n","[[2782   52]\n"," [ 406   19]]\n","\n","Label: optimism\n","[[1825  291]\n"," [ 338  805]]\n","\n","Label: trust\n","[[3077   29]\n"," [ 151    2]]\n","\n","Label: joy\n","[[1617  200]\n"," [ 308 1134]]\n","\n","Label: love\n","[[2698   45]\n"," [ 396  120]]\n","\n","Label: anger\n","[[1960  198]\n"," [ 308  793]]\n","\n","Label: disgust\n","[[1904  256]\n"," [ 314  785]]\n","\n","Label: pessimism\n","[[2773  111]\n"," [ 278   97]]\n","\n","Label: sadness\n","[[2159  140]\n"," [ 461  499]]\n","\n","Label: fear\n","[[2702   72]\n"," [ 211  274]]\n","\n","Label: surprise\n","[[3089    0]\n"," [ 170    0]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 2"],"metadata":{"id":"M4pOBERjNcuJ"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"half_gsd_half_gpt_data_role_2.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"byjX6-mCNmhO","executionInfo":{"status":"ok","timestamp":1723025968867,"user_tz":-120,"elapsed":294755,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"fe193d5c-fd5e-4cbd-caa7-ecef9ac3ce51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5280, Val Loss: 0.4252\n","Epoch 2, Train Loss: 0.4012, Val Loss: 0.3716\n","Epoch 3, Train Loss: 0.3443, Val Loss: 0.3496\n","Epoch 4, Train Loss: 0.3058, Val Loss: 0.3351\n","Epoch 5, Train Loss: 0.2769, Val Loss: 0.3246\n","Test Accuracy: 26.27\n","Test F1-macro: 46.64\n","Test F1-micro: 66.44\n","Confusion Matrix:\n","Label: anticipation\n","[[2816   18]\n"," [ 418    7]]\n","\n","Label: optimism\n","[[1814  302]\n"," [ 358  785]]\n","\n","Label: trust\n","[[3059   47]\n"," [ 151    2]]\n","\n","Label: joy\n","[[1615  202]\n"," [ 314 1128]]\n","\n","Label: love\n","[[2673   70]\n"," [ 369  147]]\n","\n","Label: anger\n","[[1901  257]\n"," [ 226  875]]\n","\n","Label: disgust\n","[[1893  267]\n"," [ 288  811]]\n","\n","Label: pessimism\n","[[2791   93]\n"," [ 284   91]]\n","\n","Label: sadness\n","[[2137  162]\n"," [ 424  536]]\n","\n","Label: fear\n","[[2730   44]\n"," [ 225  260]]\n","\n","Label: surprise\n","[[3089    0]\n"," [ 170    0]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 3"],"metadata":{"id":"eg2ayUuMNdwx"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"half_gsd_half_gpt_data_role_3.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PH9C8yt1NoRW","executionInfo":{"status":"ok","timestamp":1723026279796,"user_tz":-120,"elapsed":301810,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"f5229dd5-d7b8-41f3-dd01-305d86f2ab80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5247, Val Loss: 0.4101\n","Epoch 2, Train Loss: 0.3792, Val Loss: 0.3605\n","Epoch 3, Train Loss: 0.3254, Val Loss: 0.3397\n","Epoch 4, Train Loss: 0.2897, Val Loss: 0.3356\n","Epoch 5, Train Loss: 0.2630, Val Loss: 0.3269\n","Test Accuracy: 25.68\n","Test F1-macro: 47.64\n","Test F1-micro: 66.53\n","Confusion Matrix:\n","Label: anticipation\n","[[2763   71]\n"," [ 404   21]]\n","\n","Label: optimism\n","[[1838  278]\n"," [ 355  788]]\n","\n","Label: trust\n","[[3046   60]\n"," [ 151    2]]\n","\n","Label: joy\n","[[1631  186]\n"," [ 324 1118]]\n","\n","Label: love\n","[[2645   98]\n"," [ 341  175]]\n","\n","Label: anger\n","[[1892  266]\n"," [ 226  875]]\n","\n","Label: disgust\n","[[1803  357]\n"," [ 232  867]]\n","\n","Label: pessimism\n","[[2767  117]\n"," [ 281   94]]\n","\n","Label: sadness\n","[[2099  200]\n"," [ 416  544]]\n","\n","Label: fear\n","[[2717   57]\n"," [ 204  281]]\n","\n","Label: surprise\n","[[3088    1]\n"," [ 170    0]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 4"],"metadata":{"id":"7q2ZI2y3NeZd"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"half_gsd_half_gpt_data_role_4.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7Nia-yWNpy1","executionInfo":{"status":"ok","timestamp":1723026588324,"user_tz":-120,"elapsed":301046,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"48d3d60d-ae88-488a-f8c2-be22342fbe5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5405, Val Loss: 0.4529\n","Epoch 2, Train Loss: 0.4173, Val Loss: 0.3658\n","Epoch 3, Train Loss: 0.3520, Val Loss: 0.3400\n","Epoch 4, Train Loss: 0.3127, Val Loss: 0.3295\n","Epoch 5, Train Loss: 0.2810, Val Loss: 0.3238\n","Test Accuracy: 26.51\n","Test F1-macro: 48.18\n","Test F1-micro: 67.47\n","Confusion Matrix:\n","Label: anticipation\n","[[2807   27]\n"," [ 402   23]]\n","\n","Label: optimism\n","[[1776  340]\n"," [ 309  834]]\n","\n","Label: trust\n","[[3080   26]\n"," [ 144    9]]\n","\n","Label: joy\n","[[1607  210]\n"," [ 265 1177]]\n","\n","Label: love\n","[[2686   57]\n"," [ 340  176]]\n","\n","Label: anger\n","[[1905  253]\n"," [ 244  857]]\n","\n","Label: disgust\n","[[1859  301]\n"," [ 261  838]]\n","\n","Label: pessimism\n","[[2772  112]\n"," [ 296   79]]\n","\n","Label: sadness\n","[[2047  252]\n"," [ 357  603]]\n","\n","Label: fear\n","[[2740   34]\n"," [ 253  232]]\n","\n","Label: surprise\n","[[3087    2]\n"," [ 170    0]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 1 + NEWS ARTICLES"],"metadata":{"id":"bCajXETuNfcl"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"half_gsd_half_gpt_data_role_5.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65XO5JW_NrYI","executionInfo":{"status":"ok","timestamp":1723026940783,"user_tz":-120,"elapsed":297272,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"fe3571b5-f5d3-4279-ab67-2476e74033ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5498, Val Loss: 0.4200\n","Epoch 2, Train Loss: 0.4120, Val Loss: 0.3549\n","Epoch 3, Train Loss: 0.3482, Val Loss: 0.3327\n","Epoch 4, Train Loss: 0.3107, Val Loss: 0.3297\n","Epoch 5, Train Loss: 0.2800, Val Loss: 0.3236\n","Test Accuracy: 25.77\n","Test F1-macro: 49.61\n","Test F1-micro: 67.13\n","Confusion Matrix:\n","Label: anticipation\n","[[2724  110]\n"," [ 381   44]]\n","\n","Label: optimism\n","[[1834  282]\n"," [ 360  783]]\n","\n","Label: trust\n","[[3090   16]\n"," [ 149    4]]\n","\n","Label: joy\n","[[1655  162]\n"," [ 322 1120]]\n","\n","Label: love\n","[[2642  101]\n"," [ 317  199]]\n","\n","Label: anger\n","[[1923  235]\n"," [ 252  849]]\n","\n","Label: disgust\n","[[1863  297]\n"," [ 276  823]]\n","\n","Label: pessimism\n","[[2754  130]\n"," [ 296   79]]\n","\n","Label: sadness\n","[[2077  222]\n"," [ 350  610]]\n","\n","Label: fear\n","[[2705   69]\n"," [ 203  282]]\n","\n","Label: surprise\n","[[3077   12]\n"," [ 161    9]]\n","\n"]}]},{"cell_type":"markdown","source":["##Single Label BERT"],"metadata":{"id":"EfZ_-9d5gMGi"}},{"cell_type":"markdown","source":["No Role"],"metadata":{"id":"rgHr1mTLgt8H"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"gsd_gpt_single_label_no_role.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6866b4ce70134efabfbc82230f66916b","f0a0128845824466b38f7e67ade1e0fc","b05cd68ad8ce4579af42e05e9b246120","0fc6c4a041634f9785d1dcf72b8e34ac","a3ab35ed293e4cf0b35e8b642f61599e","6add07bdf462417297d5afd202c93d6c","cf7d243972594323ac7c0b9f0ca59383","3954f9f8514d4f289701f596348d50c0","214f97275df84d2dae1245c2bdab1db2","98cff2c6201c4b3f85bdf3769a769485","2fd312b2e53e45efbaad06e0ea3df351","359eb80da67e470690aa89f1387c9fe2","abaa155c8b69409398af221172708264","a060d198587a42bcb82311e1fc8a7f72","7103bb87a939402ab10b9cd471cf11d4","b9684873fa814b259fcc45d7ac6e513c","93c37e6525e24b099987114fb63f590f","9fd42fdf6c72408fadd58e1b3d4aa9d0","9bb4c6e4f9354b5389df4a5925e5f71b","260208633ba8402cbb507202240bcd5f","029d74814ed445a3a1a631edf65221ca","c8a50b03eb814230b236c72b9fda9fff","81d82c3406b84cc884ef754d0303ca03","633bb44c2c6e4a3db16146654f330629","19c815c4eba041ea9e89e48d9f87fb16","451b7bde5f504589bb565fcbf38088ab","d9fc32b9aacb466c9265a938b28353ca","c787a747c6994d768e70b46f13ad62e1","2f8d15da078f48be9bf2816254f4ae54","ce1644c0309946c5aa5b416c71e51e53","b2419fb398db45b284722821170a088c","923ef2e8a978461f90474ebd90edc9e3","a060fd95a786479db811e2d334ad14e0","07f833e296d44a2b99d114f5d6414501","7e97fc197ca44e4fa19733d40a5c151c","42b17db690774f549b38cc5c15f4ea71","4e6891157b344fc6b4bb6fdc8c4ada10","ab3cda59859043918a273405acfa00f0","4faec5f1f0354cf99f4b5b6fc75b916c","841934a303094c9eb8af6abed006ebf4","104d696b322e40d5b5c607a5ceaa43b6","a73eaca9a05241419b565db0a13af251","3689d76da3f54fd5a8c4f6ff7646d139","63142bd5c4764761b13faa1535d24af1","00c12cbfe6624be19ce9ca5f496408ff","647b6be741e64bc4b11db3d7792e8f4e","30ffdb2f52914d15ae57a1ec043ae2d3","089acced28654f75ad82edfc31cf5c11","f5bfb25b2b3340a29f1ab4474dfef345","769b8dbd8aa540efaae877237f751784","fff94b5e566346b189805b7c69aceb39","9794f36184ed426691d1217e544c0332","b196a725cca84208818ebcbd0724a313","149260945f084d2899fa3edbd22bc159","b80a43a71b774c1ba22da762647c119f"]},"id":"NclUbLzJgOG7","executionInfo":{"status":"ok","timestamp":1723129564868,"user_tz":-120,"elapsed":338064,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"98f55f01-24d9-4fba-dca6-2a5c4c38b8dd"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6866b4ce70134efabfbc82230f66916b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"359eb80da67e470690aa89f1387c9fe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81d82c3406b84cc884ef754d0303ca03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07f833e296d44a2b99d114f5d6414501"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00c12cbfe6624be19ce9ca5f496408ff"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4928, Val Loss: 0.4284\n","Epoch 2, Train Loss: 0.3760, Val Loss: 0.3675\n","Epoch 3, Train Loss: 0.3219, Val Loss: 0.3427\n","Epoch 4, Train Loss: 0.2812, Val Loss: 0.3275\n","Epoch 5, Train Loss: 0.2520, Val Loss: 0.3201\n","Test Accuracy: 26.08\n","Test F1-macro: 44.53\n","Test F1-micro: 66.65\n","Confusion Matrix:\n","Label: anticipation\n","[[2826    8]\n"," [ 413   12]]\n","\n","Label: optimism\n","[[1794  322]\n"," [ 348  795]]\n","\n","Label: trust\n","[[3106    0]\n"," [ 153    0]]\n","\n","Label: joy\n","[[1641  176]\n"," [ 316 1126]]\n","\n","Label: love\n","[[2685   58]\n"," [ 371  145]]\n","\n","Label: anger\n","[[1927  231]\n"," [ 252  849]]\n","\n","Label: disgust\n","[[1863  297]\n"," [ 255  844]]\n","\n","Label: pessimism\n","[[2870   14]\n"," [ 360   15]]\n","\n","Label: sadness\n","[[2161  138]\n"," [ 429  531]]\n","\n","Label: fear\n","[[2731   43]\n"," [ 226  259]]\n","\n","Label: surprise\n","[[3089    0]\n"," [ 170    0]]\n","\n"]}]},{"cell_type":"markdown","source":["Role 1"],"metadata":{"id":"cCY56ifdgwCE"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"gsd_gpt_single_label_role_1.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J0q03iKfgZ_i","executionInfo":{"status":"ok","timestamp":1723129891112,"user_tz":-120,"elapsed":322570,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"d6511a40-129e-4d2f-9037-18c984b7fef6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4911, Val Loss: 0.4347\n","Epoch 2, Train Loss: 0.3864, Val Loss: 0.3759\n","Epoch 3, Train Loss: 0.3308, Val Loss: 0.3480\n","Epoch 4, Train Loss: 0.2884, Val Loss: 0.3310\n","Epoch 5, Train Loss: 0.2572, Val Loss: 0.3221\n","Test Accuracy: 26.76\n","Test F1-macro: 45.68\n","Test F1-micro: 66.28\n","Confusion Matrix:\n","Label: anticipation\n","[[2834    0]\n"," [ 425    0]]\n","\n","Label: optimism\n","[[1853  263]\n"," [ 416  727]]\n","\n","Label: trust\n","[[3106    0]\n"," [ 153    0]]\n","\n","Label: joy\n","[[1644  173]\n"," [ 311 1131]]\n","\n","Label: love\n","[[2712   31]\n"," [ 382  134]]\n","\n","Label: anger\n","[[1969  189]\n"," [ 296  805]]\n","\n","Label: disgust\n","[[1907  253]\n"," [ 299  800]]\n","\n","Label: pessimism\n","[[2823   61]\n"," [ 305   70]]\n","\n","Label: sadness\n","[[2165  134]\n"," [ 437  523]]\n","\n","Label: fear\n","[[2718   56]\n"," [ 200  285]]\n","\n","Label: surprise\n","[[3089    0]\n"," [ 170    0]]\n","\n"]}]},{"cell_type":"markdown","source":["Role 1 + NEWS"],"metadata":{"id":"r4iD4Dvqgxy-"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"gsd_gpt_single_label_role_1_news.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P62T2cE6grJW","executionInfo":{"status":"ok","timestamp":1723130218824,"user_tz":-120,"elapsed":321376,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"1b6d7df3-bf9e-42d3-9640-85efa4b4e81d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5008, Val Loss: 0.4702\n","Epoch 2, Train Loss: 0.3997, Val Loss: 0.3741\n","Epoch 3, Train Loss: 0.3355, Val Loss: 0.3418\n","Epoch 4, Train Loss: 0.2946, Val Loss: 0.3242\n","Epoch 5, Train Loss: 0.2646, Val Loss: 0.3197\n","Test Accuracy: 26.48\n","Test F1-macro: 44.55\n","Test F1-micro: 65.60\n","Confusion Matrix:\n","Label: anticipation\n","[[2834    0]\n"," [ 424    1]]\n","\n","Label: optimism\n","[[1835  281]\n"," [ 376  767]]\n","\n","Label: trust\n","[[3106    0]\n"," [ 153    0]]\n","\n","Label: joy\n","[[1641  176]\n"," [ 312 1130]]\n","\n","Label: love\n","[[2701   42]\n"," [ 381  135]]\n","\n","Label: anger\n","[[1989  169]\n"," [ 333  768]]\n","\n","Label: disgust\n","[[1961  199]\n"," [ 378  721]]\n","\n","Label: pessimism\n","[[2840   44]\n"," [ 335   40]]\n","\n","Label: sadness\n","[[2167  132]\n"," [ 428  532]]\n","\n","Label: fear\n","[[2714   60]\n"," [ 200  285]]\n","\n","Label: surprise\n","[[3089    0]\n"," [ 170    0]]\n","\n"]}]},{"cell_type":"markdown","source":["Increased multi-label"],"metadata":{"id":"rntZD4p43enT"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"gsd_gpt_increased_multi.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7fe2e23aca7f4244a188a7fded063157","64d7d75a8f084a0da52c65cd8020acdf","084a891323a140d9bc357e739c2748e1","ad155b6ec9e84265ac96e200a1568b37","cfc3a9db4924456591ceae0708c85af4","15af3c6c62594fe7b42270e257d1401c","a80c37d6a97a489bb52d1d021b0ad702","1ac6bd387927483ca93c8d5d9a1f2721","9d6cb909d73747018bd58dbbb2e7e9d6","1bea77c5910c44fe85cb3681e97566aa","8e06aee840cb4213bdafd2c52dff50a6","1aec21b6434b4f53808ffc79f5149d85","f757c6cea36d45958a337d027a805e37","0da92ab01daa46ef950a4601dfcc2895","845c05041d264ca28a00517a1c5b11a7","70a16c9055ea4ee097093aab4dd45bc8","655b3ac7982146e7aee73b6cb280264a","e083ca4c1c99478f85b0eb41a3e6470d","374b53c7f7414a09bf1ca0e5ebb0fc81","5bcc2782dccf43a5ad65a068575d08cc","cfe8758be1ad498489da7c905da2bfb8","ac8745f811f54cc9adbca6079c889961","b22b945f90ba45229af24f8e63460c7f","6b84bd1844094118b142ab5e93a905a6","f78780bace364622aa14826e2be40f45","ef8b297c4dd04062aef1590c0c58b881","4dd79f81753545efa036039b41cdb4b7","0dadf70b10804b15b620204ddb7b3b81","31e1c6e478db439caea9cfa984d781e9","ef14345728044f71a34d798ef3be962d","e6df23e4f031491b97c932e37743f969","7cc4fa7a7ae04fc8bf65e0caf5d402d6","9a08886fbad24357a7bd8cd0e9e0b0f0","b59d09ea68424bdba82cb07a87fb7e43","b81077db5ba543eab0a4f36f8fe5ae15","8bf58d66300b4aa0b84eeca1fba14025","979bfbd608c84d5788df47de736f225f","c1f3dc716f2346de99d7cc84a6eb4035","e067649cd933415aa2f7c613bb669166","1f88e602902c4537adfb3cc5e5894090","2e7281e294d4453b98f838e54e241658","f842fff323e6405c8f145341670f1007","e99222c5a9214cb69bc91a0d15911e3b","c6d1705c41f84dedbd8993564e1cfc8a","11537150b6fb4d6383ea66da5f585a01","3a650064d11142e0a0272445ad3edaf1","867d391a45514c4abdd2a22506c9e1e1","ae1b5ad1641848a4829a45593dff2a77","7c5d513e9d7b4d60b0bf4723d3277033","a97ca5e1e222415eaaa5a0c84085617b","f130cad6afeb452c98dde002615df790","7ca1c819a9684ee88835ff85f4dccf63","bdfb27a3cd1846b0a9cea66662b3f3a9","e6a0613d4fb041e4a706837d90e16a13","13eeba59a4ed45fbb1120b58cfc62498"]},"id":"MjKXgazd3eCH","executionInfo":{"status":"ok","timestamp":1723202891698,"user_tz":-120,"elapsed":379870,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"526ea6a4-f42c-489d-a61d-5d3e3651cbfa"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fe2e23aca7f4244a188a7fded063157"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aec21b6434b4f53808ffc79f5149d85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b22b945f90ba45229af24f8e63460c7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b59d09ea68424bdba82cb07a87fb7e43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11537150b6fb4d6383ea66da5f585a01"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4966, Val Loss: 0.3981\n","Epoch 2, Train Loss: 0.3413, Val Loss: 0.3607\n","Epoch 3, Train Loss: 0.2895, Val Loss: 0.3423\n","Epoch 4, Train Loss: 0.2553, Val Loss: 0.3392\n","Epoch 5, Train Loss: 0.2298, Val Loss: 0.3254\n","Test Accuracy: 25.71\n","Test F1-macro: 49.70\n","Test F1-micro: 67.47\n","Confusion Matrix:\n","Label: anticipation\n","[[2773   61]\n"," [ 394   31]]\n","\n","Label: optimism\n","[[1777  339]\n"," [ 321  822]]\n","\n","Label: trust\n","[[3037   69]\n"," [ 148    5]]\n","\n","Label: joy\n","[[1596  221]\n"," [ 281 1161]]\n","\n","Label: love\n","[[2600  143]\n"," [ 251  265]]\n","\n","Label: anger\n","[[1921  237]\n"," [ 258  843]]\n","\n","Label: disgust\n","[[1898  262]\n"," [ 298  801]]\n","\n","Label: pessimism\n","[[2771  113]\n"," [ 274  101]]\n","\n","Label: sadness\n","[[2071  228]\n"," [ 369  591]]\n","\n","Label: fear\n","[[2697   77]\n"," [ 208  277]]\n","\n","Label: surprise\n","[[3088    1]\n"," [ 169    1]]\n","\n"]}]},{"cell_type":"markdown","source":["Increased single-label"],"metadata":{"id":"5q9MF0VR3g8H"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"gsd_gpt_increased_single.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-yTNsY0B37VW","executionInfo":{"status":"ok","timestamp":1723203255361,"user_tz":-120,"elapsed":357285,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"ef872210-a3f6-4b25-bd3e-0399e00eee16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4579, Val Loss: 0.4300\n","Epoch 2, Train Loss: 0.3426, Val Loss: 0.3672\n","Epoch 3, Train Loss: 0.2774, Val Loss: 0.3353\n","Epoch 4, Train Loss: 0.2332, Val Loss: 0.3234\n","Epoch 5, Train Loss: 0.2023, Val Loss: 0.3188\n","Test Accuracy: 25.35\n","Test F1-macro: 45.81\n","Test F1-micro: 66.13\n","Confusion Matrix:\n","Label: anticipation\n","[[2822   12]\n"," [ 411   14]]\n","\n","Label: optimism\n","[[1873  243]\n"," [ 463  680]]\n","\n","Label: trust\n","[[3103    3]\n"," [ 153    0]]\n","\n","Label: joy\n","[[1627  190]\n"," [ 274 1168]]\n","\n","Label: love\n","[[2664   79]\n"," [ 342  174]]\n","\n","Label: anger\n","[[1961  197]\n"," [ 291  810]]\n","\n","Label: disgust\n","[[1918  242]\n"," [ 322  777]]\n","\n","Label: pessimism\n","[[2866   18]\n"," [ 354   21]]\n","\n","Label: sadness\n","[[2100  199]\n"," [ 380  580]]\n","\n","Label: fear\n","[[2733   41]\n"," [ 225  260]]\n","\n","Label: surprise\n","[[3087    2]\n"," [ 161    9]]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"G3h9MFtfcIkk"},"source":["#RoBERTa"]},{"cell_type":"markdown","source":["RoBERTa + half GSD + half synthetic data"],"metadata":{"id":"2Dw5SrWOvkdX"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"half_gsd_half_gpt_data.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F5rrB_LEvky0","executionInfo":{"status":"ok","timestamp":1723104001931,"user_tz":-120,"elapsed":635742,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"db5fdcb0-f117-4a12-a77b-c9df6f9d9117"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.6093, Val Loss: 0.4846\n","Epoch 2, Train Loss: 0.4915, Val Loss: 0.4096\n","Epoch 3, Train Loss: 0.4102, Val Loss: 0.3771\n","Epoch 4, Train Loss: 0.3706, Val Loss: 0.3681\n","Epoch 5, Train Loss: 0.3474, Val Loss: 0.3607\n","Epoch 6, Train Loss: 0.3309, Val Loss: 0.3617\n","Epoch 7, Train Loss: 0.3170, Val Loss: 0.3499\n","Epoch 8, Train Loss: 0.3033, Val Loss: 0.3419\n","Epoch 9, Train Loss: 0.2929, Val Loss: 0.3419\n","Epoch 10, Train Loss: 0.2840, Val Loss: 0.3358\n","Test Accuracy: 22.31\n","Test F1-macro: 51.49\n","Test F1-micro: 66.42\n","Confusion Matrix:\n","Label: anticipation\n","[[2598  236]\n"," [ 343   82]]\n","\n","Label: optimism\n","[[1665  451]\n"," [ 193  950]]\n","\n","Label: trust\n","[[2921  185]\n"," [ 130   23]]\n","\n","Label: joy\n","[[1561  256]\n"," [ 212 1230]]\n","\n","Label: love\n","[[2564  179]\n"," [ 235  281]]\n","\n","Label: anger\n","[[2012  146]\n"," [ 356  745]]\n","\n","Label: disgust\n","[[1975  185]\n"," [ 364  735]]\n","\n","Label: pessimism\n","[[2680  204]\n"," [ 246  129]]\n","\n","Label: sadness\n","[[2097  202]\n"," [ 407  553]]\n","\n","Label: fear\n","[[2642  132]\n"," [ 218  267]]\n","\n","Label: surprise\n","[[3076   13]\n"," [ 164    6]]\n","\n"]}]},{"cell_type":"markdown","source":["No role + NEWS"],"metadata":{"id":"P7R3fC1bhSyl"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"half_gsd_half_gpt_data_no_role_news.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"id":"Y9nhCa9ChY-R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723104649641,"user_tz":-120,"elapsed":630377,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"de8cee7b-ccbd-4bb2-c346-c175f63876bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.6228, Val Loss: 0.4947\n","Epoch 2, Train Loss: 0.5401, Val Loss: 0.4417\n","Epoch 3, Train Loss: 0.4567, Val Loss: 0.3930\n","Epoch 4, Train Loss: 0.4088, Val Loss: 0.3689\n","Epoch 5, Train Loss: 0.3852, Val Loss: 0.3556\n","Epoch 6, Train Loss: 0.3622, Val Loss: 0.3452\n","Epoch 7, Train Loss: 0.3476, Val Loss: 0.3442\n","Epoch 8, Train Loss: 0.3337, Val Loss: 0.3306\n","Epoch 9, Train Loss: 0.3205, Val Loss: 0.3244\n","Epoch 10, Train Loss: 0.3126, Val Loss: 0.3223\n","Test Accuracy: 25.81\n","Test F1-macro: 52.02\n","Test F1-micro: 67.90\n","Confusion Matrix:\n","Label: anticipation\n","[[2689  145]\n"," [ 362   63]]\n","\n","Label: optimism\n","[[1682  434]\n"," [ 212  931]]\n","\n","Label: trust\n","[[3016   90]\n"," [ 136   17]]\n","\n","Label: joy\n","[[1625  192]\n"," [ 294 1148]]\n","\n","Label: love\n","[[2578  165]\n"," [ 231  285]]\n","\n","Label: anger\n","[[1920  238]\n"," [ 241  860]]\n","\n","Label: disgust\n","[[1881  279]\n"," [ 274  825]]\n","\n","Label: pessimism\n","[[2743  141]\n"," [ 285   90]]\n","\n","Label: sadness\n","[[2136  163]\n"," [ 418  542]]\n","\n","Label: fear\n","[[2666  108]\n"," [ 197  288]]\n","\n","Label: surprise\n","[[3071   18]\n"," [ 160   10]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 1"],"metadata":{"id":"LCFwmoT8WnCo"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"half_gsd_half_gpt_data_role_1.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"id":"mMSNsMo8heqL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723105281803,"user_tz":-120,"elapsed":629518,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"661a815c-453d-40a7-8db3-89ee2d6bda3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.6172, Val Loss: 0.4897\n","Epoch 2, Train Loss: 0.5062, Val Loss: 0.4134\n","Epoch 3, Train Loss: 0.4146, Val Loss: 0.3944\n","Epoch 4, Train Loss: 0.3765, Val Loss: 0.3679\n","Epoch 5, Train Loss: 0.3542, Val Loss: 0.3603\n","Epoch 6, Train Loss: 0.3351, Val Loss: 0.3498\n","Epoch 7, Train Loss: 0.3206, Val Loss: 0.3410\n","Epoch 8, Train Loss: 0.3051, Val Loss: 0.3301\n","Epoch 9, Train Loss: 0.2938, Val Loss: 0.3338\n","Epoch 10, Train Loss: 0.2849, Val Loss: 0.3275\n","Test Accuracy: 24.42\n","Test F1-macro: 51.05\n","Test F1-micro: 66.93\n","Confusion Matrix:\n","Label: anticipation\n","[[2727  107]\n"," [ 395   30]]\n","\n","Label: optimism\n","[[1757  359]\n"," [ 275  868]]\n","\n","Label: trust\n","[[2991  115]\n"," [ 137   16]]\n","\n","Label: joy\n","[[1629  188]\n"," [ 286 1156]]\n","\n","Label: love\n","[[2565  178]\n"," [ 222  294]]\n","\n","Label: anger\n","[[1970  188]\n"," [ 313  788]]\n","\n","Label: disgust\n","[[1921  239]\n"," [ 323  776]]\n","\n","Label: pessimism\n","[[2598  286]\n"," [ 208  167]]\n","\n","Label: sadness\n","[[2051  248]\n"," [ 361  599]]\n","\n","Label: fear\n","[[2689   85]\n"," [ 220  265]]\n","\n","Label: surprise\n","[[3084    5]\n"," [ 166    4]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 2"],"metadata":{"id":"xX-rkFORWu6X"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5 #5e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"half_gsd_half_gpt_data_role_2.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"id":"075osLv2gNx6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723105914117,"user_tz":-120,"elapsed":628433,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"8c38b18a-ac95-4010-eec1-626db42cd4c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.6173, Val Loss: 0.4894\n","Epoch 2, Train Loss: 0.5152, Val Loss: 0.4217\n","Epoch 3, Train Loss: 0.4248, Val Loss: 0.3889\n","Epoch 4, Train Loss: 0.3858, Val Loss: 0.3735\n","Epoch 5, Train Loss: 0.3567, Val Loss: 0.3610\n","Epoch 6, Train Loss: 0.3393, Val Loss: 0.3504\n","Epoch 7, Train Loss: 0.3226, Val Loss: 0.3356\n","Epoch 8, Train Loss: 0.3057, Val Loss: 0.3368\n","Epoch 9, Train Loss: 0.2947, Val Loss: 0.3336\n","Epoch 10, Train Loss: 0.2823, Val Loss: 0.3254\n","Test Accuracy: 24.39\n","Test F1-macro: 51.56\n","Test F1-micro: 67.25\n","Confusion Matrix:\n","Label: anticipation\n","[[2696  138]\n"," [ 382   43]]\n","\n","Label: optimism\n","[[1712  404]\n"," [ 225  918]]\n","\n","Label: trust\n","[[2878  228]\n"," [ 122   31]]\n","\n","Label: joy\n","[[1595  222]\n"," [ 233 1209]]\n","\n","Label: love\n","[[2503  240]\n"," [ 209  307]]\n","\n","Label: anger\n","[[1963  195]\n"," [ 284  817]]\n","\n","Label: disgust\n","[[1929  231]\n"," [ 304  795]]\n","\n","Label: pessimism\n","[[2746  138]\n"," [ 269  106]]\n","\n","Label: sadness\n","[[2160  139]\n"," [ 456  504]]\n","\n","Label: fear\n","[[2708   66]\n"," [ 215  270]]\n","\n","Label: surprise\n","[[3078   11]\n"," [ 164    6]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 3"],"metadata":{"id":"ZW7EQHICWyOF"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"half_gsd_half_gpt_data_role_3.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKoxrMx4WzFV","executionInfo":{"status":"ok","timestamp":1723106544572,"user_tz":-120,"elapsed":625919,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"42f37ecd-a2a8-490f-f2b6-0835a3fbd9d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.6263, Val Loss: 0.4950\n","Epoch 2, Train Loss: 0.5108, Val Loss: 0.4252\n","Epoch 3, Train Loss: 0.4224, Val Loss: 0.3940\n","Epoch 4, Train Loss: 0.3872, Val Loss: 0.3760\n","Epoch 5, Train Loss: 0.3607, Val Loss: 0.3689\n","Epoch 6, Train Loss: 0.3417, Val Loss: 0.3614\n","Epoch 7, Train Loss: 0.3272, Val Loss: 0.3555\n","Epoch 8, Train Loss: 0.3141, Val Loss: 0.3481\n","Epoch 9, Train Loss: 0.3018, Val Loss: 0.3403\n","Epoch 10, Train Loss: 0.2901, Val Loss: 0.3365\n","Test Accuracy: 22.74\n","Test F1-macro: 50.31\n","Test F1-micro: 65.88\n","Confusion Matrix:\n","Label: anticipation\n","[[2617  217]\n"," [ 383   42]]\n","\n","Label: optimism\n","[[1699  417]\n"," [ 220  923]]\n","\n","Label: trust\n","[[2894  212]\n"," [ 125   28]]\n","\n","Label: joy\n","[[1632  185]\n"," [ 281 1161]]\n","\n","Label: love\n","[[2471  272]\n"," [ 206  310]]\n","\n","Label: anger\n","[[1988  170]\n"," [ 319  782]]\n","\n","Label: disgust\n","[[1952  208]\n"," [ 335  764]]\n","\n","Label: pessimism\n","[[2763  121]\n"," [ 277   98]]\n","\n","Label: sadness\n","[[2176  123]\n"," [ 495  465]]\n","\n","Label: fear\n","[[2642  132]\n"," [ 177  308]]\n","\n","Label: surprise\n","[[3071   18]\n"," [ 167    3]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 4"],"metadata":{"id":"YUG4nok5W1ol"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"half_gsd_half_gpt_data_role_4.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Qn_uQxJgXAV","outputId":"fa924e60-098d-4035-a4f8-e8abb2095ee8","executionInfo":{"status":"ok","timestamp":1723107175995,"user_tz":-120,"elapsed":625859,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.6133, Val Loss: 0.4938\n","Epoch 2, Train Loss: 0.5108, Val Loss: 0.4374\n","Epoch 3, Train Loss: 0.4414, Val Loss: 0.3882\n","Epoch 4, Train Loss: 0.3944, Val Loss: 0.3659\n","Epoch 5, Train Loss: 0.3713, Val Loss: 0.3497\n","Epoch 6, Train Loss: 0.3522, Val Loss: 0.3415\n","Epoch 7, Train Loss: 0.3346, Val Loss: 0.3391\n","Epoch 8, Train Loss: 0.3197, Val Loss: 0.3336\n","Epoch 9, Train Loss: 0.3096, Val Loss: 0.3231\n","Epoch 10, Train Loss: 0.2990, Val Loss: 0.3226\n","Test Accuracy: 26.08\n","Test F1-macro: 51.73\n","Test F1-micro: 68.19\n","Confusion Matrix:\n","Label: anticipation\n","[[2772   62]\n"," [ 392   33]]\n","\n","Label: optimism\n","[[1707  409]\n"," [ 228  915]]\n","\n","Label: trust\n","[[3065   41]\n"," [ 138   15]]\n","\n","Label: joy\n","[[1598  219]\n"," [ 242 1200]]\n","\n","Label: love\n","[[2593  150]\n"," [ 227  289]]\n","\n","Label: anger\n","[[1996  162]\n"," [ 330  771]]\n","\n","Label: disgust\n","[[1953  207]\n"," [ 340  759]]\n","\n","Label: pessimism\n","[[2703  181]\n"," [ 253  122]]\n","\n","Label: sadness\n","[[2108  191]\n"," [ 381  579]]\n","\n","Label: fear\n","[[2670  104]\n"," [ 202  283]]\n","\n","Label: surprise\n","[[3073   16]\n"," [ 164    6]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 1 + NEWS"],"metadata":{"id":"AFt27HJMW6V1"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"half_gsd_half_gpt_data_role_5.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DG3p5x8Amn2z","executionInfo":{"status":"ok","timestamp":1723108120395,"user_tz":-120,"elapsed":624130,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"a7ab62c6-a7e2-45fa-9b67-c34e9d39a9bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.6105, Val Loss: 0.4939\n","Epoch 2, Train Loss: 0.5193, Val Loss: 0.4209\n","Epoch 3, Train Loss: 0.4381, Val Loss: 0.3759\n","Epoch 4, Train Loss: 0.4024, Val Loss: 0.3529\n","Epoch 5, Train Loss: 0.3783, Val Loss: 0.3527\n","Epoch 6, Train Loss: 0.3589, Val Loss: 0.3366\n","Epoch 7, Train Loss: 0.3439, Val Loss: 0.3364\n","Epoch 8, Train Loss: 0.3325, Val Loss: 0.3264\n","Epoch 9, Train Loss: 0.3185, Val Loss: 0.3283\n","Epoch 10, Train Loss: 0.3098, Val Loss: 0.3178\n","Test Accuracy: 26.11\n","Test F1-macro: 51.46\n","Test F1-micro: 67.91\n","Confusion Matrix:\n","Label: anticipation\n","[[2729  105]\n"," [ 370   55]]\n","\n","Label: optimism\n","[[1713  403]\n"," [ 217  926]]\n","\n","Label: trust\n","[[3072   34]\n"," [ 145    8]]\n","\n","Label: joy\n","[[1554  263]\n"," [ 217 1225]]\n","\n","Label: love\n","[[2556  187]\n"," [ 236  280]]\n","\n","Label: anger\n","[[1977  181]\n"," [ 314  787]]\n","\n","Label: disgust\n","[[1939  221]\n"," [ 328  771]]\n","\n","Label: pessimism\n","[[2743  141]\n"," [ 290   85]]\n","\n","Label: sadness\n","[[2094  205]\n"," [ 394  566]]\n","\n","Label: fear\n","[[2698   76]\n"," [ 210  275]]\n","\n","Label: surprise\n","[[3063   26]\n"," [ 155   15]]\n","\n"]}]},{"cell_type":"markdown","source":["##RoBERTa Single Label"],"metadata":{"id":"sd9FAjJXhtO2"}},{"cell_type":"markdown","source":["NO ROLE"],"metadata":{"id":"5Mr1QFfah1JY"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"gsd_gpt_single_label_no_role.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d490420e8fbf474dbc2f76f890309465","3a3e5e2ab04f4438873f3936f9dfccb9","fa6896db5d314f95921b5692f47500c2","3986740808eb46c28d06f4b42b8085c6","a01330ee600546be82dadb996a13e816","ec21943214cc4282aaa25531ecac9f79","835b6048d31d4840b4aa5ef295790547","912858ec723d4f1a92d2691c3242dfc7","cd32c19751944f8a919a51000085a388","2ce8313b718b4389ae7c3a9396fca041","eaf84a3cdafa4f958ee63de760b1819d","54625e12ab55488c9582b83d04921d0c","df0ecd89c0894bd59e3cada0903c9e5f","cf9ab924e86943cab7c723d6d549e846","74cb2354a6794fbd9e12a3f32dfee2a5","8bdd1f4b146241b19a4077abe4023ff3","e10106c56c8042ba9cd0fc652033d43b","ba5022f9512e4ca0bfed2b70579cf588","789b18bef4b0448e8ec208f2ce62b1f5","39ed0bbb2c4741249d4e6cd37b11839c","50ca3868a5a949118f721b274d7c53a7","fe9624e7658b4cd9828a744758006a88","f619f8de8b6a428a8050aed707edae2b","0ac2f00e0a4949d6ad2057df4555288a","74525066746b482b99de9fb68ff62b9f","2be98ae5f2c9478caef7547436ca4015","469f6e1dc711410983af8b5fd3608372","4e3d32a8051b4859862f151a31353df8","27e6813023d5460e9edde49e72643bcd","08183e2a56354ccbb392a234cd5603aa","f02a19fa90f749b69c553055435e69d0","a790a386374340d8a521643b73ab0c8c","8215d52a466f40c9b15f978a64580a9b","2b0b54959b2c434482f59a255980d12c","11120604789b42f3b19d0acbfd35dd68","2bc7165986514288ae25ab26cc0eee1b","2aae06b7a1b448b0ae6f36671c4f26ec","56f8af52ec3b40b08b80be446cd4e64b","bbff215f43744ac48d55fa472e84f447","aa8f88c140c04dfca1a52776beb5ad21","8656fbde05be4b87975cd0a604e84fa2","c16685b40ae94e78b5894d774bea113c","26fce69f81df4b2482f59742bbbf458f","9bdb54e88fae4728987331b854835d2f","d851fb5920144013b4ae89cf97a2e7ca","72fd1255f84d461d936c5228fc0b11f1","9cf969bb9c884987981b48a1ea44df6c","c25060701200461ba8710b7b24120da1","9a4f83ce884c43f5934776954fabd51d","609d4ed6d4be4aebba48fed7ecd72d53","c404f35e7dc9415ca0d3c8d52993d940","ff4fd8e475a647528ce9d9f4fd4c5d14","9e5d67c8871b4a42b6d9ce37aec3451c","819181b9cc2a4ad0b139e2ac97d02105","9944dd38937244368b135ce8e911dd81","8599a94aaca640d580dc18898f0039d5","05256dfdf6f54bc38c76e37e0621f3de","b6c26d7dc0ed40188b58222bc98fa557","3746f7f1060d455c92878f34f5601627","c64a52785b7f42f0b0e71f2bcf4fe847","6e412d0c8ffb4a079c36f32fa580c1dc","59d2920d85444d8184b340815b142e47","2ec21578ed8549ca902bb4ce98a76c73","287b5823d4fc46529299e3bd96dfba23","86ba71cf76824675ba288cd2bbe8b1e4","bfa538a574e645f38494cdc47bedb88b"]},"id":"4Uh6xV0whv4r","executionInfo":{"status":"ok","timestamp":1723130890163,"user_tz":-120,"elapsed":648196,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"b2563192-bf71-4fc2-bb9a-81b511e94f1f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d490420e8fbf474dbc2f76f890309465"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54625e12ab55488c9582b83d04921d0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f619f8de8b6a428a8050aed707edae2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b0b54959b2c434482f59a255980d12c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d851fb5920144013b4ae89cf97a2e7ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8599a94aaca640d580dc18898f0039d5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5723, Val Loss: 0.4858\n","Epoch 2, Train Loss: 0.4513, Val Loss: 0.4760\n","Epoch 3, Train Loss: 0.4177, Val Loss: 0.3980\n","Epoch 4, Train Loss: 0.3753, Val Loss: 0.3697\n","Epoch 5, Train Loss: 0.3456, Val Loss: 0.3536\n","Epoch 6, Train Loss: 0.3231, Val Loss: 0.3393\n","Epoch 7, Train Loss: 0.3031, Val Loss: 0.3318\n","Epoch 8, Train Loss: 0.2874, Val Loss: 0.3260\n","Epoch 9, Train Loss: 0.2760, Val Loss: 0.3184\n","Epoch 10, Train Loss: 0.2649, Val Loss: 0.3162\n","Test Accuracy: 26.02\n","Test F1-macro: 45.97\n","Test F1-micro: 66.87\n","Confusion Matrix:\n","Label: anticipation\n","[[2791   43]\n"," [ 387   38]]\n","\n","Label: optimism\n","[[1705  411]\n"," [ 267  876]]\n","\n","Label: trust\n","[[3106    0]\n"," [ 153    0]]\n","\n","Label: joy\n","[[1586  231]\n"," [ 260 1182]]\n","\n","Label: love\n","[[2635  108]\n"," [ 293  223]]\n","\n","Label: anger\n","[[1976  182]\n"," [ 301  800]]\n","\n","Label: disgust\n","[[1948  212]\n"," [ 327  772]]\n","\n","Label: pessimism\n","[[2880    4]\n"," [ 366    9]]\n","\n","Label: sadness\n","[[2190  109]\n"," [ 487  473]]\n","\n","Label: fear\n","[[2690   84]\n"," [ 211  274]]\n","\n","Label: surprise\n","[[3089    0]\n"," [ 169    1]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 1"],"metadata":{"id":"Rj-a9KL4h5GF"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"gsd_gpt_single_label_role_1.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xs4Bl4gqh5vx","executionInfo":{"status":"ok","timestamp":1723131546919,"user_tz":-120,"elapsed":649351,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"c676fbec-710a-40f1-e15f-d454152627a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.6008, Val Loss: 0.4906\n","Epoch 2, Train Loss: 0.4570, Val Loss: 0.4863\n","Epoch 3, Train Loss: 0.4431, Val Loss: 0.4427\n","Epoch 4, Train Loss: 0.4004, Val Loss: 0.3946\n","Epoch 5, Train Loss: 0.3661, Val Loss: 0.3636\n","Epoch 6, Train Loss: 0.3405, Val Loss: 0.3537\n","Epoch 7, Train Loss: 0.3187, Val Loss: 0.3466\n","Epoch 8, Train Loss: 0.2998, Val Loss: 0.3327\n","Epoch 9, Train Loss: 0.2864, Val Loss: 0.3262\n","Epoch 10, Train Loss: 0.2724, Val Loss: 0.3228\n","Test Accuracy: 26.39\n","Test F1-macro: 48.25\n","Test F1-micro: 67.40\n","Confusion Matrix:\n","Label: anticipation\n","[[2793   41]\n"," [ 392   33]]\n","\n","Label: optimism\n","[[1760  356]\n"," [ 301  842]]\n","\n","Label: trust\n","[[3103    3]\n"," [ 149    4]]\n","\n","Label: joy\n","[[1657  160]\n"," [ 319 1123]]\n","\n","Label: love\n","[[2543  200]\n"," [ 213  303]]\n","\n","Label: anger\n","[[1929  229]\n"," [ 262  839]]\n","\n","Label: disgust\n","[[1895  265]\n"," [ 296  803]]\n","\n","Label: pessimism\n","[[2860   24]\n"," [ 337   38]]\n","\n","Label: sadness\n","[[2184  115]\n"," [ 482  478]]\n","\n","Label: fear\n","[[2692   82]\n"," [ 199  286]]\n","\n","Label: surprise\n","[[3089    0]\n"," [ 169    1]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 1 + NEWS"],"metadata":{"id":"hvJXiqjkh8LJ"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"gsd_gpt_single_label_role_1_news.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2lCsho9h9EO","executionInfo":{"status":"ok","timestamp":1723132196161,"user_tz":-120,"elapsed":641914,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"e926d963-1ae8-433d-b49b-6b399635ec40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5742, Val Loss: 0.4861\n","Epoch 2, Train Loss: 0.4548, Val Loss: 0.4808\n","Epoch 3, Train Loss: 0.4342, Val Loss: 0.4116\n","Epoch 4, Train Loss: 0.3920, Val Loss: 0.3754\n","Epoch 5, Train Loss: 0.3595, Val Loss: 0.3522\n","Epoch 6, Train Loss: 0.3385, Val Loss: 0.3382\n","Epoch 7, Train Loss: 0.3234, Val Loss: 0.3384\n","Epoch 8, Train Loss: 0.3053, Val Loss: 0.3276\n","Epoch 9, Train Loss: 0.2969, Val Loss: 0.3189\n","Epoch 10, Train Loss: 0.2851, Val Loss: 0.3182\n","Test Accuracy: 26.88\n","Test F1-macro: 46.78\n","Test F1-micro: 67.21\n","Confusion Matrix:\n","Label: anticipation\n","[[2831    3]\n"," [ 424    1]]\n","\n","Label: optimism\n","[[1647  469]\n"," [ 208  935]]\n","\n","Label: trust\n","[[3106    0]\n"," [ 153    0]]\n","\n","Label: joy\n","[[1551  266]\n"," [ 196 1246]]\n","\n","Label: love\n","[[2579  164]\n"," [ 221  295]]\n","\n","Label: anger\n","[[2013  145]\n"," [ 349  752]]\n","\n","Label: disgust\n","[[1987  173]\n"," [ 377  722]]\n","\n","Label: pessimism\n","[[2845   39]\n"," [ 319   56]]\n","\n","Label: sadness\n","[[2199  100]\n"," [ 511  449]]\n","\n","Label: fear\n","[[2741   33]\n"," [ 256  229]]\n","\n","Label: surprise\n","[[3088    1]\n"," [ 167    3]]\n","\n"]}]},{"cell_type":"markdown","source":["Increased multi-label"],"metadata":{"id":"dhQVOXTw4CeS"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"gsd_gpt_increased_multi.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1ffec70c17ae440a93d4958c9483f599","9884bd2a6fd14a1fafcbdd19e9c61eb4","605455bfec134bb7bd665e60d63705b1","ae455097ab29484697d9f52a9750d925","f984811d6eea4771be638803644102b0","ea6bf797f9344ac789b32491a9b94dbf","451ea0d4da3540d888ddde67a9a0b2f2","1aa46fa1ad874ff0a900a60086a5a88f","e9338ac654ad47eab728f3c8e75e7f9d","0fa9470f8fd54b8aac0963c445a83511","0d328f1d995643efbf009430ffc4e063","4f5233f7a85b4cc5b694c808228da264","7285d68e9da345a487fcd6202a1746b7","cf5d8f5123d4456087ce5cb2573c0c24","19ab87b9349c4015b78fdb3ed73e1110","887698bb0bf84241bb5988cc6bf24f05","a97495399adc4589a090f6263dec8919","9202ee81c3084a0a99b8b505a5181fd7","f8725afcbc3d4f2c953431ee1faee1c6","7c1a446ac8fe4fb8acbf94e5a8f5575e","3e2cd25a56d0447cb8e1480b022f96a0","fa73950001a24716a9f3a295f38de8fb","7e268a01a2444bc696a0f53bcc2867e2","b020b789533e4c748129e1e83001c99c","a4d9c62eb6764f08b5ab6c096d8dbd60","751b05f75a214b5ea69f682e3fd94576","26d50baa9a63492cabddb50db4624ac4","3fa412d9b8d7490ea7618292fe34746f","e0b96835ed4f47cb912e851d4e3c3402","de7ef3c29ae14c92a1fbe0cad66f1731","12771d84b0b6482c8c4b91004d30df2e","f3d9bc1074144e24a1a0766a88517514","ff4a02c5640e424b83dad460e52ca58a","1969b2ffa0fa412d944a807d8c3e93c1","7a4614bf86ca4130a3f0faa23ae8c6d1","86c3120a0b9c4955b2a51743150cf260","d2872075fc0242c5a0c436ce451af7c4","74e1f84077eb44559ebeb96500b6770d","856d320562244d0e9979074676600521","d2f9d2f5a7514b3fa6e9211a5b9c397b","f005824fef4849acac5979dec9f39e96","233ed0774f3e45c9a3f2382ae74f833d","b17c6e7900054fee808f6bd1db201b8b","f09db17c39524b0eac1ccda8ad078dd9","0f777898e3af46dda86978a3391f6f4b","b6b975d730a443139c1001b89a66ade2","adabe25462b248ad9463aadba525be1b","8786bf2e12134a8a8748b1d62ee73743","c86fa111794c4d96b2d73032c938477f","07920d19fc6e4458a492338865db8b7d","37f29c16ff6a4ddab96b0dfbeaef9d2b","c811c4a3bc8d472baacd767236bdbd74","57d96ce946644547af55196f8ae2930d","8aa4fb7ac6e64fb296037f15db811751","77acfe065816484789ee2b787dc121ad","3f1254d9a5284a4483e71610d8fca2af","10331b025fb449978944f17b68a5b1e6","46c3c9b6a124459fbf4480be58e0e20d","d3d4fab526fe4a69a9471c4c688c855b","3c94f76efeb04b748233f65e26875c9c","aa06dae160ef48238e771f9c8dd96094","6312dd219f544cc3bfbb1bb684bbec64","b7023865968340249e47dcd126104781","e027c9e909064d109d43ca6443461867","45d252ab38d64b3097f2cfd1a4deeb86","201b511579b949dd95c667a26238f52d"]},"id":"VBM38T0o4D_-","executionInfo":{"status":"ok","timestamp":1723204015892,"user_tz":-120,"elapsed":724226,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"221eb530-e863-4307-98bc-a73367fcf918"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ffec70c17ae440a93d4958c9483f599"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f5233f7a85b4cc5b694c808228da264"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e268a01a2444bc696a0f53bcc2867e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1969b2ffa0fa412d944a807d8c3e93c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f777898e3af46dda86978a3391f6f4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f1254d9a5284a4483e71610d8fca2af"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.6135, Val Loss: 0.4731\n","Epoch 2, Train Loss: 0.4339, Val Loss: 0.3949\n","Epoch 3, Train Loss: 0.3595, Val Loss: 0.3745\n","Epoch 4, Train Loss: 0.3273, Val Loss: 0.3593\n","Epoch 5, Train Loss: 0.3043, Val Loss: 0.3479\n","Epoch 6, Train Loss: 0.2888, Val Loss: 0.3444\n","Epoch 7, Train Loss: 0.2716, Val Loss: 0.3332\n","Epoch 8, Train Loss: 0.2606, Val Loss: 0.3364\n","Epoch 9, Train Loss: 0.2525, Val Loss: 0.3330\n","Epoch 10, Train Loss: 0.2407, Val Loss: 0.3204\n","Test Accuracy: 24.42\n","Test F1-macro: 52.58\n","Test F1-micro: 67.58\n","Confusion Matrix:\n","Label: anticipation\n","[[2698  136]\n"," [ 376   49]]\n","\n","Label: optimism\n","[[1770  346]\n"," [ 279  864]]\n","\n","Label: trust\n","[[2949  157]\n"," [ 135   18]]\n","\n","Label: joy\n","[[1617  200]\n"," [ 283 1159]]\n","\n","Label: love\n","[[2558  185]\n"," [ 213  303]]\n","\n","Label: anger\n","[[1941  217]\n"," [ 268  833]]\n","\n","Label: disgust\n","[[1907  253]\n"," [ 285  814]]\n","\n","Label: pessimism\n","[[2694  190]\n"," [ 253  122]]\n","\n","Label: sadness\n","[[2049  250]\n"," [ 353  607]]\n","\n","Label: fear\n","[[2659  115]\n"," [ 186  299]]\n","\n","Label: surprise\n","[[3048   41]\n"," [ 155   15]]\n","\n"]}]},{"cell_type":"markdown","source":["Increased single-label"],"metadata":{"id":"c2m0PYvo4EaN"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"gsd_gpt_increased_single.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ayyl4pMm4Flp","executionInfo":{"status":"ok","timestamp":1723204751656,"user_tz":-120,"elapsed":720967,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"bea27b73-e49e-402d-dfc0-6ec2387768a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5494, Val Loss: 0.4948\n","Epoch 2, Train Loss: 0.4178, Val Loss: 0.4276\n","Epoch 3, Train Loss: 0.3669, Val Loss: 0.3920\n","Epoch 4, Train Loss: 0.3266, Val Loss: 0.3632\n","Epoch 5, Train Loss: 0.2968, Val Loss: 0.3502\n","Epoch 6, Train Loss: 0.2746, Val Loss: 0.3341\n","Epoch 7, Train Loss: 0.2524, Val Loss: 0.3270\n","Epoch 8, Train Loss: 0.2405, Val Loss: 0.3238\n","Epoch 9, Train Loss: 0.2267, Val Loss: 0.3175\n","Epoch 10, Train Loss: 0.2176, Val Loss: 0.3152\n","Test Accuracy: 25.50\n","Test F1-macro: 49.97\n","Test F1-micro: 67.15\n","Confusion Matrix:\n","Label: anticipation\n","[[2787   47]\n"," [ 382   43]]\n","\n","Label: optimism\n","[[1850  266]\n"," [ 408  735]]\n","\n","Label: trust\n","[[3090   16]\n"," [ 145    8]]\n","\n","Label: joy\n","[[1668  149]\n"," [ 340 1102]]\n","\n","Label: love\n","[[2604  139]\n"," [ 257  259]]\n","\n","Label: anger\n","[[1937  221]\n"," [ 273  828]]\n","\n","Label: disgust\n","[[1885  275]\n"," [ 273  826]]\n","\n","Label: pessimism\n","[[2833   51]\n"," [ 340   35]]\n","\n","Label: sadness\n","[[2118  181]\n"," [ 393  567]]\n","\n","Label: fear\n","[[2660  114]\n"," [ 183  302]]\n","\n","Label: surprise\n","[[3076   13]\n"," [ 154   16]]\n","\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7c6598125fd14388a80a2fc91c591a01":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9825fa236f6a497d8011629111f44e37","IPY_MODEL_c81fab7945a24f76b8223aed7d104742","IPY_MODEL_c16be8df05854bd790249a2b9234c70c"],"layout":"IPY_MODEL_a68e934fb7c6417ea5a45ac4fd0ff761"}},"9825fa236f6a497d8011629111f44e37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2e20208d0d64690b5c2a710c1478ba7","placeholder":"","style":"IPY_MODEL_273966be0c9b44809e16544b8cc1fc48","value":"tokenizer_config.json:100%"}},"c81fab7945a24f76b8223aed7d104742":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_76e7700fcbab46c4ac54371709f5faf9","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d89aacea0ab4f63876f6772b61038d2","value":48}},"c16be8df05854bd790249a2b9234c70c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db533772ff1f4180b8864c615b00f7f3","placeholder":"","style":"IPY_MODEL_61cf82ea86c14a73bac7a104c074dc81","value":"48.0/48.0[00:00&lt;00:00,1.12kB/s]"}},"a68e934fb7c6417ea5a45ac4fd0ff761":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2e20208d0d64690b5c2a710c1478ba7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"273966be0c9b44809e16544b8cc1fc48":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76e7700fcbab46c4ac54371709f5faf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d89aacea0ab4f63876f6772b61038d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db533772ff1f4180b8864c615b00f7f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61cf82ea86c14a73bac7a104c074dc81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7dae49384134478bb2d0f40b0190f69f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_50e99765f46d4fd3bb073eb46c41502c","IPY_MODEL_7ce2c48ab3cb4515b577d8c17b3f5483","IPY_MODEL_a025e5dbe52a4498b79e4cc5fdb08d8b"],"layout":"IPY_MODEL_7dd3ee3f4d514971b391ad9792d89dc0"}},"50e99765f46d4fd3bb073eb46c41502c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1134dafebdfc47bda1f24572dd878a22","placeholder":"","style":"IPY_MODEL_55ea6c6793bf45c69c62c7301d66195c","value":"vocab.txt:100%"}},"7ce2c48ab3cb4515b577d8c17b3f5483":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10d95c2f353144e4a91b1605861b5676","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fd3d8a6859a049a3a5d4bf2042ee163a","value":231508}},"a025e5dbe52a4498b79e4cc5fdb08d8b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcd25c6f756e4f54a29a3c98bdd4eccd","placeholder":"","style":"IPY_MODEL_77853a9cc6c4439fbad36ebc92e7afb8","value":"232k/232k[00:00&lt;00:00,2.40MB/s]"}},"7dd3ee3f4d514971b391ad9792d89dc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1134dafebdfc47bda1f24572dd878a22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55ea6c6793bf45c69c62c7301d66195c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10d95c2f353144e4a91b1605861b5676":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd3d8a6859a049a3a5d4bf2042ee163a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fcd25c6f756e4f54a29a3c98bdd4eccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77853a9cc6c4439fbad36ebc92e7afb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bddaa0b2c826427babd8438bad2e582a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_29b9fb12723d4bb683d2fe0b146b3913","IPY_MODEL_5d828a52833d4586bcf07b6b9d7d80bf","IPY_MODEL_44d290b2103f4f3db53bc69f4e14f75f"],"layout":"IPY_MODEL_733f0ec3e1a84b8aaae7f0b1bd47ebe6"}},"29b9fb12723d4bb683d2fe0b146b3913":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a26ec8fd9095491da765253d3093519a","placeholder":"","style":"IPY_MODEL_0a564acd7a2548499ca7e0b090b8f5b6","value":"tokenizer.json:100%"}},"5d828a52833d4586bcf07b6b9d7d80bf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2159d1fbbeac4ba7bd9ebba93cb1588e","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_706add22b0054aaa9b59dda6d423d808","value":466062}},"44d290b2103f4f3db53bc69f4e14f75f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a776f5036214fcaa413122ffa66ead3","placeholder":"","style":"IPY_MODEL_1840f6d195dc4f63b4c58a9c8ef71ccd","value":"466k/466k[00:00&lt;00:00,2.32MB/s]"}},"733f0ec3e1a84b8aaae7f0b1bd47ebe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a26ec8fd9095491da765253d3093519a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a564acd7a2548499ca7e0b090b8f5b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2159d1fbbeac4ba7bd9ebba93cb1588e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"706add22b0054aaa9b59dda6d423d808":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a776f5036214fcaa413122ffa66ead3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1840f6d195dc4f63b4c58a9c8ef71ccd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b81faa97a37d4354a6ec304785a76322":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f40a08c26284ce09b03b6940bb408f2","IPY_MODEL_3b0b9679c0cf47249e6d1e4fa09052b7","IPY_MODEL_d5f5d1af7f8f4bcc9ceb45bc622895af"],"layout":"IPY_MODEL_fd39058fbc5f4a23bce2a888e0e83eca"}},"4f40a08c26284ce09b03b6940bb408f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54f311f67b164751a48e981f3995bb5d","placeholder":"","style":"IPY_MODEL_b3e51991f0b943a0bc654a00a3d74db9","value":"config.json:100%"}},"3b0b9679c0cf47249e6d1e4fa09052b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_487fa7f735894dad8106cdd24fb00732","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_826e70b90cc845efae148230bc23d856","value":570}},"d5f5d1af7f8f4bcc9ceb45bc622895af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ee1f7554a2745388990837fa576d1fe","placeholder":"","style":"IPY_MODEL_7400cfa4235442b48952aa3a782f0391","value":"570/570[00:00&lt;00:00,10.4kB/s]"}},"fd39058fbc5f4a23bce2a888e0e83eca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54f311f67b164751a48e981f3995bb5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3e51991f0b943a0bc654a00a3d74db9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"487fa7f735894dad8106cdd24fb00732":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"826e70b90cc845efae148230bc23d856":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ee1f7554a2745388990837fa576d1fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7400cfa4235442b48952aa3a782f0391":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff3b105270fd4f50b4e034fd6a5b5014":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_57472a9678414521950e09bffbd39cb7","IPY_MODEL_040a58a4ee7345a2ae2fa938595dc272","IPY_MODEL_d6d5cd817f2b4e22ab54ebd5622b2c93"],"layout":"IPY_MODEL_1ab851f2a04649f4b3932c0845eb7acd"}},"57472a9678414521950e09bffbd39cb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7d214a47de24c9dad0bfe45eb685717","placeholder":"","style":"IPY_MODEL_640384c0ef694c92822f1bff96d15559","value":"model.safetensors:100%"}},"040a58a4ee7345a2ae2fa938595dc272":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_92abc84017284791b00d67ff66b0e54d","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b98897ee698648bb9ff1f103ce07d7c2","value":440449768}},"d6d5cd817f2b4e22ab54ebd5622b2c93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1e6087ab11943b88761b0743fbecbd2","placeholder":"","style":"IPY_MODEL_e5918de91219450aae4f9c819ea047a1","value":"440M/440M[00:04&lt;00:00,118MB/s]"}},"1ab851f2a04649f4b3932c0845eb7acd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7d214a47de24c9dad0bfe45eb685717":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"640384c0ef694c92822f1bff96d15559":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92abc84017284791b00d67ff66b0e54d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b98897ee698648bb9ff1f103ce07d7c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1e6087ab11943b88761b0743fbecbd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5918de91219450aae4f9c819ea047a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d4b9bc822ee411cbd794fa36e03e979":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fef8c2f11fb446c69fa9f10a5f92f55a","IPY_MODEL_b548199952ef4b9fbd5c095002ebe0a2","IPY_MODEL_3c9a03d0c62548e2ba38aff3d46ef140"],"layout":"IPY_MODEL_4e15964de7424e9780e02ae93a505c6e"}},"fef8c2f11fb446c69fa9f10a5f92f55a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfa313fcfd324d52a0bda46013cd744c","placeholder":"","style":"IPY_MODEL_eb9272b241b14da4be2d03d0323fae85","value":"tokenizer_config.json:100%"}},"b548199952ef4b9fbd5c095002ebe0a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_650d4c325db44b9e8af23de1eb9810c9","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6bb75129aa4145b08e7b94750cb9e782","value":48}},"3c9a03d0c62548e2ba38aff3d46ef140":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0990d9dea484886b9a1031f81dd67df","placeholder":"","style":"IPY_MODEL_975bb59131284e189ad05073a226db1d","value":"48.0/48.0[00:00&lt;00:00,1.14kB/s]"}},"4e15964de7424e9780e02ae93a505c6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfa313fcfd324d52a0bda46013cd744c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb9272b241b14da4be2d03d0323fae85":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"650d4c325db44b9e8af23de1eb9810c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bb75129aa4145b08e7b94750cb9e782":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f0990d9dea484886b9a1031f81dd67df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"975bb59131284e189ad05073a226db1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf5f9fd288b4438787672ae7df8e33b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b3f1e6de7cb9494ea88185e81b5a51ee","IPY_MODEL_08006ce0b7814784bbb8c0c4eae7531f","IPY_MODEL_724549cd5c5c4faebb660ca006a09c98"],"layout":"IPY_MODEL_1f761e94fcc4475f8a6a6a4065fd263a"}},"b3f1e6de7cb9494ea88185e81b5a51ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f7755a60f3b4c2d83c0d89e1176a354","placeholder":"","style":"IPY_MODEL_ab292304ee3049568721edf6a6edefe1","value":"vocab.txt:100%"}},"08006ce0b7814784bbb8c0c4eae7531f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c77479897a5d47db86c66ce6d0831d0e","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_361198cb08ef44b780ec941e262b398f","value":231508}},"724549cd5c5c4faebb660ca006a09c98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99b61d0a2b604176a301cbd0fe347ba0","placeholder":"","style":"IPY_MODEL_c2a23f833f9a43fa9fb292ce848ac016","value":"232k/232k[00:00&lt;00:00,3.37MB/s]"}},"1f761e94fcc4475f8a6a6a4065fd263a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f7755a60f3b4c2d83c0d89e1176a354":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab292304ee3049568721edf6a6edefe1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c77479897a5d47db86c66ce6d0831d0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"361198cb08ef44b780ec941e262b398f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"99b61d0a2b604176a301cbd0fe347ba0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2a23f833f9a43fa9fb292ce848ac016":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8594c4299f04bf89b9bffab3af5e429":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3d286db8a344ef495464eca438eb44a","IPY_MODEL_05434ba8cbfa4eef90e46375224d1fbc","IPY_MODEL_3070c249ae7a4147bdc934c2ec3183ee"],"layout":"IPY_MODEL_19c98d8090994e69abab9a3a5dcd1db2"}},"f3d286db8a344ef495464eca438eb44a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3ac59e4ff3847ee8ff746f621a9c0bd","placeholder":"","style":"IPY_MODEL_5a1967a84a8643c8ba1d6d89cce5e0f4","value":"tokenizer.json:100%"}},"05434ba8cbfa4eef90e46375224d1fbc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf0612b4359c4e16bb602ed887c40833","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2a7b3defe317404391b58d8ea2e71646","value":466062}},"3070c249ae7a4147bdc934c2ec3183ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df610d10c2fa450c96135ae91a0d1aaf","placeholder":"","style":"IPY_MODEL_7f1e7fa3ee55454db0a3321b3536fc23","value":"466k/466k[00:00&lt;00:00,15.0MB/s]"}},"19c98d8090994e69abab9a3a5dcd1db2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3ac59e4ff3847ee8ff746f621a9c0bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a1967a84a8643c8ba1d6d89cce5e0f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf0612b4359c4e16bb602ed887c40833":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a7b3defe317404391b58d8ea2e71646":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"df610d10c2fa450c96135ae91a0d1aaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f1e7fa3ee55454db0a3321b3536fc23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02c76b299656463581d31a870277b7f0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c7bd8707fe34551bd343f9ab2ca49c7","IPY_MODEL_d9acdb86793346e78e69c60557b046b8","IPY_MODEL_dd536fa0f03542b6895facd9b3615509"],"layout":"IPY_MODEL_1ab523b261ca4e18ac79015ad799dfe2"}},"3c7bd8707fe34551bd343f9ab2ca49c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_caa616778f4a4dc38d444b4e56594b7d","placeholder":"","style":"IPY_MODEL_8bd760aab04747f5af84d7e84633c3d5","value":"config.json:100%"}},"d9acdb86793346e78e69c60557b046b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aed91a97c61546d68ec604180f531cbf","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2a416d504d3c4b27aad80ba25d155d0b","value":570}},"dd536fa0f03542b6895facd9b3615509":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_290a62c13dd947288639119ffe6ca208","placeholder":"","style":"IPY_MODEL_0c69d3ce08f14d218f3d34fbd9c38a51","value":"570/570[00:00&lt;00:00,9.90kB/s]"}},"1ab523b261ca4e18ac79015ad799dfe2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caa616778f4a4dc38d444b4e56594b7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bd760aab04747f5af84d7e84633c3d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aed91a97c61546d68ec604180f531cbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a416d504d3c4b27aad80ba25d155d0b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"290a62c13dd947288639119ffe6ca208":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c69d3ce08f14d218f3d34fbd9c38a51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4159b3accc9b46cfb13f4ff620ccfc43":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a073eb19b3be481f93e0859d8a1c626e","IPY_MODEL_3574a09805b84ce0b5b2b05f7a0f7bde","IPY_MODEL_75969a9b8f164646a3b3305b35f3b248"],"layout":"IPY_MODEL_e2942467348a486d9c1e883b6f299db9"}},"a073eb19b3be481f93e0859d8a1c626e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c94168c6cc14b6bbc929c887641c417","placeholder":"","style":"IPY_MODEL_dc6279d4874249709bc81337e1210e8f","value":"model.safetensors:100%"}},"3574a09805b84ce0b5b2b05f7a0f7bde":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_869e8cec7ec64613b1fa4c21eec26e7e","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce7edf0c3df24b3b96b2beebf38ed44e","value":440449768}},"75969a9b8f164646a3b3305b35f3b248":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_288f9f6ccda24c5fbf36432f4c0f5ea3","placeholder":"","style":"IPY_MODEL_ca0f9d92c9aa40c99efee9c34cc8c448","value":"440M/440M[00:03&lt;00:00,153MB/s]"}},"e2942467348a486d9c1e883b6f299db9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c94168c6cc14b6bbc929c887641c417":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc6279d4874249709bc81337e1210e8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"869e8cec7ec64613b1fa4c21eec26e7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce7edf0c3df24b3b96b2beebf38ed44e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"288f9f6ccda24c5fbf36432f4c0f5ea3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca0f9d92c9aa40c99efee9c34cc8c448":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6866b4ce70134efabfbc82230f66916b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f0a0128845824466b38f7e67ade1e0fc","IPY_MODEL_b05cd68ad8ce4579af42e05e9b246120","IPY_MODEL_0fc6c4a041634f9785d1dcf72b8e34ac"],"layout":"IPY_MODEL_a3ab35ed293e4cf0b35e8b642f61599e"}},"f0a0128845824466b38f7e67ade1e0fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6add07bdf462417297d5afd202c93d6c","placeholder":"","style":"IPY_MODEL_cf7d243972594323ac7c0b9f0ca59383","value":"tokenizer_config.json:100%"}},"b05cd68ad8ce4579af42e05e9b246120":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3954f9f8514d4f289701f596348d50c0","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_214f97275df84d2dae1245c2bdab1db2","value":48}},"0fc6c4a041634f9785d1dcf72b8e34ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98cff2c6201c4b3f85bdf3769a769485","placeholder":"","style":"IPY_MODEL_2fd312b2e53e45efbaad06e0ea3df351","value":"48.0/48.0[00:00&lt;00:00,1.06kB/s]"}},"a3ab35ed293e4cf0b35e8b642f61599e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6add07bdf462417297d5afd202c93d6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf7d243972594323ac7c0b9f0ca59383":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3954f9f8514d4f289701f596348d50c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"214f97275df84d2dae1245c2bdab1db2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98cff2c6201c4b3f85bdf3769a769485":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fd312b2e53e45efbaad06e0ea3df351":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"359eb80da67e470690aa89f1387c9fe2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_abaa155c8b69409398af221172708264","IPY_MODEL_a060d198587a42bcb82311e1fc8a7f72","IPY_MODEL_7103bb87a939402ab10b9cd471cf11d4"],"layout":"IPY_MODEL_b9684873fa814b259fcc45d7ac6e513c"}},"abaa155c8b69409398af221172708264":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93c37e6525e24b099987114fb63f590f","placeholder":"","style":"IPY_MODEL_9fd42fdf6c72408fadd58e1b3d4aa9d0","value":"vocab.txt:100%"}},"a060d198587a42bcb82311e1fc8a7f72":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bb4c6e4f9354b5389df4a5925e5f71b","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_260208633ba8402cbb507202240bcd5f","value":231508}},"7103bb87a939402ab10b9cd471cf11d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_029d74814ed445a3a1a631edf65221ca","placeholder":"","style":"IPY_MODEL_c8a50b03eb814230b236c72b9fda9fff","value":"232k/232k[00:00&lt;00:00,1.68MB/s]"}},"b9684873fa814b259fcc45d7ac6e513c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93c37e6525e24b099987114fb63f590f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fd42fdf6c72408fadd58e1b3d4aa9d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bb4c6e4f9354b5389df4a5925e5f71b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"260208633ba8402cbb507202240bcd5f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"029d74814ed445a3a1a631edf65221ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8a50b03eb814230b236c72b9fda9fff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81d82c3406b84cc884ef754d0303ca03":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_633bb44c2c6e4a3db16146654f330629","IPY_MODEL_19c815c4eba041ea9e89e48d9f87fb16","IPY_MODEL_451b7bde5f504589bb565fcbf38088ab"],"layout":"IPY_MODEL_d9fc32b9aacb466c9265a938b28353ca"}},"633bb44c2c6e4a3db16146654f330629":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c787a747c6994d768e70b46f13ad62e1","placeholder":"","style":"IPY_MODEL_2f8d15da078f48be9bf2816254f4ae54","value":"tokenizer.json:100%"}},"19c815c4eba041ea9e89e48d9f87fb16":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce1644c0309946c5aa5b416c71e51e53","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2419fb398db45b284722821170a088c","value":466062}},"451b7bde5f504589bb565fcbf38088ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_923ef2e8a978461f90474ebd90edc9e3","placeholder":"","style":"IPY_MODEL_a060fd95a786479db811e2d334ad14e0","value":"466k/466k[00:00&lt;00:00,4.58MB/s]"}},"d9fc32b9aacb466c9265a938b28353ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c787a747c6994d768e70b46f13ad62e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f8d15da078f48be9bf2816254f4ae54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce1644c0309946c5aa5b416c71e51e53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2419fb398db45b284722821170a088c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"923ef2e8a978461f90474ebd90edc9e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a060fd95a786479db811e2d334ad14e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07f833e296d44a2b99d114f5d6414501":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e97fc197ca44e4fa19733d40a5c151c","IPY_MODEL_42b17db690774f549b38cc5c15f4ea71","IPY_MODEL_4e6891157b344fc6b4bb6fdc8c4ada10"],"layout":"IPY_MODEL_ab3cda59859043918a273405acfa00f0"}},"7e97fc197ca44e4fa19733d40a5c151c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4faec5f1f0354cf99f4b5b6fc75b916c","placeholder":"","style":"IPY_MODEL_841934a303094c9eb8af6abed006ebf4","value":"config.json:100%"}},"42b17db690774f549b38cc5c15f4ea71":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_104d696b322e40d5b5c607a5ceaa43b6","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a73eaca9a05241419b565db0a13af251","value":570}},"4e6891157b344fc6b4bb6fdc8c4ada10":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3689d76da3f54fd5a8c4f6ff7646d139","placeholder":"","style":"IPY_MODEL_63142bd5c4764761b13faa1535d24af1","value":"570/570[00:00&lt;00:00,15.8kB/s]"}},"ab3cda59859043918a273405acfa00f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4faec5f1f0354cf99f4b5b6fc75b916c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"841934a303094c9eb8af6abed006ebf4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"104d696b322e40d5b5c607a5ceaa43b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a73eaca9a05241419b565db0a13af251":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3689d76da3f54fd5a8c4f6ff7646d139":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63142bd5c4764761b13faa1535d24af1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00c12cbfe6624be19ce9ca5f496408ff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_647b6be741e64bc4b11db3d7792e8f4e","IPY_MODEL_30ffdb2f52914d15ae57a1ec043ae2d3","IPY_MODEL_089acced28654f75ad82edfc31cf5c11"],"layout":"IPY_MODEL_f5bfb25b2b3340a29f1ab4474dfef345"}},"647b6be741e64bc4b11db3d7792e8f4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_769b8dbd8aa540efaae877237f751784","placeholder":"","style":"IPY_MODEL_fff94b5e566346b189805b7c69aceb39","value":"model.safetensors:100%"}},"30ffdb2f52914d15ae57a1ec043ae2d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9794f36184ed426691d1217e544c0332","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b196a725cca84208818ebcbd0724a313","value":440449768}},"089acced28654f75ad82edfc31cf5c11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_149260945f084d2899fa3edbd22bc159","placeholder":"","style":"IPY_MODEL_b80a43a71b774c1ba22da762647c119f","value":"440M/440M[00:04&lt;00:00,109MB/s]"}},"f5bfb25b2b3340a29f1ab4474dfef345":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"769b8dbd8aa540efaae877237f751784":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fff94b5e566346b189805b7c69aceb39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9794f36184ed426691d1217e544c0332":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b196a725cca84208818ebcbd0724a313":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"149260945f084d2899fa3edbd22bc159":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b80a43a71b774c1ba22da762647c119f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d490420e8fbf474dbc2f76f890309465":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3a3e5e2ab04f4438873f3936f9dfccb9","IPY_MODEL_fa6896db5d314f95921b5692f47500c2","IPY_MODEL_3986740808eb46c28d06f4b42b8085c6"],"layout":"IPY_MODEL_a01330ee600546be82dadb996a13e816"}},"3a3e5e2ab04f4438873f3936f9dfccb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec21943214cc4282aaa25531ecac9f79","placeholder":"","style":"IPY_MODEL_835b6048d31d4840b4aa5ef295790547","value":"tokenizer_config.json:100%"}},"fa6896db5d314f95921b5692f47500c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_912858ec723d4f1a92d2691c3242dfc7","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd32c19751944f8a919a51000085a388","value":25}},"3986740808eb46c28d06f4b42b8085c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ce8313b718b4389ae7c3a9396fca041","placeholder":"","style":"IPY_MODEL_eaf84a3cdafa4f958ee63de760b1819d","value":"25.0/25.0[00:00&lt;00:00,1.47kB/s]"}},"a01330ee600546be82dadb996a13e816":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec21943214cc4282aaa25531ecac9f79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"835b6048d31d4840b4aa5ef295790547":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"912858ec723d4f1a92d2691c3242dfc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd32c19751944f8a919a51000085a388":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2ce8313b718b4389ae7c3a9396fca041":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eaf84a3cdafa4f958ee63de760b1819d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54625e12ab55488c9582b83d04921d0c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_df0ecd89c0894bd59e3cada0903c9e5f","IPY_MODEL_cf9ab924e86943cab7c723d6d549e846","IPY_MODEL_74cb2354a6794fbd9e12a3f32dfee2a5"],"layout":"IPY_MODEL_8bdd1f4b146241b19a4077abe4023ff3"}},"df0ecd89c0894bd59e3cada0903c9e5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e10106c56c8042ba9cd0fc652033d43b","placeholder":"","style":"IPY_MODEL_ba5022f9512e4ca0bfed2b70579cf588","value":"config.json:100%"}},"cf9ab924e86943cab7c723d6d549e846":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_789b18bef4b0448e8ec208f2ce62b1f5","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39ed0bbb2c4741249d4e6cd37b11839c","value":481}},"74cb2354a6794fbd9e12a3f32dfee2a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50ca3868a5a949118f721b274d7c53a7","placeholder":"","style":"IPY_MODEL_fe9624e7658b4cd9828a744758006a88","value":"481/481[00:00&lt;00:00,30.8kB/s]"}},"8bdd1f4b146241b19a4077abe4023ff3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e10106c56c8042ba9cd0fc652033d43b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba5022f9512e4ca0bfed2b70579cf588":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"789b18bef4b0448e8ec208f2ce62b1f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39ed0bbb2c4741249d4e6cd37b11839c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50ca3868a5a949118f721b274d7c53a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe9624e7658b4cd9828a744758006a88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f619f8de8b6a428a8050aed707edae2b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ac2f00e0a4949d6ad2057df4555288a","IPY_MODEL_74525066746b482b99de9fb68ff62b9f","IPY_MODEL_2be98ae5f2c9478caef7547436ca4015"],"layout":"IPY_MODEL_469f6e1dc711410983af8b5fd3608372"}},"0ac2f00e0a4949d6ad2057df4555288a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e3d32a8051b4859862f151a31353df8","placeholder":"","style":"IPY_MODEL_27e6813023d5460e9edde49e72643bcd","value":"vocab.json:100%"}},"74525066746b482b99de9fb68ff62b9f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_08183e2a56354ccbb392a234cd5603aa","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f02a19fa90f749b69c553055435e69d0","value":898823}},"2be98ae5f2c9478caef7547436ca4015":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a790a386374340d8a521643b73ab0c8c","placeholder":"","style":"IPY_MODEL_8215d52a466f40c9b15f978a64580a9b","value":"899k/899k[00:00&lt;00:00,6.37MB/s]"}},"469f6e1dc711410983af8b5fd3608372":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e3d32a8051b4859862f151a31353df8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27e6813023d5460e9edde49e72643bcd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08183e2a56354ccbb392a234cd5603aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f02a19fa90f749b69c553055435e69d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a790a386374340d8a521643b73ab0c8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8215d52a466f40c9b15f978a64580a9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b0b54959b2c434482f59a255980d12c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11120604789b42f3b19d0acbfd35dd68","IPY_MODEL_2bc7165986514288ae25ab26cc0eee1b","IPY_MODEL_2aae06b7a1b448b0ae6f36671c4f26ec"],"layout":"IPY_MODEL_56f8af52ec3b40b08b80be446cd4e64b"}},"11120604789b42f3b19d0acbfd35dd68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbff215f43744ac48d55fa472e84f447","placeholder":"","style":"IPY_MODEL_aa8f88c140c04dfca1a52776beb5ad21","value":"merges.txt:100%"}},"2bc7165986514288ae25ab26cc0eee1b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8656fbde05be4b87975cd0a604e84fa2","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c16685b40ae94e78b5894d774bea113c","value":456318}},"2aae06b7a1b448b0ae6f36671c4f26ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26fce69f81df4b2482f59742bbbf458f","placeholder":"","style":"IPY_MODEL_9bdb54e88fae4728987331b854835d2f","value":"456k/456k[00:00&lt;00:00,6.01MB/s]"}},"56f8af52ec3b40b08b80be446cd4e64b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbff215f43744ac48d55fa472e84f447":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa8f88c140c04dfca1a52776beb5ad21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8656fbde05be4b87975cd0a604e84fa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c16685b40ae94e78b5894d774bea113c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"26fce69f81df4b2482f59742bbbf458f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bdb54e88fae4728987331b854835d2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d851fb5920144013b4ae89cf97a2e7ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72fd1255f84d461d936c5228fc0b11f1","IPY_MODEL_9cf969bb9c884987981b48a1ea44df6c","IPY_MODEL_c25060701200461ba8710b7b24120da1"],"layout":"IPY_MODEL_9a4f83ce884c43f5934776954fabd51d"}},"72fd1255f84d461d936c5228fc0b11f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_609d4ed6d4be4aebba48fed7ecd72d53","placeholder":"","style":"IPY_MODEL_c404f35e7dc9415ca0d3c8d52993d940","value":"tokenizer.json:100%"}},"9cf969bb9c884987981b48a1ea44df6c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff4fd8e475a647528ce9d9f4fd4c5d14","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e5d67c8871b4a42b6d9ce37aec3451c","value":1355863}},"c25060701200461ba8710b7b24120da1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_819181b9cc2a4ad0b139e2ac97d02105","placeholder":"","style":"IPY_MODEL_9944dd38937244368b135ce8e911dd81","value":"1.36M/1.36M[00:00&lt;00:00,14.0MB/s]"}},"9a4f83ce884c43f5934776954fabd51d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"609d4ed6d4be4aebba48fed7ecd72d53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c404f35e7dc9415ca0d3c8d52993d940":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff4fd8e475a647528ce9d9f4fd4c5d14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e5d67c8871b4a42b6d9ce37aec3451c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"819181b9cc2a4ad0b139e2ac97d02105":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9944dd38937244368b135ce8e911dd81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8599a94aaca640d580dc18898f0039d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_05256dfdf6f54bc38c76e37e0621f3de","IPY_MODEL_b6c26d7dc0ed40188b58222bc98fa557","IPY_MODEL_3746f7f1060d455c92878f34f5601627"],"layout":"IPY_MODEL_c64a52785b7f42f0b0e71f2bcf4fe847"}},"05256dfdf6f54bc38c76e37e0621f3de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e412d0c8ffb4a079c36f32fa580c1dc","placeholder":"","style":"IPY_MODEL_59d2920d85444d8184b340815b142e47","value":"model.safetensors:100%"}},"b6c26d7dc0ed40188b58222bc98fa557":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ec21578ed8549ca902bb4ce98a76c73","max":498818054,"min":0,"orientation":"horizontal","style":"IPY_MODEL_287b5823d4fc46529299e3bd96dfba23","value":498818054}},"3746f7f1060d455c92878f34f5601627":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86ba71cf76824675ba288cd2bbe8b1e4","placeholder":"","style":"IPY_MODEL_bfa538a574e645f38494cdc47bedb88b","value":"499M/499M[00:05&lt;00:00,168MB/s]"}},"c64a52785b7f42f0b0e71f2bcf4fe847":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e412d0c8ffb4a079c36f32fa580c1dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59d2920d85444d8184b340815b142e47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ec21578ed8549ca902bb4ce98a76c73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"287b5823d4fc46529299e3bd96dfba23":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86ba71cf76824675ba288cd2bbe8b1e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfa538a574e645f38494cdc47bedb88b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6789a06dd4e6459c9dfd9d4f7348423f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c5282bc687e4be2a698fd4b2d70fb6e","IPY_MODEL_e2f181dd18824461bac289f1d810f926","IPY_MODEL_457b9d05610d45f681054728123923b1"],"layout":"IPY_MODEL_66b5f310dc4a480bb9fc4774ad4dfefd"}},"5c5282bc687e4be2a698fd4b2d70fb6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1a3b40b99b943e89c22a2c836ebf056","placeholder":"","style":"IPY_MODEL_1603cdf5244f47ab9e468dcbbb2c0393","value":"Downloadingbuilderscript:100%"}},"e2f181dd18824461bac289f1d810f926":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_62bf1c9001634bb68604754954d32e77","max":6288,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3789bf4cf59c40fbb87297803425fc0e","value":6288}},"457b9d05610d45f681054728123923b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c39fa26e87b4fd3858c8145d65dd4f6","placeholder":"","style":"IPY_MODEL_ceeb699bcf5744e7a95823da1ca719fc","value":"6.29k/6.29k[00:00&lt;00:00,152kB/s]"}},"66b5f310dc4a480bb9fc4774ad4dfefd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1a3b40b99b943e89c22a2c836ebf056":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1603cdf5244f47ab9e468dcbbb2c0393":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62bf1c9001634bb68604754954d32e77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3789bf4cf59c40fbb87297803425fc0e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1c39fa26e87b4fd3858c8145d65dd4f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ceeb699bcf5744e7a95823da1ca719fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"380a88cb4d964393a57f2ec8ba576393":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34030bfd299c43f7b0bafc166e4d22f3","IPY_MODEL_45da0a23ecdd4393b5e026bcd5330ae5","IPY_MODEL_4167af55dd6b4c97b8e7b253ee887d15"],"layout":"IPY_MODEL_9472874bd55e4e618ee26e6ff83c2ff6"}},"34030bfd299c43f7b0bafc166e4d22f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3b7a923975c432cbd36637dc35e282c","placeholder":"","style":"IPY_MODEL_1ce65a7af59d43ebb2b203b258ad1654","value":"Downloadingreadme:100%"}},"45da0a23ecdd4393b5e026bcd5330ae5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b7a5e326d094d3e87d171c3c7a51488","max":10566,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c3e7b15abc94a0da89b76ae376954fb","value":10566}},"4167af55dd6b4c97b8e7b253ee887d15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b73df2a3262e406580fce04dcc0de490","placeholder":"","style":"IPY_MODEL_ed9e644b137c4b21a1ebd213e0953d30","value":"10.6k/10.6k[00:00&lt;00:00,173kB/s]"}},"9472874bd55e4e618ee26e6ff83c2ff6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3b7a923975c432cbd36637dc35e282c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ce65a7af59d43ebb2b203b258ad1654":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b7a5e326d094d3e87d171c3c7a51488":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c3e7b15abc94a0da89b76ae376954fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b73df2a3262e406580fce04dcc0de490":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed9e644b137c4b21a1ebd213e0953d30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47ce7f4bd4e0479a978fcf3d4c94dc8d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6bb198802df4de7874faa9a5291449b","IPY_MODEL_9933f2e4adad40719e4f68237edd109c","IPY_MODEL_84df407f2364487ea5e45537c0bb775b"],"layout":"IPY_MODEL_8e55f1b7a92e459e8cb5e5a9c10f44e7"}},"a6bb198802df4de7874faa9a5291449b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62ceb250c08b47bebae6ad10865e88e1","placeholder":"","style":"IPY_MODEL_cc75f116f0514ca0b9d4a1e9d47ae234","value":"Downloadingdata:100%"}},"9933f2e4adad40719e4f68237edd109c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8a43148bfaf479dae12f850c81ec685","max":5975590,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a1a7efb909d1419496891f61eadf8df8","value":5975590}},"84df407f2364487ea5e45537c0bb775b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f271bfcbeacb434d995d7403ec6d3019","placeholder":"","style":"IPY_MODEL_94780ae2789243ac80404eb103632dc5","value":"5.98M/5.98M[00:00&lt;00:00,12.4MB/s]"}},"8e55f1b7a92e459e8cb5e5a9c10f44e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62ceb250c08b47bebae6ad10865e88e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc75f116f0514ca0b9d4a1e9d47ae234":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8a43148bfaf479dae12f850c81ec685":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1a7efb909d1419496891f61eadf8df8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f271bfcbeacb434d995d7403ec6d3019":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94780ae2789243ac80404eb103632dc5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"257878e0e07b4e969078cb700e970e80":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_528c27c967de4e9fb39eb92cea986527","IPY_MODEL_bb8763067394466c9aedb49f25182da7","IPY_MODEL_6551eb2da31b4d04ae1d887b69455b84"],"layout":"IPY_MODEL_85f0e54af57c4acaa1241b295da84a81"}},"528c27c967de4e9fb39eb92cea986527":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a25f3444232646ec98e0b2fe24ceb81a","placeholder":"","style":"IPY_MODEL_5eb233454371405a96c03c41849cb176","value":"Generatingtrainsplit:100%"}},"bb8763067394466c9aedb49f25182da7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bac8e2da8094fdcadfcd1485776fdc2","max":6838,"min":0,"orientation":"horizontal","style":"IPY_MODEL_129161b50b08484aa9c43ff32a49f047","value":6838}},"6551eb2da31b4d04ae1d887b69455b84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c857e88b5e14d719ac8f0ef3d661478","placeholder":"","style":"IPY_MODEL_8651528f272445339114eacf67f8b289","value":"6838/6838[00:01&lt;00:00,4408.90examples/s]"}},"85f0e54af57c4acaa1241b295da84a81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a25f3444232646ec98e0b2fe24ceb81a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eb233454371405a96c03c41849cb176":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bac8e2da8094fdcadfcd1485776fdc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"129161b50b08484aa9c43ff32a49f047":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c857e88b5e14d719ac8f0ef3d661478":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8651528f272445339114eacf67f8b289":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d24a6ed90e54af896595898fdad953e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_674f011b83184f158d8f132267c44df4","IPY_MODEL_1c730557132949b794684989d6e6856e","IPY_MODEL_7cd755facbbf4734a282ae513d257ddc"],"layout":"IPY_MODEL_9e290699f47444d5b20526243765d0d9"}},"674f011b83184f158d8f132267c44df4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cd1b59331c3427eb8cfa6d968f627c9","placeholder":"","style":"IPY_MODEL_14c4794ee26748908471fed5d26a2f99","value":"Generatingtestsplit:100%"}},"1c730557132949b794684989d6e6856e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9e24d21cc3146b4a0ac6ff1c58a2b10","max":3259,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07bdffb0a1d84dea921d645f25679716","value":3259}},"7cd755facbbf4734a282ae513d257ddc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7210a6d284b042e181a0da4eddea4a32","placeholder":"","style":"IPY_MODEL_20b2366495624c0ca637c335ccbd2bdd","value":"3259/3259[00:00&lt;00:00,3640.25examples/s]"}},"9e290699f47444d5b20526243765d0d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cd1b59331c3427eb8cfa6d968f627c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14c4794ee26748908471fed5d26a2f99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9e24d21cc3146b4a0ac6ff1c58a2b10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07bdffb0a1d84dea921d645f25679716":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7210a6d284b042e181a0da4eddea4a32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20b2366495624c0ca637c335ccbd2bdd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"225d5e4fbd2142f8b349d5c651fd2b91":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5d7ab9c72ba646519738c2c14ffef747","IPY_MODEL_6e6bc71e12934345ad88c8a5764c39d4","IPY_MODEL_1799de1201f54d588ad1a276de48f4d4"],"layout":"IPY_MODEL_318769065b454c31ba3129ae8e4b80ff"}},"5d7ab9c72ba646519738c2c14ffef747":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0113ffc04db44799e9e6616e3543d4c","placeholder":"","style":"IPY_MODEL_8edd4fd579b54752a1dd63140677cca1","value":"Generatingvalidationsplit:100%"}},"6e6bc71e12934345ad88c8a5764c39d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_354ac2defe6b415199d862344b592d16","max":886,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d53ecbeb63c94bc6bf17a61a703eae2d","value":886}},"1799de1201f54d588ad1a276de48f4d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8a26ce96d6244049e3287a51d577f06","placeholder":"","style":"IPY_MODEL_90b962b75cb04c7db31b5832b8b85538","value":"886/886[00:00&lt;00:00,3178.41examples/s]"}},"318769065b454c31ba3129ae8e4b80ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0113ffc04db44799e9e6616e3543d4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8edd4fd579b54752a1dd63140677cca1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"354ac2defe6b415199d862344b592d16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d53ecbeb63c94bc6bf17a61a703eae2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8a26ce96d6244049e3287a51d577f06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90b962b75cb04c7db31b5832b8b85538":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fe2e23aca7f4244a188a7fded063157":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64d7d75a8f084a0da52c65cd8020acdf","IPY_MODEL_084a891323a140d9bc357e739c2748e1","IPY_MODEL_ad155b6ec9e84265ac96e200a1568b37"],"layout":"IPY_MODEL_cfc3a9db4924456591ceae0708c85af4"}},"64d7d75a8f084a0da52c65cd8020acdf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15af3c6c62594fe7b42270e257d1401c","placeholder":"","style":"IPY_MODEL_a80c37d6a97a489bb52d1d021b0ad702","value":"tokenizer_config.json:100%"}},"084a891323a140d9bc357e739c2748e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ac6bd387927483ca93c8d5d9a1f2721","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d6cb909d73747018bd58dbbb2e7e9d6","value":48}},"ad155b6ec9e84265ac96e200a1568b37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bea77c5910c44fe85cb3681e97566aa","placeholder":"","style":"IPY_MODEL_8e06aee840cb4213bdafd2c52dff50a6","value":"48.0/48.0[00:00&lt;00:00,1.14kB/s]"}},"cfc3a9db4924456591ceae0708c85af4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15af3c6c62594fe7b42270e257d1401c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a80c37d6a97a489bb52d1d021b0ad702":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ac6bd387927483ca93c8d5d9a1f2721":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d6cb909d73747018bd58dbbb2e7e9d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1bea77c5910c44fe85cb3681e97566aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e06aee840cb4213bdafd2c52dff50a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1aec21b6434b4f53808ffc79f5149d85":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f757c6cea36d45958a337d027a805e37","IPY_MODEL_0da92ab01daa46ef950a4601dfcc2895","IPY_MODEL_845c05041d264ca28a00517a1c5b11a7"],"layout":"IPY_MODEL_70a16c9055ea4ee097093aab4dd45bc8"}},"f757c6cea36d45958a337d027a805e37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_655b3ac7982146e7aee73b6cb280264a","placeholder":"","style":"IPY_MODEL_e083ca4c1c99478f85b0eb41a3e6470d","value":"vocab.txt:100%"}},"0da92ab01daa46ef950a4601dfcc2895":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_374b53c7f7414a09bf1ca0e5ebb0fc81","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5bcc2782dccf43a5ad65a068575d08cc","value":231508}},"845c05041d264ca28a00517a1c5b11a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfe8758be1ad498489da7c905da2bfb8","placeholder":"","style":"IPY_MODEL_ac8745f811f54cc9adbca6079c889961","value":"232k/232k[00:00&lt;00:00,1.40MB/s]"}},"70a16c9055ea4ee097093aab4dd45bc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"655b3ac7982146e7aee73b6cb280264a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e083ca4c1c99478f85b0eb41a3e6470d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"374b53c7f7414a09bf1ca0e5ebb0fc81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bcc2782dccf43a5ad65a068575d08cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cfe8758be1ad498489da7c905da2bfb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac8745f811f54cc9adbca6079c889961":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b22b945f90ba45229af24f8e63460c7f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b84bd1844094118b142ab5e93a905a6","IPY_MODEL_f78780bace364622aa14826e2be40f45","IPY_MODEL_ef8b297c4dd04062aef1590c0c58b881"],"layout":"IPY_MODEL_4dd79f81753545efa036039b41cdb4b7"}},"6b84bd1844094118b142ab5e93a905a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dadf70b10804b15b620204ddb7b3b81","placeholder":"","style":"IPY_MODEL_31e1c6e478db439caea9cfa984d781e9","value":"tokenizer.json:100%"}},"f78780bace364622aa14826e2be40f45":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef14345728044f71a34d798ef3be962d","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6df23e4f031491b97c932e37743f969","value":466062}},"ef8b297c4dd04062aef1590c0c58b881":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cc4fa7a7ae04fc8bf65e0caf5d402d6","placeholder":"","style":"IPY_MODEL_9a08886fbad24357a7bd8cd0e9e0b0f0","value":"466k/466k[00:00&lt;00:00,1.91MB/s]"}},"4dd79f81753545efa036039b41cdb4b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dadf70b10804b15b620204ddb7b3b81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31e1c6e478db439caea9cfa984d781e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef14345728044f71a34d798ef3be962d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6df23e4f031491b97c932e37743f969":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7cc4fa7a7ae04fc8bf65e0caf5d402d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a08886fbad24357a7bd8cd0e9e0b0f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b59d09ea68424bdba82cb07a87fb7e43":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b81077db5ba543eab0a4f36f8fe5ae15","IPY_MODEL_8bf58d66300b4aa0b84eeca1fba14025","IPY_MODEL_979bfbd608c84d5788df47de736f225f"],"layout":"IPY_MODEL_c1f3dc716f2346de99d7cc84a6eb4035"}},"b81077db5ba543eab0a4f36f8fe5ae15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e067649cd933415aa2f7c613bb669166","placeholder":"","style":"IPY_MODEL_1f88e602902c4537adfb3cc5e5894090","value":"config.json:100%"}},"8bf58d66300b4aa0b84eeca1fba14025":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e7281e294d4453b98f838e54e241658","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f842fff323e6405c8f145341670f1007","value":570}},"979bfbd608c84d5788df47de736f225f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e99222c5a9214cb69bc91a0d15911e3b","placeholder":"","style":"IPY_MODEL_c6d1705c41f84dedbd8993564e1cfc8a","value":"570/570[00:00&lt;00:00,16.8kB/s]"}},"c1f3dc716f2346de99d7cc84a6eb4035":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e067649cd933415aa2f7c613bb669166":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f88e602902c4537adfb3cc5e5894090":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e7281e294d4453b98f838e54e241658":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f842fff323e6405c8f145341670f1007":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e99222c5a9214cb69bc91a0d15911e3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6d1705c41f84dedbd8993564e1cfc8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11537150b6fb4d6383ea66da5f585a01":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3a650064d11142e0a0272445ad3edaf1","IPY_MODEL_867d391a45514c4abdd2a22506c9e1e1","IPY_MODEL_ae1b5ad1641848a4829a45593dff2a77"],"layout":"IPY_MODEL_7c5d513e9d7b4d60b0bf4723d3277033"}},"3a650064d11142e0a0272445ad3edaf1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a97ca5e1e222415eaaa5a0c84085617b","placeholder":"","style":"IPY_MODEL_f130cad6afeb452c98dde002615df790","value":"model.safetensors:100%"}},"867d391a45514c4abdd2a22506c9e1e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ca1c819a9684ee88835ff85f4dccf63","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bdfb27a3cd1846b0a9cea66662b3f3a9","value":440449768}},"ae1b5ad1641848a4829a45593dff2a77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6a0613d4fb041e4a706837d90e16a13","placeholder":"","style":"IPY_MODEL_13eeba59a4ed45fbb1120b58cfc62498","value":"440M/440M[00:04&lt;00:00,71.2MB/s]"}},"7c5d513e9d7b4d60b0bf4723d3277033":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a97ca5e1e222415eaaa5a0c84085617b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f130cad6afeb452c98dde002615df790":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ca1c819a9684ee88835ff85f4dccf63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdfb27a3cd1846b0a9cea66662b3f3a9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6a0613d4fb041e4a706837d90e16a13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13eeba59a4ed45fbb1120b58cfc62498":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ffec70c17ae440a93d4958c9483f599":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9884bd2a6fd14a1fafcbdd19e9c61eb4","IPY_MODEL_605455bfec134bb7bd665e60d63705b1","IPY_MODEL_ae455097ab29484697d9f52a9750d925"],"layout":"IPY_MODEL_f984811d6eea4771be638803644102b0"}},"9884bd2a6fd14a1fafcbdd19e9c61eb4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea6bf797f9344ac789b32491a9b94dbf","placeholder":"","style":"IPY_MODEL_451ea0d4da3540d888ddde67a9a0b2f2","value":"tokenizer_config.json:100%"}},"605455bfec134bb7bd665e60d63705b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1aa46fa1ad874ff0a900a60086a5a88f","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e9338ac654ad47eab728f3c8e75e7f9d","value":25}},"ae455097ab29484697d9f52a9750d925":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fa9470f8fd54b8aac0963c445a83511","placeholder":"","style":"IPY_MODEL_0d328f1d995643efbf009430ffc4e063","value":"25.0/25.0[00:00&lt;00:00,835B/s]"}},"f984811d6eea4771be638803644102b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea6bf797f9344ac789b32491a9b94dbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"451ea0d4da3540d888ddde67a9a0b2f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1aa46fa1ad874ff0a900a60086a5a88f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9338ac654ad47eab728f3c8e75e7f9d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fa9470f8fd54b8aac0963c445a83511":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d328f1d995643efbf009430ffc4e063":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f5233f7a85b4cc5b694c808228da264":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7285d68e9da345a487fcd6202a1746b7","IPY_MODEL_cf5d8f5123d4456087ce5cb2573c0c24","IPY_MODEL_19ab87b9349c4015b78fdb3ed73e1110"],"layout":"IPY_MODEL_887698bb0bf84241bb5988cc6bf24f05"}},"7285d68e9da345a487fcd6202a1746b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a97495399adc4589a090f6263dec8919","placeholder":"","style":"IPY_MODEL_9202ee81c3084a0a99b8b505a5181fd7","value":"config.json:100%"}},"cf5d8f5123d4456087ce5cb2573c0c24":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8725afcbc3d4f2c953431ee1faee1c6","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c1a446ac8fe4fb8acbf94e5a8f5575e","value":481}},"19ab87b9349c4015b78fdb3ed73e1110":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e2cd25a56d0447cb8e1480b022f96a0","placeholder":"","style":"IPY_MODEL_fa73950001a24716a9f3a295f38de8fb","value":"481/481[00:00&lt;00:00,19.3kB/s]"}},"887698bb0bf84241bb5988cc6bf24f05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a97495399adc4589a090f6263dec8919":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9202ee81c3084a0a99b8b505a5181fd7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8725afcbc3d4f2c953431ee1faee1c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c1a446ac8fe4fb8acbf94e5a8f5575e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e2cd25a56d0447cb8e1480b022f96a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa73950001a24716a9f3a295f38de8fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e268a01a2444bc696a0f53bcc2867e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b020b789533e4c748129e1e83001c99c","IPY_MODEL_a4d9c62eb6764f08b5ab6c096d8dbd60","IPY_MODEL_751b05f75a214b5ea69f682e3fd94576"],"layout":"IPY_MODEL_26d50baa9a63492cabddb50db4624ac4"}},"b020b789533e4c748129e1e83001c99c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fa412d9b8d7490ea7618292fe34746f","placeholder":"","style":"IPY_MODEL_e0b96835ed4f47cb912e851d4e3c3402","value":"vocab.json:100%"}},"a4d9c62eb6764f08b5ab6c096d8dbd60":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de7ef3c29ae14c92a1fbe0cad66f1731","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_12771d84b0b6482c8c4b91004d30df2e","value":898823}},"751b05f75a214b5ea69f682e3fd94576":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3d9bc1074144e24a1a0766a88517514","placeholder":"","style":"IPY_MODEL_ff4a02c5640e424b83dad460e52ca58a","value":"899k/899k[00:00&lt;00:00,2.76MB/s]"}},"26d50baa9a63492cabddb50db4624ac4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fa412d9b8d7490ea7618292fe34746f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0b96835ed4f47cb912e851d4e3c3402":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de7ef3c29ae14c92a1fbe0cad66f1731":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12771d84b0b6482c8c4b91004d30df2e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3d9bc1074144e24a1a0766a88517514":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff4a02c5640e424b83dad460e52ca58a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1969b2ffa0fa412d944a807d8c3e93c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a4614bf86ca4130a3f0faa23ae8c6d1","IPY_MODEL_86c3120a0b9c4955b2a51743150cf260","IPY_MODEL_d2872075fc0242c5a0c436ce451af7c4"],"layout":"IPY_MODEL_74e1f84077eb44559ebeb96500b6770d"}},"7a4614bf86ca4130a3f0faa23ae8c6d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_856d320562244d0e9979074676600521","placeholder":"","style":"IPY_MODEL_d2f9d2f5a7514b3fa6e9211a5b9c397b","value":"merges.txt:100%"}},"86c3120a0b9c4955b2a51743150cf260":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f005824fef4849acac5979dec9f39e96","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_233ed0774f3e45c9a3f2382ae74f833d","value":456318}},"d2872075fc0242c5a0c436ce451af7c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b17c6e7900054fee808f6bd1db201b8b","placeholder":"","style":"IPY_MODEL_f09db17c39524b0eac1ccda8ad078dd9","value":"456k/456k[00:00&lt;00:00,2.83MB/s]"}},"74e1f84077eb44559ebeb96500b6770d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"856d320562244d0e9979074676600521":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2f9d2f5a7514b3fa6e9211a5b9c397b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f005824fef4849acac5979dec9f39e96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"233ed0774f3e45c9a3f2382ae74f833d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b17c6e7900054fee808f6bd1db201b8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f09db17c39524b0eac1ccda8ad078dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f777898e3af46dda86978a3391f6f4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6b975d730a443139c1001b89a66ade2","IPY_MODEL_adabe25462b248ad9463aadba525be1b","IPY_MODEL_8786bf2e12134a8a8748b1d62ee73743"],"layout":"IPY_MODEL_c86fa111794c4d96b2d73032c938477f"}},"b6b975d730a443139c1001b89a66ade2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07920d19fc6e4458a492338865db8b7d","placeholder":"","style":"IPY_MODEL_37f29c16ff6a4ddab96b0dfbeaef9d2b","value":"tokenizer.json:100%"}},"adabe25462b248ad9463aadba525be1b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c811c4a3bc8d472baacd767236bdbd74","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57d96ce946644547af55196f8ae2930d","value":1355863}},"8786bf2e12134a8a8748b1d62ee73743":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8aa4fb7ac6e64fb296037f15db811751","placeholder":"","style":"IPY_MODEL_77acfe065816484789ee2b787dc121ad","value":"1.36M/1.36M[00:00&lt;00:00,15.7MB/s]"}},"c86fa111794c4d96b2d73032c938477f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07920d19fc6e4458a492338865db8b7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37f29c16ff6a4ddab96b0dfbeaef9d2b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c811c4a3bc8d472baacd767236bdbd74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57d96ce946644547af55196f8ae2930d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8aa4fb7ac6e64fb296037f15db811751":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77acfe065816484789ee2b787dc121ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f1254d9a5284a4483e71610d8fca2af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10331b025fb449978944f17b68a5b1e6","IPY_MODEL_46c3c9b6a124459fbf4480be58e0e20d","IPY_MODEL_d3d4fab526fe4a69a9471c4c688c855b"],"layout":"IPY_MODEL_3c94f76efeb04b748233f65e26875c9c"}},"10331b025fb449978944f17b68a5b1e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa06dae160ef48238e771f9c8dd96094","placeholder":"","style":"IPY_MODEL_6312dd219f544cc3bfbb1bb684bbec64","value":"model.safetensors:100%"}},"46c3c9b6a124459fbf4480be58e0e20d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7023865968340249e47dcd126104781","max":498818054,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e027c9e909064d109d43ca6443461867","value":498818054}},"d3d4fab526fe4a69a9471c4c688c855b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45d252ab38d64b3097f2cfd1a4deeb86","placeholder":"","style":"IPY_MODEL_201b511579b949dd95c667a26238f52d","value":"499M/499M[00:03&lt;00:00,190MB/s]"}},"3c94f76efeb04b748233f65e26875c9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa06dae160ef48238e771f9c8dd96094":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6312dd219f544cc3bfbb1bb684bbec64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7023865968340249e47dcd126104781":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e027c9e909064d109d43ca6443461867":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45d252ab38d64b3097f2cfd1a4deeb86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"201b511579b949dd95c667a26238f52d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}