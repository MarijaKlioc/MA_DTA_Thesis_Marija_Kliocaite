{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"cQSb4y2-FUSP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723889708922,"user_tz":-120,"elapsed":23388,"user":{"displayName":"Eugenijus Kliocas","userId":"11927212131385663608"}},"outputId":"9cc9fe73-5fc3-4f95-ebef-ddb8aae8df09"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m898.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["%pip install datasets transformers --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKPJ94b6ApvN"},"outputs":[],"source":["import sklearn\n","import pandas as pd\n","import numpy as np\n","import random\n","import torch\n","import json\n","\n","seed = 42\n","\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"markdown","metadata":{"id":"6Vzmlkc0eFrO"},"source":["Getting the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qlj0cokYFeEY","colab":{"base_uri":"https://localhost:8080/","height":333,"referenced_widgets":["6d6deea706164b9e9b713601885d5b3a","855683555af249bfa7876ab80b990114","67f77135f51f4ae794e2fd8c353ff430","e021df59e25c4f79af322ff69a43385d","9ef5272104fc43298ef1ca027522b971","c49e3b8e6d104db8ac3f6218e15c73e8","2f66a7a685cb4ede836593a7df183093","e43d23a83c4840dd93dfdd289bb3a048","75dc1f2eaa1e4505b86f4b319a0da6ef","5cf6b200f0764672b5380ee6b0b025b3","58bbe52516574330ac4989c269794bb6","0d9490c215e042d7bc807b0ebd7ab32c","21db14007bfb4d778563ccdf6f0043a0","b7a87550f22d485098dfd1ad14a62a12","9e2662a65e7642dfae4fe523d918a84e","4475e30e6f004dfcb655bf24ebc95057","5e2b1386296f45e48efe793a9c7ae7d4","4615712c78a44efc9eb0e897752d7613","5c0d118e438644fca0073b384e6e38b4","c66d45e0e0104668890fc2c576d40328","7a9b7cf127f1447abd79e2dec9d905cb","8158df217bf44588846bb3aaffb8e1b6","78c65744dc99401c8274691aba0da24d","2b0f55d2d3f24045a3e91325ad0453c0","481bd1eeec534d1cb0d0e3952b313d04","7ff14e0197e349c99bd9c6538a4e3d11","b85af78814cf4aa79b17e876a1c26002","ca8cf606c2a74e1b995ac9ccbaae7f8a","300f8da996944ec29868272b50959f92","8f32e64005f6490db9ccdedfa9f6ac3c","1f4ada9eae81495daf7f9019650dad62","243fb5975d664783ac727e7519bda8eb","d0dba3c70dfa46bd9552c13e52043410","d7b6f52913dd40fe9fbf6c064f72af03","3937e1f2d6714d64985be9abb684d7b4","0e49533d3cca4ca897e9379f3cafbb90","12a01607f87e4b999e4e239d450be841","ae3016f50d1c410884d15fc5f39f17ef","cef22989322849fda4215bec758ead24","75cd2c5ae7754bfaaaa559e388c5003f","a4289399d5804a3aadca3cec90101304","316da883b4444f6f9be384933877e899","d906c24360c043c888a6843ab57936f2","7d414696c96c4c6f826f04c3dc97e94e","3177845a32fa453fa7d94636c55a9bc8","0eb84b8b3c01410d982726090ffba339","7f20295853dc449fb938c39f93b9fc4d","5ff26d969d90421caf090509b67eaf18","7650f4b5ee9441ac8b21f291e05bccea","0b7d746b85dd4379bb549c445a816227","ada36259e603424a875d7e72e5c1df47","c9cb8c122dba42daba9881aa348a5a5a","99d76bcf7fea4b5c84a2239c2f56c726","a979b376285b40188a6f20f0c8ff9efb","8a83ecd420dd4eb2b6b16e433e65079f","72454818b4e64f87aeaba8e7934c4929","221f429041f54a42ba16505174903c7d","e7216360fdef454997b6939b446c8f9b","07a6c8764f58401bbd690da1d8bb9e00","460fd7ca8f0e4039846346b8d8b2cd04","9eeb232ed3c34c1cae664fe61cff094f","c798567b2034459597025c7640317d76","c26f601d16ae47bf96f2c8f295e12db9","03ba2083ad4e441d9f42b8dbc293c381","7d4d08b708704d5c8de1e8a15f93c302","d57b26c1ce5d415da5f5baded87b87bc"]},"executionInfo":{"status":"ok","timestamp":1723889726948,"user_tz":-120,"elapsed":8673,"user":{"displayName":"Eugenijus Kliocas","userId":"11927212131385663608"}},"outputId":"18217845-46bc-4b66-f695-bff3ac0c0e11"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.29k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d6deea706164b9e9b713601885d5b3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d9490c215e042d7bc807b0ebd7ab32c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/5.98M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78c65744dc99401c8274691aba0da24d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/6838 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7b6f52913dd40fe9fbf6c064f72af03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/3259 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3177845a32fa453fa7d94636c55a9bc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/886 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72454818b4e64f87aeaba8e7934c4929"}},"metadata":{}}],"source":["from datasets import load_dataset\n","sem_eval_2018_task_1 = load_dataset('sem_eval_2018_task_1', 'subtask5.english', trust_remote_code=True)"]},{"cell_type":"markdown","metadata":{"id":"iwaGC6VzgDnj"},"source":["#BERT"]},{"cell_type":"markdown","metadata":{"id":"WBI7zhY0b_32"},"source":["BERT + 100% gold standard data + synthetic data"]},{"cell_type":"markdown","source":["NO ROLE"],"metadata":{"id":"OEpDJO9QNWsN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5n_OKvMVcEYn","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7bddee1aec0f4132887202c62e7262cf","6c288df58d9e4d9a805ce3c88a9c9d52","405f649dea22401fa1d82ca4dd080e1c","77aa5ca91fb74a23934866e76e6bc46f","cc6ceaacedf44731b21bdfeee8d4dd5a","deaecb17f6bc4d5f912ba64406e735a3","3ba526087e8f474fbb0c09b207af30e1","fe7c46f25255423495247896ce8c2d2e","199c46024a8f4ba59d288635425b85ba","cc7df7259a47473e95c5429573b47970","97221faa12dc49cfb0ebb8de84cd63a1","e6db730becba4f9296ef19b26feb1e9e","ae1c03fca0fb40d1bb3f8aaa839ef1dc","0e2decaac6f54816a5f8b9b752cbbf6d","182bfc0fa50b4531a8611361a3849e94","095d200f1dcf43a5aca87e236fbc159e","e0cac7ca2cbc400d8ce1636781968e13","a2ebc7246d9d4f0d9dad1852024a17bc","887d61a568c740e69987bbb83cc6dc5b","967073b9809a441198d1a4125f5ff82b","cbd6744c051c4e648fb6ac559f655ddb","0ffc082574474a3b8fdf2c2bded46145","f69455d5d6894c6983b0507a04d86319","d9f05598d6f64ca9b0a83d7703bae362","a51c0322bf3d4269842db154624d1e81","dfdffde72ed844bb879107546e43e763","fe9d07b1a8044dfeb72de5f2141a6608","b354d55a51804907a2b03f2af0748840","4e20908a793f40929e78a1fb067bfdc4","5245109373a1459999d84b37777d52e4","15efca04ef8443a48c4eeb3ec377f092","f578a03190d14343bfb177ed5d3497ab","1014b271b4544c8da6dc192edb5a1acc","46239bf76d6c4e66bc2c6ad233052a7d","56c032bded6443979780a48a8c5afa7a","ee9196323eb644a9a3cb157655e245d8","ecdbcd379a6945fab99273f4db2c7127","98f42cdc44c14afd9f4b71145585d048","5adf0bf75f864f399cc23392506ba709","69d95ca8f8404e65a30a6dd1618f5267","422e24e03a4d408393a7ff774ec61bfb","f120d79121844bb9afed3d907f5c8a78","4f399adad53143289ad6ec792b07206f","83e685a4519c43e59946ba6716d4372f","b64271812de440629b5fdbb42c4bfc4a","be64d9b530ed463b957db50a3d27c41b","fc3d963c777d49a289293c749ae45180","466c8daa6dcd4da99021ec5d9819588c","c4997318b53f4a6caf54e0e5395b0c75","6a8514269f7746c2979f171c50baf55d","09579506dfb14967b681256651b0529f","c9d3bb56d67f418697ef9f0bb91cb5ef","ea63dd2beb194fae986ee7bc7ad1d458","0168e3e81c9a4324bbdc40c57312a820","f8a8c7588be24f2289cd2cae947ce6a8"]},"executionInfo":{"status":"ok","timestamp":1723811902185,"user_tz":-120,"elapsed":526553,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"0a0c4dee-6a60-4fc8-a7f1-9f8d068fcfb9"},"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bddee1aec0f4132887202c62e7262cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6db730becba4f9296ef19b26feb1e9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f69455d5d6894c6983b0507a04d86319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46239bf76d6c4e66bc2c6ad233052a7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b64271812de440629b5fdbb42c4bfc4a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4667, Val Loss: 0.3740\n","Epoch 2, Train Loss: 0.3446, Val Loss: 0.3302\n","Epoch 3, Train Loss: 0.2990, Val Loss: 0.3174\n","Epoch 4, Train Loss: 0.2692, Val Loss: 0.3119\n","Epoch 5, Train Loss: 0.2463, Val Loss: 0.3096\n","Test Accuracy: 27.19\n","Test F1-macro: 52.46\n","Test F1-micro: 68.75\n","Confusion Matrix:\n","Label: anticipation\n","[[2757   77]\n"," [ 372   53]]\n","\n","Label: optimism\n","[[1845  271]\n"," [ 338  805]]\n","\n","Label: trust\n","[[3032   74]\n"," [ 143   10]]\n","\n","Label: joy\n","[[1628  189]\n"," [ 260 1182]]\n","\n","Label: love\n","[[2544  199]\n"," [ 204  312]]\n","\n","Label: anger\n","[[2017  141]\n"," [ 317  784]]\n","\n","Label: disgust\n","[[1942  218]\n"," [ 328  771]]\n","\n","Label: pessimism\n","[[2778  106]\n"," [ 277   98]]\n","\n","Label: sadness\n","[[2142  157]\n"," [ 386  574]]\n","\n","Label: fear\n","[[2707   67]\n"," [ 171  314]]\n","\n","Label: surprise\n","[[3085    4]\n"," [ 164    6]]\n","\n"]}],"source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"full_gsd_multi_gpt_data.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"]},{"cell_type":"markdown","source":["NO ROLE + NEWS ARTICLES"],"metadata":{"id":"IaeKR9t-NYt1"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"full_gsd_multi_gpt_data_no_role_news.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8yO8c3xNIte","executionInfo":{"status":"ok","timestamp":1723812414565,"user_tz":-120,"elapsed":502298,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"e04034b3-0a24-4b7f-fe02-99f8068d4fd8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4861, Val Loss: 0.3784\n","Epoch 2, Train Loss: 0.3662, Val Loss: 0.3298\n","Epoch 3, Train Loss: 0.3154, Val Loss: 0.3168\n","Epoch 4, Train Loss: 0.2825, Val Loss: 0.3081\n","Epoch 5, Train Loss: 0.2575, Val Loss: 0.3114\n","Test Accuracy: 27.92\n","Test F1-macro: 53.48\n","Test F1-micro: 69.09\n","Confusion Matrix:\n","Label: anticipation\n","[[2743   91]\n"," [ 367   58]]\n","\n","Label: optimism\n","[[1771  345]\n"," [ 255  888]]\n","\n","Label: trust\n","[[3079   27]\n"," [ 144    9]]\n","\n","Label: joy\n","[[1613  204]\n"," [ 246 1196]]\n","\n","Label: love\n","[[2585  158]\n"," [ 232  284]]\n","\n","Label: anger\n","[[2010  148]\n"," [ 332  769]]\n","\n","Label: disgust\n","[[1952  208]\n"," [ 367  732]]\n","\n","Label: pessimism\n","[[2754  130]\n"," [ 277   98]]\n","\n","Label: sadness\n","[[2140  159]\n"," [ 369  591]]\n","\n","Label: fear\n","[[2688   86]\n"," [ 146  339]]\n","\n","Label: surprise\n","[[3077   12]\n"," [ 154   16]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 1"],"metadata":{"id":"Gbnykt87Nb1R"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"full_gsd_multi_gpt_data_role_1.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-dl6i8ONhWe","executionInfo":{"status":"ok","timestamp":1723812944479,"user_tz":-120,"elapsed":507021,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"e7422a4c-499b-4709-b1c8-3edc7dd8f9a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4705, Val Loss: 0.3681\n","Epoch 2, Train Loss: 0.3430, Val Loss: 0.3397\n","Epoch 3, Train Loss: 0.2964, Val Loss: 0.3164\n","Epoch 4, Train Loss: 0.2671, Val Loss: 0.3069\n","Epoch 5, Train Loss: 0.2445, Val Loss: 0.3043\n","Test Accuracy: 28.35\n","Test F1-macro: 51.99\n","Test F1-micro: 69.41\n","Confusion Matrix:\n","Label: anticipation\n","[[2767   67]\n"," [ 373   52]]\n","\n","Label: optimism\n","[[1837  279]\n"," [ 323  820]]\n","\n","Label: trust\n","[[3083   23]\n"," [ 151    2]]\n","\n","Label: joy\n","[[1646  171]\n"," [ 296 1146]]\n","\n","Label: love\n","[[2633  110]\n"," [ 262  254]]\n","\n","Label: anger\n","[[1986  172]\n"," [ 289  812]]\n","\n","Label: disgust\n","[[1894  266]\n"," [ 278  821]]\n","\n","Label: pessimism\n","[[2816   68]\n"," [ 300   75]]\n","\n","Label: sadness\n","[[2124  175]\n"," [ 345  615]]\n","\n","Label: fear\n","[[2702   72]\n"," [ 162  323]]\n","\n","Label: surprise\n","[[3085    4]\n"," [ 160   10]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 2"],"metadata":{"id":"M4pOBERjNcuJ"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"full_gsd_multi_gpt_data_role_2.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"byjX6-mCNmhO","executionInfo":{"status":"ok","timestamp":1723813455055,"user_tz":-120,"elapsed":502430,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"d9e7ec6d-fb10-485d-ef75-1bd9b12c1d2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4884, Val Loss: 0.3749\n","Epoch 2, Train Loss: 0.3548, Val Loss: 0.3407\n","Epoch 3, Train Loss: 0.3037, Val Loss: 0.3202\n","Epoch 4, Train Loss: 0.2706, Val Loss: 0.3136\n","Epoch 5, Train Loss: 0.2477, Val Loss: 0.3118\n","Test Accuracy: 27.25\n","Test F1-macro: 52.59\n","Test F1-micro: 69.24\n","Confusion Matrix:\n","Label: anticipation\n","[[2753   81]\n"," [ 362   63]]\n","\n","Label: optimism\n","[[1783  333]\n"," [ 261  882]]\n","\n","Label: trust\n","[[3030   76]\n"," [ 138   15]]\n","\n","Label: joy\n","[[1609  208]\n"," [ 244 1198]]\n","\n","Label: love\n","[[2531  212]\n"," [ 192  324]]\n","\n","Label: anger\n","[[1993  165]\n"," [ 300  801]]\n","\n","Label: disgust\n","[[1934  226]\n"," [ 325  774]]\n","\n","Label: pessimism\n","[[2806   78]\n"," [ 297   78]]\n","\n","Label: sadness\n","[[2140  159]\n"," [ 389  571]]\n","\n","Label: fear\n","[[2724   50]\n"," [ 185  300]]\n","\n","Label: surprise\n","[[3087    2]\n"," [ 167    3]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 3"],"metadata":{"id":"eg2ayUuMNdwx"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"full_gsd_multi_gpt_data_role_3.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PH9C8yt1NoRW","executionInfo":{"status":"ok","timestamp":1723813981467,"user_tz":-120,"elapsed":499414,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"9a09b46e-cd95-4b0d-ea1e-352754e50faf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4697, Val Loss: 0.3673\n","Epoch 2, Train Loss: 0.3402, Val Loss: 0.3298\n","Epoch 3, Train Loss: 0.2923, Val Loss: 0.3162\n","Epoch 4, Train Loss: 0.2634, Val Loss: 0.3137\n","Epoch 5, Train Loss: 0.2423, Val Loss: 0.3205\n","Test Accuracy: 27.16\n","Test F1-macro: 54.20\n","Test F1-micro: 69.56\n","Confusion Matrix:\n","Label: anticipation\n","[[2711  123]\n"," [ 353   72]]\n","\n","Label: optimism\n","[[1811  305]\n"," [ 283  860]]\n","\n","Label: trust\n","[[3050   56]\n"," [ 139   14]]\n","\n","Label: joy\n","[[1673  144]\n"," [ 317 1125]]\n","\n","Label: love\n","[[2576  167]\n"," [ 235  281]]\n","\n","Label: anger\n","[[1937  221]\n"," [ 239  862]]\n","\n","Label: disgust\n","[[1875  285]\n"," [ 277  822]]\n","\n","Label: pessimism\n","[[2713  171]\n"," [ 234  141]]\n","\n","Label: sadness\n","[[2107  192]\n"," [ 348  612]]\n","\n","Label: fear\n","[[2679   95]\n"," [ 144  341]]\n","\n","Label: surprise\n","[[3086    3]\n"," [ 164    6]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 4"],"metadata":{"id":"7q2ZI2y3NeZd"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"full_gsd_multi_gpt_data_role_4.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data (assuming df_val remains unchanged)\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set (assuming df_test remains unchanged)\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7Nia-yWNpy1","executionInfo":{"status":"ok","timestamp":1723814523899,"user_tz":-120,"elapsed":502687,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"d9199584-fbbd-434b-870b-df33e0428118"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4656, Val Loss: 0.3659\n","Epoch 2, Train Loss: 0.3482, Val Loss: 0.3304\n","Epoch 3, Train Loss: 0.3026, Val Loss: 0.3157\n","Epoch 4, Train Loss: 0.2741, Val Loss: 0.3099\n","Epoch 5, Train Loss: 0.2519, Val Loss: 0.3096\n","Test Accuracy: 28.44\n","Test F1-macro: 52.72\n","Test F1-micro: 69.38\n","Confusion Matrix:\n","Label: anticipation\n","[[2788   46]\n"," [ 389   36]]\n","\n","Label: optimism\n","[[1766  350]\n"," [ 237  906]]\n","\n","Label: trust\n","[[3086   20]\n"," [ 143   10]]\n","\n","Label: joy\n","[[1598  219]\n"," [ 218 1224]]\n","\n","Label: love\n","[[2656   87]\n"," [ 275  241]]\n","\n","Label: anger\n","[[2006  152]\n"," [ 306  795]]\n","\n","Label: disgust\n","[[1947  213]\n"," [ 345  754]]\n","\n","Label: pessimism\n","[[2749  135]\n"," [ 266  109]]\n","\n","Label: sadness\n","[[2157  142]\n"," [ 410  550]]\n","\n","Label: fear\n","[[2702   72]\n"," [ 179  306]]\n","\n","Label: surprise\n","[[3086    3]\n"," [ 157   13]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 1 + NEWS ARTICLES"],"metadata":{"id":"bCajXETuNfcl"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"full_gsd_multi_gpt_data_role_5.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65XO5JW_NrYI","executionInfo":{"status":"ok","timestamp":1723815112921,"user_tz":-120,"elapsed":502021,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"ef80c5c0-9326-440d-a105-96fb3b80334a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4957, Val Loss: 0.3754\n","Epoch 2, Train Loss: 0.3639, Val Loss: 0.3387\n","Epoch 3, Train Loss: 0.3161, Val Loss: 0.3237\n","Epoch 4, Train Loss: 0.2839, Val Loss: 0.3140\n","Epoch 5, Train Loss: 0.2587, Val Loss: 0.3135\n","Test Accuracy: 27.95\n","Test F1-macro: 53.04\n","Test F1-micro: 69.68\n","Confusion Matrix:\n","Label: anticipation\n","[[2774   60]\n"," [ 379   46]]\n","\n","Label: optimism\n","[[1794  322]\n"," [ 274  869]]\n","\n","Label: trust\n","[[3084   22]\n"," [ 144    9]]\n","\n","Label: joy\n","[[1658  159]\n"," [ 300 1142]]\n","\n","Label: love\n","[[2614  129]\n"," [ 235  281]]\n","\n","Label: anger\n","[[1889  269]\n"," [ 220  881]]\n","\n","Label: disgust\n","[[1815  345]\n"," [ 230  869]]\n","\n","Label: pessimism\n","[[2750  134]\n"," [ 278   97]]\n","\n","Label: sadness\n","[[2092  207]\n"," [ 347  613]]\n","\n","Label: fear\n","[[2721   53]\n"," [ 184  301]]\n","\n","Label: surprise\n","[[3085    4]\n"," [ 159   11]]\n","\n"]}]},{"cell_type":"markdown","source":["##Single Label BERT"],"metadata":{"id":"EfZ_-9d5gMGi"}},{"cell_type":"markdown","source":["No Role"],"metadata":{"id":"rgHr1mTLgt8H"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"full_gsd_single_gpt_data_no_role.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NclUbLzJgOG7","executionInfo":{"status":"ok","timestamp":1723815824142,"user_tz":-120,"elapsed":501035,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"4ff5adbf-408c-4c7f-f732-c30cf77a0c9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4462, Val Loss: 0.3742\n","Epoch 2, Train Loss: 0.3362, Val Loss: 0.3282\n","Epoch 3, Train Loss: 0.2897, Val Loss: 0.3136\n","Epoch 4, Train Loss: 0.2582, Val Loss: 0.3085\n","Epoch 5, Train Loss: 0.2341, Val Loss: 0.3124\n","Test Accuracy: 27.31\n","Test F1-macro: 50.85\n","Test F1-micro: 67.91\n","Confusion Matrix:\n","Label: anticipation\n","[[2768   66]\n"," [ 385   40]]\n","\n","Label: optimism\n","[[1836  280]\n"," [ 378  765]]\n","\n","Label: trust\n","[[3101    5]\n"," [ 152    1]]\n","\n","Label: joy\n","[[1643  174]\n"," [ 284 1158]]\n","\n","Label: love\n","[[2594  149]\n"," [ 239  277]]\n","\n","Label: anger\n","[[2020  138]\n"," [ 328  773]]\n","\n","Label: disgust\n","[[1958  202]\n"," [ 367  732]]\n","\n","Label: pessimism\n","[[2832   52]\n"," [ 317   58]]\n","\n","Label: sadness\n","[[2153  146]\n"," [ 399  561]]\n","\n","Label: fear\n","[[2697   77]\n"," [ 156  329]]\n","\n","Label: surprise\n","[[3085    4]\n"," [ 154   16]]\n","\n"]}]},{"cell_type":"markdown","source":["Role 1"],"metadata":{"id":"cCY56ifdgwCE"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"full_gsd_single_gpt_data_role_1.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J0q03iKfgZ_i","executionInfo":{"status":"ok","timestamp":1723816428054,"user_tz":-120,"elapsed":499919,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"b5b5cd79-ba97-40f4-df8e-2676200df62f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4815, Val Loss: 0.4016\n","Epoch 2, Train Loss: 0.3605, Val Loss: 0.3361\n","Epoch 3, Train Loss: 0.3028, Val Loss: 0.3176\n","Epoch 4, Train Loss: 0.2692, Val Loss: 0.3102\n","Epoch 5, Train Loss: 0.2438, Val Loss: 0.3031\n","Test Accuracy: 28.11\n","Test F1-macro: 51.73\n","Test F1-micro: 69.04\n","Confusion Matrix:\n","Label: anticipation\n","[[2791   43]\n"," [ 386   39]]\n","\n","Label: optimism\n","[[1830  286]\n"," [ 369  774]]\n","\n","Label: trust\n","[[3103    3]\n"," [ 153    0]]\n","\n","Label: joy\n","[[1648  169]\n"," [ 293 1149]]\n","\n","Label: love\n","[[2594  149]\n"," [ 231  285]]\n","\n","Label: anger\n","[[1977  181]\n"," [ 271  830]]\n","\n","Label: disgust\n","[[1906  254]\n"," [ 281  818]]\n","\n","Label: pessimism\n","[[2807   77]\n"," [ 288   87]]\n","\n","Label: sadness\n","[[2145  154]\n"," [ 397  563]]\n","\n","Label: fear\n","[[2718   56]\n"," [ 169  316]]\n","\n","Label: surprise\n","[[3086    3]\n"," [ 158   12]]\n","\n"]}]},{"cell_type":"markdown","source":["Role 1 + NEWS"],"metadata":{"id":"r4iD4Dvqgxy-"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"full_gsd_single_gpt_data_role_1_news.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P62T2cE6grJW","executionInfo":{"status":"ok","timestamp":1723816941850,"user_tz":-120,"elapsed":502557,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"00af4509-a294-4e38-f19a-4546fe3fe0e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4528, Val Loss: 0.3726\n","Epoch 2, Train Loss: 0.3446, Val Loss: 0.3258\n","Epoch 3, Train Loss: 0.2958, Val Loss: 0.3093\n","Epoch 4, Train Loss: 0.2661, Val Loss: 0.3045\n","Epoch 5, Train Loss: 0.2429, Val Loss: 0.3076\n","Test Accuracy: 27.16\n","Test F1-macro: 53.01\n","Test F1-micro: 69.12\n","Confusion Matrix:\n","Label: anticipation\n","[[2751   83]\n"," [ 363   62]]\n","\n","Label: optimism\n","[[1806  310]\n"," [ 335  808]]\n","\n","Label: trust\n","[[3101    5]\n"," [ 148    5]]\n","\n","Label: joy\n","[[1552  265]\n"," [ 210 1232]]\n","\n","Label: love\n","[[2561  182]\n"," [ 211  305]]\n","\n","Label: anger\n","[[2016  142]\n"," [ 318  783]]\n","\n","Label: disgust\n","[[1941  219]\n"," [ 336  763]]\n","\n","Label: pessimism\n","[[2781  103]\n"," [ 282   93]]\n","\n","Label: sadness\n","[[2142  157]\n"," [ 375  585]]\n","\n","Label: fear\n","[[2710   64]\n"," [ 168  317]]\n","\n","Label: surprise\n","[[3085    4]\n"," [ 157   13]]\n","\n"]}]},{"cell_type":"markdown","source":["Increased multi-label"],"metadata":{"id":"rntZD4p43enT"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"full_gsd_increased_gpt_data_multi.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MjKXgazd3eCH","executionInfo":{"status":"ok","timestamp":1723817511511,"user_tz":-120,"elapsed":564756,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"ee08e509-4ca9-42c1-a0a7-fae7d8231313"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4523, Val Loss: 0.3607\n","Epoch 2, Train Loss: 0.3128, Val Loss: 0.3285\n","Epoch 3, Train Loss: 0.2686, Val Loss: 0.3155\n","Epoch 4, Train Loss: 0.2400, Val Loss: 0.3122\n","Epoch 5, Train Loss: 0.2190, Val Loss: 0.3223\n","Test Accuracy: 26.70\n","Test F1-macro: 54.20\n","Test F1-micro: 69.71\n","Confusion Matrix:\n","Label: anticipation\n","[[2780   54]\n"," [ 373   52]]\n","\n","Label: optimism\n","[[1824  292]\n"," [ 316  827]]\n","\n","Label: trust\n","[[3036   70]\n"," [ 144    9]]\n","\n","Label: joy\n","[[1674  143]\n"," [ 332 1110]]\n","\n","Label: love\n","[[2543  200]\n"," [ 188  328]]\n","\n","Label: anger\n","[[1966  192]\n"," [ 266  835]]\n","\n","Label: disgust\n","[[1867  293]\n"," [ 266  833]]\n","\n","Label: pessimism\n","[[2692  192]\n"," [ 236  139]]\n","\n","Label: sadness\n","[[2013  286]\n"," [ 272  688]]\n","\n","Label: fear\n","[[2693   81]\n"," [ 139  346]]\n","\n","Label: surprise\n","[[3084    5]\n"," [ 160   10]]\n","\n"]}]},{"cell_type":"markdown","source":["Increased single-label"],"metadata":{"id":"5q9MF0VR3g8H"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-5\n","dropout = 0.2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","mixed_df = pd.read_csv(\"full_gsd_increased_gpt_data_single.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_columns))\n","model.dropout = torch.nn.Dropout(dropout)  # Adding dropout to the model\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(mixed_df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(mixed_df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","# Evaluation on test set\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs_test = model(input_ids=inputs_test['input_ids'], attention_mask=inputs_test['attention_mask'])\n","    logits_test = outputs_test.logits\n","    predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).cpu().numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test.cpu(), predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test.cpu(), predicted_labels_test, average='micro') * 100\n","\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test.cpu(), predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-yTNsY0B37VW","executionInfo":{"status":"ok","timestamp":1723818940368,"user_tz":-120,"elapsed":573990,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"c77b2fd5-d842-4d5c-aecd-64fa31331a46"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.4230, Val Loss: 0.3613\n","Epoch 2, Train Loss: 0.3057, Val Loss: 0.3219\n","Epoch 3, Train Loss: 0.2549, Val Loss: 0.3136\n","Epoch 4, Train Loss: 0.2246, Val Loss: 0.3044\n","Epoch 5, Train Loss: 0.2015, Val Loss: 0.3136\n","Test Accuracy: 27.98\n","Test F1-macro: 53.01\n","Test F1-micro: 69.45\n","Confusion Matrix:\n","Label: anticipation\n","[[2788   46]\n"," [ 388   37]]\n","\n","Label: optimism\n","[[1871  245]\n"," [ 406  737]]\n","\n","Label: trust\n","[[3095   11]\n"," [ 146    7]]\n","\n","Label: joy\n","[[1675  142]\n"," [ 323 1119]]\n","\n","Label: love\n","[[2563  180]\n"," [ 220  296]]\n","\n","Label: anger\n","[[1937  221]\n"," [ 234  867]]\n","\n","Label: disgust\n","[[1830  330]\n"," [ 229  870]]\n","\n","Label: pessimism\n","[[2806   78]\n"," [ 308   67]]\n","\n","Label: sadness\n","[[2048  251]\n"," [ 282  678]]\n","\n","Label: fear\n","[[2697   77]\n"," [ 153  332]]\n","\n","Label: surprise\n","[[3079   10]\n"," [ 148   22]]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"G3h9MFtfcIkk"},"source":["#RoBERTa"]},{"cell_type":"markdown","source":["RoBERTa + GSD + synthetic data"],"metadata":{"id":"2Dw5SrWOvkdX"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset\n","df = pd.read_csv(\"full_gsd_multi_gpt_data.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3b54e2e8e9fe4cb09e31cffa598b3083","0da0af466498429891335a33c051c6be","44d3675ae77e482c9e42785c67a07804","0303055eec1d495381833a4fd1956c9a","84c43b090cad4a2798156602145be7cd","8439dd8e228040ee89cf41f064a7c294","555697e85ccd4931bd6fde37e7bbeecc","9d363f44f8174d9e87186b3f385dc30b","f28b4bd4dbce475e9c2f70a5d20070ba","c1e0d271141b4207a77d115b130c9b57","f4011992fa78497bb03f2dd597662a90","2ca55695d3c44faaacfdb2abdc49773f","5d8586e66c5242aeb63b5ed41625925f","0a8a10ac097d47c0bccc10d0f8ea28b5","2b5b4cc3a88f413db5cc18344f08297d","21ffdf271c954a97a2fa7740e0e44d98","18604f8e3c014841b916f4a7f4c48175","300e1899ad9b4603a3a741bc235859a1","031804f4031048b9af3dc25468c04b66","511c3aca2b11405fb7282a6f433163ad","b5f78f5fde6644fe9a945ffb663613b4","283ccdf03c334339af8ba60c8adf546a","6afe62bc97044592931a83d07f837486","50bd0b64789a477f9e44a175f14992b1","d3bd84410c4d486bbd11af404209b240","05019c0eef7c4c7782d13e0c1a53e8f4","744614491461405c84ca4380c9cdc1d3","efd98c9a9b3c4eb3acf0693a5ac25984","7dd79836cfb04f1cbf7533c9052bfdd7","8299af08714f40769725ccf680db8d6e","2ef719f89673450eb110715c3a5ec13e","4e95b5a10ed9430ab4532085a1d807d2","39b9faee3d314c1f8af015e700bc9002","3c2a5fb577c3448f8509b9a53ffaf279","c6159beaf04c4889b193f2a641bc65ac","a7c74ee1c0d847ec82479cee32e29950","de69cabeec4a47e481634cf3230bdc8d","f5c829e3dab3474cbfc8156682d20e35","182bd70bd3894374906e68883892717f","ffacab2372144df899cc1efe8023f73e","0eb5498a28ad4006b3b3abfad9e5fa0f","43f4ddfdb6484ea895c0cff6ee40f2c6","ab23d57777d540d8a52d62c9cda51246","dfc4cd5c48be428d817964beecce4432","a9057b7919114b09b64c76ab163528c3","8f38a0ff569b475c93832986e02ebc8d","e625def06f304fc2b75a0cb154b88970","70318ae86cfa431489dbc2403aac9730","9e46422db1d34d9199cbd4328c1961d5","5f78b7ae6991455198e589d2a5bbd174","2e230e1dcd42461f847dbdc7a60be60e","7f05db8e097448faacda27d27bc990f7","96462350f53a4f749e6c020b65951ee6","ece8ed0b580a4fe587f5adfec031b2e7","31e7a5aa455a4b2997292578e213acb7","fd2588bfbb9845b8a8a6fd1b8173dbf5","68f574f032ed4f58947177518078f913","3ae07d733ea64116accd8dbca713101f","f79afb706af44093b4527399c51035e5","aca7920e2cbe464d8c5cb3f4d9e216ea","e0d153d800a948a092c29eb01848fb5a","0f8f12878593409ebf6ed3ccc92df012","9b1e68ddbaa44de89e89b5a12540602b","3799cbe4f2b943298572a118f0764ae2","47c4b282eaac45b58dcb06a82df86106","ef50f581d00b43afbc344c521de1be54"]},"id":"F5rrB_LEvky0","executionInfo":{"status":"ok","timestamp":1723821170637,"user_tz":-120,"elapsed":2224832,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"bb50ba82-dbba-4e07-adac-670e045de8e7"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b54e2e8e9fe4cb09e31cffa598b3083"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ca55695d3c44faaacfdb2abdc49773f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6afe62bc97044592931a83d07f837486"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c2a5fb577c3448f8509b9a53ffaf279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9057b7919114b09b64c76ab163528c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd2588bfbb9845b8a8a6fd1b8173dbf5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5657, Val Loss: 0.4245\n","Epoch 2, Train Loss: 0.4122, Val Loss: 0.3629\n","Epoch 3, Train Loss: 0.3637, Val Loss: 0.3477\n","Epoch 4, Train Loss: 0.3375, Val Loss: 0.3279\n","Epoch 5, Train Loss: 0.3186, Val Loss: 0.3231\n","Epoch 6, Train Loss: 0.3037, Val Loss: 0.3152\n","Epoch 7, Train Loss: 0.2924, Val Loss: 0.3139\n","Epoch 8, Train Loss: 0.2820, Val Loss: 0.3113\n","Epoch 9, Train Loss: 0.2762, Val Loss: 0.3103\n","Epoch 10, Train Loss: 0.2683, Val Loss: 0.3020\n","Test Accuracy: 27.31\n","Test F1-macro: 53.76\n","Test F1-micro: 70.15\n","Confusion Matrix:\n","Label: anticipation\n","[[2768   66]\n"," [ 384   41]]\n","\n","Label: optimism\n","[[1730  386]\n"," [ 223  920]]\n","\n","Label: trust\n","[[3023   83]\n"," [ 145    8]]\n","\n","Label: joy\n","[[1624  193]\n"," [ 256 1186]]\n","\n","Label: love\n","[[2492  251]\n"," [ 156  360]]\n","\n","Label: anger\n","[[1924  234]\n"," [ 222  879]]\n","\n","Label: disgust\n","[[1881  279]\n"," [ 257  842]]\n","\n","Label: pessimism\n","[[2725  159]\n"," [ 255  120]]\n","\n","Label: sadness\n","[[2121  178]\n"," [ 353  607]]\n","\n","Label: fear\n","[[2703   71]\n"," [ 173  312]]\n","\n","Label: surprise\n","[[3070   19]\n"," [ 157   13]]\n","\n"]}]},{"cell_type":"markdown","source":["No role + NEWS"],"metadata":{"id":"P7R3fC1bhSyl"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset\n","df = pd.read_csv(\"full_gsd_multi_gpt_data_no_role_news.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"id":"Y9nhCa9ChY-R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723823413686,"user_tz":-120,"elapsed":2229812,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"9dd8a062-e862-4378-e3ca-9025f98c4b16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5690, Val Loss: 0.4298\n","Epoch 2, Train Loss: 0.4343, Val Loss: 0.3740\n","Epoch 3, Train Loss: 0.3846, Val Loss: 0.3545\n","Epoch 4, Train Loss: 0.3569, Val Loss: 0.3361\n","Epoch 5, Train Loss: 0.3377, Val Loss: 0.3309\n","Epoch 6, Train Loss: 0.3217, Val Loss: 0.3181\n","Epoch 7, Train Loss: 0.3098, Val Loss: 0.3159\n","Epoch 8, Train Loss: 0.2993, Val Loss: 0.3136\n","Epoch 9, Train Loss: 0.2906, Val Loss: 0.3077\n","Epoch 10, Train Loss: 0.2841, Val Loss: 0.3046\n","Test Accuracy: 28.29\n","Test F1-macro: 54.35\n","Test F1-micro: 70.75\n","Confusion Matrix:\n","Label: anticipation\n","[[2752   82]\n"," [ 371   54]]\n","\n","Label: optimism\n","[[1713  403]\n"," [ 198  945]]\n","\n","Label: trust\n","[[3065   41]\n"," [ 147    6]]\n","\n","Label: joy\n","[[1600  217]\n"," [ 220 1222]]\n","\n","Label: love\n","[[2474  269]\n"," [ 146  370]]\n","\n","Label: anger\n","[[1914  244]\n"," [ 227  874]]\n","\n","Label: disgust\n","[[1888  272]\n"," [ 250  849]]\n","\n","Label: pessimism\n","[[2760  124]\n"," [ 280   95]]\n","\n","Label: sadness\n","[[2108  191]\n"," [ 337  623]]\n","\n","Label: fear\n","[[2709   65]\n"," [ 175  310]]\n","\n","Label: surprise\n","[[3060   29]\n"," [ 150   20]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 1"],"metadata":{"id":"LCFwmoT8WnCo"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset\n","df = pd.read_csv(\"full_gsd_multi_gpt_data_role_1.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"id":"mMSNsMo8heqL","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["91a82e1b70714e29a77a1611655ef5a1","65fb8157b96d43ba994c030696190571","c1204ce91caa453a981a77e896ba3db4","0c00497aba114c818c24ee354feae1f1","0dbdf7616cba40a39ae8c3978e961257","73ce93af19284581b73d078cdc0f5f81","f8792981ec574e62bb8ae05e744bd7f2","d6a29dfcc1bf4296b784ea3c38ce83f4","eb1166e0bb514f7eb76fa9ac50de6600","554ecfda6a734e7a9cf99c465e0c3499","8490b32431574356ae7fc35c97039a66","f46224ead6f147279b9f88a2be97f52b","0cabd5a963454dacab1446f25862a795","16e1fbaebafe4683a9733601decbb1b6","6f19626c11334ba684922cc75230e141","0d9068a0f83d4b00a42d7e283573de24","46fe8041e8e547c7a9ba2894757f3d04","cc42f4975bf846c2850ab2740a536a5d","5226143687024cfc8f52b5998cdbce42","c840bb8e4004417c9e22ddf023585f6e","dcc98c0e80b744699c05f245c38e10b8","39d972d6666c48c6a4e003cc0a2a8e6b","790a390d36e74bed84d957ef61278860","641cdfe0a3134dd896db9c091827c7d3","8bfbdf9e31574aad82acbe472c8ac2c5","0ab60fcadf0c409bbd37df509bcfbd7c","6d57f97cf4c046b78e8e2ce57e4a2bc9","f741992852d14c96a757261995101f8d","b95ab8021d0844579ffc978414a5163e","43cf4a40910b422a802047820c52fe78","721daa733066437d95c8521cf2021cc7","814bb2caa4b64e5dbbfb17029030c9a8","bb44ca8b866f43f1bb49d6d6c7f3d50c","ff4a1fe1707f4a2d958cbb49c81f0077","24b256d587f647d9a855f18760baa4f3","831a548017294d33855057e8dddd631f","f45cafdaa73f4edd9b257b6093717709","aec4d22bdd584758b5c56f9c9beac363","c9b6a5a3f83d4314aa1b2a1c20449a6d","92dacb3f38eb4643954dddb0b001d720","edf4a08b555b4e33938faf3b99b0a940","592905c32e804d40835ef62865af9265","33ae7d12d6c54681b2a1d886c5912b10","b0759dcdfe3046f09e0ecb8148681645","38931569ec8d4304bc296062b4340df7","5517ce7f22d84909ab8dafcd5feff1e7","e88463f146544d3693d2482913af6503","4086a48615b74850be0c50f4b5856f89","ce204ed931784055bb9784189b5c2f33","634f59ad8b1847af942866520268d9a0","984c68c3cda14dbdbfcdb98702d68d3f","affd217187b045bf82391bc1a9b79621","e9efecb54d644251b597cf17b22d5f1b","ab78609dc9514818a293787ff308cdbe","09b70881944d41c19f880bc72e2d9f84","e6399928716449c2ad78e8a71e40545b","9917fe1d8c954fba88ca8fede7c19ac0","7909112b5cb444adb9a9e8c5d46b7743","486a5402ab8c48c1b5c36e0a65833580","de0e21c186704735936e7efa14ec4e3b","d2854116fbc54233b134080c8eccee34","3f54e4d272d84b89b5d27cc12fcebcf4","b96e13426313497bbf83dee7dc778f4c","7ca3e9b8d9d448f59da073ec184d205b","1aa0363fd289404eb454aa30f0917e46","ede7bb2f06164cf78df342bdc3f7f475"]},"executionInfo":{"status":"ok","timestamp":1723839767365,"user_tz":-120,"elapsed":2298960,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"00f5afb3-d998-491c-a431-e21ddd1340e6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91a82e1b70714e29a77a1611655ef5a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f46224ead6f147279b9f88a2be97f52b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"790a390d36e74bed84d957ef61278860"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff4a1fe1707f4a2d958cbb49c81f0077"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38931569ec8d4304bc296062b4340df7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6399928716449c2ad78e8a71e40545b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5655, Val Loss: 0.4474\n","Epoch 2, Train Loss: 0.4274, Val Loss: 0.3706\n","Epoch 3, Train Loss: 0.3707, Val Loss: 0.3549\n","Epoch 4, Train Loss: 0.3438, Val Loss: 0.3393\n","Epoch 5, Train Loss: 0.3234, Val Loss: 0.3239\n","Epoch 6, Train Loss: 0.3099, Val Loss: 0.3221\n","Epoch 7, Train Loss: 0.2962, Val Loss: 0.3124\n","Epoch 8, Train Loss: 0.2865, Val Loss: 0.3165\n","Epoch 9, Train Loss: 0.2784, Val Loss: 0.3086\n","Epoch 10, Train Loss: 0.2707, Val Loss: 0.3137\n","Test Accuracy: 26.30\n","Test F1-macro: 55.53\n","Test F1-micro: 70.09\n","Confusion Matrix:\n","Label: anticipation\n","[[2710  124]\n"," [ 351   74]]\n","\n","Label: optimism\n","[[1655  461]\n"," [ 166  977]]\n","\n","Label: trust\n","[[2971  135]\n"," [ 132   21]]\n","\n","Label: joy\n","[[1594  223]\n"," [ 236 1206]]\n","\n","Label: love\n","[[2442  301]\n"," [ 125  391]]\n","\n","Label: anger\n","[[1982  176]\n"," [ 287  814]]\n","\n","Label: disgust\n","[[1923  237]\n"," [ 291  808]]\n","\n","Label: pessimism\n","[[2687  197]\n"," [ 229  146]]\n","\n","Label: sadness\n","[[2109  190]\n"," [ 329  631]]\n","\n","Label: fear\n","[[2684   90]\n"," [ 164  321]]\n","\n","Label: surprise\n","[[3081    8]\n"," [ 158   12]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 2"],"metadata":{"id":"xX-rkFORWu6X"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset\n","df = pd.read_csv(\"full_gsd_multi_gpt_data_role_2.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"id":"075osLv2gNx6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723842041281,"user_tz":-120,"elapsed":2263620,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"90c2defb-19cb-45a4-f9b0-be841a8197cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5761, Val Loss: 0.4410\n","Epoch 2, Train Loss: 0.4214, Val Loss: 0.3708\n","Epoch 3, Train Loss: 0.3684, Val Loss: 0.3463\n","Epoch 4, Train Loss: 0.3403, Val Loss: 0.3398\n","Epoch 5, Train Loss: 0.3206, Val Loss: 0.3233\n","Epoch 6, Train Loss: 0.3052, Val Loss: 0.3240\n","Epoch 7, Train Loss: 0.2964, Val Loss: 0.3141\n","Epoch 8, Train Loss: 0.2851, Val Loss: 0.3082\n","Epoch 9, Train Loss: 0.2758, Val Loss: 0.3164\n","Epoch 10, Train Loss: 0.2692, Val Loss: 0.3098\n","Test Accuracy: 26.88\n","Test F1-macro: 56.17\n","Test F1-micro: 70.00\n","Confusion Matrix:\n","Label: anticipation\n","[[2724  110]\n"," [ 354   71]]\n","\n","Label: optimism\n","[[1786  330]\n"," [ 244  899]]\n","\n","Label: trust\n","[[2951  155]\n"," [ 126   27]]\n","\n","Label: joy\n","[[1669  148]\n"," [ 289 1153]]\n","\n","Label: love\n","[[2525  218]\n"," [ 186  330]]\n","\n","Label: anger\n","[[1950  208]\n"," [ 267  834]]\n","\n","Label: disgust\n","[[1906  254]\n"," [ 268  831]]\n","\n","Label: pessimism\n","[[2684  200]\n"," [ 236  139]]\n","\n","Label: sadness\n","[[2098  201]\n"," [ 319  641]]\n","\n","Label: fear\n","[[2662  112]\n"," [ 143  342]]\n","\n","Label: surprise\n","[[3075   14]\n"," [ 150   20]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 3"],"metadata":{"id":"ZW7EQHICWyOF"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset\n","df = pd.read_csv(\"full_gsd_multi_gpt_data_role_3.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKoxrMx4WzFV","executionInfo":{"status":"ok","timestamp":1723844359376,"user_tz":-120,"elapsed":2257621,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"9518191f-e4c1-4a1a-8462-1a3dfee0c83d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5581, Val Loss: 0.4243\n","Epoch 2, Train Loss: 0.4135, Val Loss: 0.3757\n","Epoch 3, Train Loss: 0.3680, Val Loss: 0.3614\n","Epoch 4, Train Loss: 0.3436, Val Loss: 0.3467\n","Epoch 5, Train Loss: 0.3237, Val Loss: 0.3342\n","Epoch 6, Train Loss: 0.3082, Val Loss: 0.3181\n","Epoch 7, Train Loss: 0.2967, Val Loss: 0.3215\n","Epoch 8, Train Loss: 0.2869, Val Loss: 0.3132\n","Epoch 9, Train Loss: 0.2796, Val Loss: 0.3108\n","Epoch 10, Train Loss: 0.2690, Val Loss: 0.3125\n","Test Accuracy: 26.60\n","Test F1-macro: 54.34\n","Test F1-micro: 69.86\n","Confusion Matrix:\n","Label: anticipation\n","[[2689  145]\n"," [ 351   74]]\n","\n","Label: optimism\n","[[1757  359]\n"," [ 219  924]]\n","\n","Label: trust\n","[[2976  130]\n"," [ 135   18]]\n","\n","Label: joy\n","[[1647  170]\n"," [ 273 1169]]\n","\n","Label: love\n","[[2474  269]\n"," [ 162  354]]\n","\n","Label: anger\n","[[1937  221]\n"," [ 253  848]]\n","\n","Label: disgust\n","[[1895  265]\n"," [ 272  827]]\n","\n","Label: pessimism\n","[[2721  163]\n"," [ 257  118]]\n","\n","Label: sadness\n","[[2138  161]\n"," [ 340  620]]\n","\n","Label: fear\n","[[2672  102]\n"," [ 146  339]]\n","\n","Label: surprise\n","[[3075   14]\n"," [ 164    6]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 4"],"metadata":{"id":"YUG4nok5W1ol"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset\n","df = pd.read_csv(\"full_gsd_multi_gpt_data_role_4.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Qn_uQxJgXAV","outputId":"5ea2146f-06b9-4cd4-d9ff-d8e4f48a8273","executionInfo":{"status":"ok","timestamp":1723846631371,"user_tz":-120,"elapsed":2259649,"user":{"displayName":"Marija","userId":"12050775141700439362"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5584, Val Loss: 0.4515\n","Epoch 2, Train Loss: 0.4332, Val Loss: 0.3659\n","Epoch 3, Train Loss: 0.3801, Val Loss: 0.3439\n","Epoch 4, Train Loss: 0.3542, Val Loss: 0.3294\n","Epoch 5, Train Loss: 0.3315, Val Loss: 0.3209\n","Epoch 6, Train Loss: 0.3174, Val Loss: 0.3120\n","Epoch 7, Train Loss: 0.3062, Val Loss: 0.3085\n","Epoch 8, Train Loss: 0.2934, Val Loss: 0.3078\n","Epoch 9, Train Loss: 0.2846, Val Loss: 0.3081\n","Epoch 10, Train Loss: 0.2770, Val Loss: 0.2991\n","Test Accuracy: 29.21\n","Test F1-macro: 55.67\n","Test F1-micro: 70.42\n","Confusion Matrix:\n","Label: anticipation\n","[[2766   68]\n"," [ 367   58]]\n","\n","Label: optimism\n","[[1790  326]\n"," [ 251  892]]\n","\n","Label: trust\n","[[3070   36]\n"," [ 139   14]]\n","\n","Label: joy\n","[[1595  222]\n"," [ 213 1229]]\n","\n","Label: love\n","[[2602  141]\n"," [ 206  310]]\n","\n","Label: anger\n","[[1995  163]\n"," [ 309  792]]\n","\n","Label: disgust\n","[[1952  208]\n"," [ 320  779]]\n","\n","Label: pessimism\n","[[2710  174]\n"," [ 247  128]]\n","\n","Label: sadness\n","[[2138  161]\n"," [ 363  597]]\n","\n","Label: fear\n","[[2697   77]\n"," [ 166  319]]\n","\n","Label: surprise\n","[[3076   13]\n"," [ 148   22]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 1 + NEWS"],"metadata":{"id":"AFt27HJMW6V1"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset\n","df = pd.read_csv(\"full_gsd_multi_gpt_data_role_5.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DG3p5x8Amn2z","executionInfo":{"status":"ok","timestamp":1723892085108,"user_tz":-120,"elapsed":2272527,"user":{"displayName":"Eugenijus Kliocas","userId":"11927212131385663608"}},"outputId":"7bf6bd9f-944e-4e45-8794-151cdfb20f10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5687, Val Loss: 0.4626\n","Epoch 2, Train Loss: 0.4480, Val Loss: 0.3765\n","Epoch 3, Train Loss: 0.3861, Val Loss: 0.3555\n","Epoch 4, Train Loss: 0.3584, Val Loss: 0.3398\n","Epoch 5, Train Loss: 0.3383, Val Loss: 0.3345\n","Epoch 6, Train Loss: 0.3222, Val Loss: 0.3233\n","Epoch 7, Train Loss: 0.3100, Val Loss: 0.3221\n","Epoch 8, Train Loss: 0.3013, Val Loss: 0.3098\n","Epoch 9, Train Loss: 0.2917, Val Loss: 0.3062\n","Epoch 10, Train Loss: 0.2847, Val Loss: 0.3116\n","Test Accuracy: 27.62\n","Test F1-macro: 55.96\n","Test F1-micro: 70.03\n","Confusion Matrix:\n","Label: anticipation\n","[[2668  166]\n"," [ 339   86]]\n","\n","Label: optimism\n","[[1739  377]\n"," [ 207  936]]\n","\n","Label: trust\n","[[3050   56]\n"," [ 142   11]]\n","\n","Label: joy\n","[[1632  185]\n"," [ 269 1173]]\n","\n","Label: love\n","[[2551  192]\n"," [ 172  344]]\n","\n","Label: anger\n","[[1969  189]\n"," [ 287  814]]\n","\n","Label: disgust\n","[[1936  224]\n"," [ 301  798]]\n","\n","Label: pessimism\n","[[2709  175]\n"," [ 249  126]]\n","\n","Label: sadness\n","[[2120  179]\n"," [ 336  624]]\n","\n","Label: fear\n","[[2601  173]\n"," [ 118  367]]\n","\n","Label: surprise\n","[[3022   67]\n"," [ 140   30]]\n","\n"]}]},{"cell_type":"markdown","source":["##RoBERTa Single Label"],"metadata":{"id":"sd9FAjJXhtO2"}},{"cell_type":"markdown","source":["NO ROLE"],"metadata":{"id":"5Mr1QFfah1JY"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset\n","df = pd.read_csv(\"full_gsd_single_gpt_data_no_role.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Uh6xV0whv4r","executionInfo":{"status":"ok","timestamp":1723894358243,"user_tz":-120,"elapsed":2270312,"user":{"displayName":"Eugenijus Kliocas","userId":"11927212131385663608"}},"outputId":"6fc4429a-95ae-4698-f1a7-27561918efc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5293, Val Loss: 0.4714\n","Epoch 2, Train Loss: 0.4160, Val Loss: 0.3738\n","Epoch 3, Train Loss: 0.3699, Val Loss: 0.3441\n","Epoch 4, Train Loss: 0.3386, Val Loss: 0.3301\n","Epoch 5, Train Loss: 0.3145, Val Loss: 0.3187\n","Epoch 6, Train Loss: 0.2976, Val Loss: 0.3118\n","Epoch 7, Train Loss: 0.2861, Val Loss: 0.3021\n","Epoch 8, Train Loss: 0.2736, Val Loss: 0.3038\n","Epoch 9, Train Loss: 0.2653, Val Loss: 0.3014\n","Epoch 10, Train Loss: 0.2588, Val Loss: 0.2980\n","Test Accuracy: 27.95\n","Test F1-macro: 52.39\n","Test F1-micro: 69.43\n","Confusion Matrix:\n","Label: anticipation\n","[[2773   61]\n"," [ 374   51]]\n","\n","Label: optimism\n","[[1818  298]\n"," [ 359  784]]\n","\n","Label: trust\n","[[3096   10]\n"," [ 146    7]]\n","\n","Label: joy\n","[[1670  147]\n"," [ 317 1125]]\n","\n","Label: love\n","[[2607  136]\n"," [ 231  285]]\n","\n","Label: anger\n","[[1928  230]\n"," [ 231  870]]\n","\n","Label: disgust\n","[[1893  267]\n"," [ 272  827]]\n","\n","Label: pessimism\n","[[2847   37]\n"," [ 332   43]]\n","\n","Label: sadness\n","[[2116  183]\n"," [ 328  632]]\n","\n","Label: fear\n","[[2705   69]\n"," [ 174  311]]\n","\n","Label: surprise\n","[[3081    8]\n"," [ 152   18]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 1"],"metadata":{"id":"Rj-a9KL4h5GF"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset\n","df = pd.read_csv(\"full_gsd_single_gpt_data_role_1.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xs4Bl4gqh5vx","executionInfo":{"status":"ok","timestamp":1723896628593,"user_tz":-120,"elapsed":2265145,"user":{"displayName":"Eugenijus Kliocas","userId":"11927212131385663608"}},"outputId":"454a7646-f062-4d7f-d9fc-d59d5280571a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5332, Val Loss: 0.4709\n","Epoch 2, Train Loss: 0.4170, Val Loss: 0.3720\n","Epoch 3, Train Loss: 0.3686, Val Loss: 0.3483\n","Epoch 4, Train Loss: 0.3426, Val Loss: 0.3383\n","Epoch 5, Train Loss: 0.3190, Val Loss: 0.3211\n","Epoch 6, Train Loss: 0.3034, Val Loss: 0.3118\n","Epoch 7, Train Loss: 0.2895, Val Loss: 0.3079\n","Epoch 8, Train Loss: 0.2786, Val Loss: 0.3034\n","Epoch 9, Train Loss: 0.2698, Val Loss: 0.3021\n","Epoch 10, Train Loss: 0.2617, Val Loss: 0.2978\n","Test Accuracy: 29.36\n","Test F1-macro: 53.25\n","Test F1-micro: 70.48\n","Confusion Matrix:\n","Label: anticipation\n","[[2800   34]\n"," [ 388   37]]\n","\n","Label: optimism\n","[[1753  363]\n"," [ 275  868]]\n","\n","Label: trust\n","[[3099    7]\n"," [ 151    2]]\n","\n","Label: joy\n","[[1604  213]\n"," [ 238 1204]]\n","\n","Label: love\n","[[2531  212]\n"," [ 180  336]]\n","\n","Label: anger\n","[[1944  214]\n"," [ 234  867]]\n","\n","Label: disgust\n","[[1906  254]\n"," [ 277  822]]\n","\n","Label: pessimism\n","[[2812   72]\n"," [ 292   83]]\n","\n","Label: sadness\n","[[2133  166]\n"," [ 348  612]]\n","\n","Label: fear\n","[[2730   44]\n"," [ 191  294]]\n","\n","Label: surprise\n","[[3084    5]\n"," [ 151   19]]\n","\n"]}]},{"cell_type":"markdown","source":["ROLE 1 + NEWS"],"metadata":{"id":"hvJXiqjkh8LJ"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset\n","df = pd.read_csv(\"full_gsd_single_gpt_data_role_1_news.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2lCsho9h9EO","executionInfo":{"status":"ok","timestamp":1723899002657,"user_tz":-120,"elapsed":2262997,"user":{"displayName":"Eugenijus Kliocas","userId":"11927212131385663608"}},"outputId":"162397c7-26a1-4ad2-9427-8b2aa9c6d5e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5343, Val Loss: 0.4763\n","Epoch 2, Train Loss: 0.4328, Val Loss: 0.3787\n","Epoch 3, Train Loss: 0.3748, Val Loss: 0.3438\n","Epoch 4, Train Loss: 0.3445, Val Loss: 0.3283\n","Epoch 5, Train Loss: 0.3213, Val Loss: 0.3191\n","Epoch 6, Train Loss: 0.3059, Val Loss: 0.3115\n","Epoch 7, Train Loss: 0.2945, Val Loss: 0.3060\n","Epoch 8, Train Loss: 0.2837, Val Loss: 0.3063\n","Epoch 9, Train Loss: 0.2748, Val Loss: 0.3021\n","Epoch 10, Train Loss: 0.2689, Val Loss: 0.2988\n","Test Accuracy: 28.81\n","Test F1-macro: 52.70\n","Test F1-micro: 70.12\n","Confusion Matrix:\n","Label: anticipation\n","[[2800   34]\n"," [ 391   34]]\n","\n","Label: optimism\n","[[1735  381]\n"," [ 254  889]]\n","\n","Label: trust\n","[[3106    0]\n"," [ 152    1]]\n","\n","Label: joy\n","[[1645  172]\n"," [ 263 1179]]\n","\n","Label: love\n","[[2530  213]\n"," [ 163  353]]\n","\n","Label: anger\n","[[1977  181]\n"," [ 278  823]]\n","\n","Label: disgust\n","[[1934  226]\n"," [ 317  782]]\n","\n","Label: pessimism\n","[[2812   72]\n"," [ 294   81]]\n","\n","Label: sadness\n","[[2128  171]\n"," [ 347  613]]\n","\n","Label: fear\n","[[2710   64]\n"," [ 187  298]]\n","\n","Label: surprise\n","[[3082    7]\n"," [ 153   17]]\n","\n"]}]},{"cell_type":"markdown","source":["Increased multi-label"],"metadata":{"id":"dhQVOXTw4CeS"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset\n","df = pd.read_csv(\"full_gsd_increased_gpt_data_multi.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1ffec70c17ae440a93d4958c9483f599","9884bd2a6fd14a1fafcbdd19e9c61eb4","605455bfec134bb7bd665e60d63705b1","ae455097ab29484697d9f52a9750d925","f984811d6eea4771be638803644102b0","ea6bf797f9344ac789b32491a9b94dbf","451ea0d4da3540d888ddde67a9a0b2f2","1aa46fa1ad874ff0a900a60086a5a88f","e9338ac654ad47eab728f3c8e75e7f9d","0fa9470f8fd54b8aac0963c445a83511","0d328f1d995643efbf009430ffc4e063","4f5233f7a85b4cc5b694c808228da264","7285d68e9da345a487fcd6202a1746b7","cf5d8f5123d4456087ce5cb2573c0c24","19ab87b9349c4015b78fdb3ed73e1110","887698bb0bf84241bb5988cc6bf24f05","a97495399adc4589a090f6263dec8919","9202ee81c3084a0a99b8b505a5181fd7","f8725afcbc3d4f2c953431ee1faee1c6","7c1a446ac8fe4fb8acbf94e5a8f5575e","3e2cd25a56d0447cb8e1480b022f96a0","fa73950001a24716a9f3a295f38de8fb","7e268a01a2444bc696a0f53bcc2867e2","b020b789533e4c748129e1e83001c99c","a4d9c62eb6764f08b5ab6c096d8dbd60","751b05f75a214b5ea69f682e3fd94576","26d50baa9a63492cabddb50db4624ac4","3fa412d9b8d7490ea7618292fe34746f","e0b96835ed4f47cb912e851d4e3c3402","de7ef3c29ae14c92a1fbe0cad66f1731","12771d84b0b6482c8c4b91004d30df2e","f3d9bc1074144e24a1a0766a88517514","ff4a02c5640e424b83dad460e52ca58a","1969b2ffa0fa412d944a807d8c3e93c1","7a4614bf86ca4130a3f0faa23ae8c6d1","86c3120a0b9c4955b2a51743150cf260","d2872075fc0242c5a0c436ce451af7c4","74e1f84077eb44559ebeb96500b6770d","856d320562244d0e9979074676600521","d2f9d2f5a7514b3fa6e9211a5b9c397b","f005824fef4849acac5979dec9f39e96","233ed0774f3e45c9a3f2382ae74f833d","b17c6e7900054fee808f6bd1db201b8b","f09db17c39524b0eac1ccda8ad078dd9","0f777898e3af46dda86978a3391f6f4b","b6b975d730a443139c1001b89a66ade2","adabe25462b248ad9463aadba525be1b","8786bf2e12134a8a8748b1d62ee73743","c86fa111794c4d96b2d73032c938477f","07920d19fc6e4458a492338865db8b7d","37f29c16ff6a4ddab96b0dfbeaef9d2b","c811c4a3bc8d472baacd767236bdbd74","57d96ce946644547af55196f8ae2930d","8aa4fb7ac6e64fb296037f15db811751","77acfe065816484789ee2b787dc121ad","3f1254d9a5284a4483e71610d8fca2af","10331b025fb449978944f17b68a5b1e6","46c3c9b6a124459fbf4480be58e0e20d","d3d4fab526fe4a69a9471c4c688c855b","3c94f76efeb04b748233f65e26875c9c","aa06dae160ef48238e771f9c8dd96094","6312dd219f544cc3bfbb1bb684bbec64","b7023865968340249e47dcd126104781","e027c9e909064d109d43ca6443461867","45d252ab38d64b3097f2cfd1a4deeb86","201b511579b949dd95c667a26238f52d"]},"id":"VBM38T0o4D_-","executionInfo":{"status":"ok","timestamp":1723204015892,"user_tz":-120,"elapsed":724226,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"221eb530-e863-4307-98bc-a73367fcf918"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ffec70c17ae440a93d4958c9483f599"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f5233f7a85b4cc5b694c808228da264"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e268a01a2444bc696a0f53bcc2867e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1969b2ffa0fa412d944a807d8c3e93c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f777898e3af46dda86978a3391f6f4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f1254d9a5284a4483e71610d8fca2af"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.6135, Val Loss: 0.4731\n","Epoch 2, Train Loss: 0.4339, Val Loss: 0.3949\n","Epoch 3, Train Loss: 0.3595, Val Loss: 0.3745\n","Epoch 4, Train Loss: 0.3273, Val Loss: 0.3593\n","Epoch 5, Train Loss: 0.3043, Val Loss: 0.3479\n","Epoch 6, Train Loss: 0.2888, Val Loss: 0.3444\n","Epoch 7, Train Loss: 0.2716, Val Loss: 0.3332\n","Epoch 8, Train Loss: 0.2606, Val Loss: 0.3364\n","Epoch 9, Train Loss: 0.2525, Val Loss: 0.3330\n","Epoch 10, Train Loss: 0.2407, Val Loss: 0.3204\n","Test Accuracy: 24.42\n","Test F1-macro: 52.58\n","Test F1-micro: 67.58\n","Confusion Matrix:\n","Label: anticipation\n","[[2698  136]\n"," [ 376   49]]\n","\n","Label: optimism\n","[[1770  346]\n"," [ 279  864]]\n","\n","Label: trust\n","[[2949  157]\n"," [ 135   18]]\n","\n","Label: joy\n","[[1617  200]\n"," [ 283 1159]]\n","\n","Label: love\n","[[2558  185]\n"," [ 213  303]]\n","\n","Label: anger\n","[[1941  217]\n"," [ 268  833]]\n","\n","Label: disgust\n","[[1907  253]\n"," [ 285  814]]\n","\n","Label: pessimism\n","[[2694  190]\n"," [ 253  122]]\n","\n","Label: sadness\n","[[2049  250]\n"," [ 353  607]]\n","\n","Label: fear\n","[[2659  115]\n"," [ 186  299]]\n","\n","Label: surprise\n","[[3048   41]\n"," [ 155   15]]\n","\n"]}]},{"cell_type":"markdown","source":["Increased single-label"],"metadata":{"id":"c2m0PYvo4EaN"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import AutoTokenizer, RobertaForSequenceClassification\n","\n","# Define hyperparameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","dropout = 0.2\n","gradient_accumulation_steps = 4\n","\n","# Load dataset (assuming combined_df and sem_eval_2018_task_1 are defined)\n","df = pd.read_csv(\"full_gsd_increased_gpt_data_single.csv\")\n","df_val = pd.DataFrame(sem_eval_2018_task_1['validation'])\n","df_test = pd.DataFrame(sem_eval_2018_task_1['test'])\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","\n","# Initialize BERT tokenizer and model with dropout\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_columns), hidden_dropout_prob=dropout, attention_probs_dropout_prob=dropout)\n","model.to(device)\n","\n","# Tokenize training data\n","inputs_train = tokenizer(list(df['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_train = torch.tensor(df[label_columns].values, dtype=torch.float32)\n","train_data = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n","train_sampler = RandomSampler(train_data, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))  # Ensuring the shuffle is consistent\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n","\n","# Tokenize validation data\n","inputs_val = tokenizer(list(df_val['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_val = torch.tensor(df_val[label_columns].values, dtype=torch.float32)\n","val_data = TensorDataset(inputs_val['input_ids'], inputs_val['attention_mask'], labels_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            # Free up memory\n","            del input_ids, attention_mask, labels, outputs, loss\n","            torch.cuda.empty_cache()\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","torch.cuda.empty_cache()\n","\n","# Evaluation on test set in batches\n","inputs_test = tokenizer(list(df_test['Tweet']), padding=True, truncation=True, return_tensors=\"pt\")\n","labels_test = torch.tensor(df_test[label_columns].values, dtype=torch.float32)\n","\n","test_data = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","model.eval()\n","all_logits = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        all_logits.append(logits.cpu())\n","\n","        # Free up memory\n","        del input_ids, attention_mask, labels, outputs, logits\n","        torch.cuda.empty_cache()\n","\n","logits_test = torch.cat(all_logits, dim=0)\n","predicted_labels_test = (torch.sigmoid(logits_test) > 0.5).numpy().astype(int)\n","\n","# Calculate test metrics\n","accuracy_test = accuracy_score(labels_test, predicted_labels_test) * 100\n","f1_macro_test = f1_score(labels_test, predicted_labels_test, average='macro') * 100\n","f1_micro_test = f1_score(labels_test, predicted_labels_test, average='micro') * 100\n","confusion_matrix_test = multilabel_confusion_matrix(labels_test, predicted_labels_test)\n","\n","print(f\"Test Accuracy: {accuracy_test:.2f}\")\n","print(f\"Test F1-macro: {f1_macro_test:.2f}\")\n","print(f\"Test F1-micro: {f1_micro_test:.2f}\")\n","print(\"Confusion Matrix:\")\n","for label, matrix in zip(label_columns, confusion_matrix_test):\n","    print(f\"Label: {label}\")\n","    print(matrix)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ayyl4pMm4Flp","executionInfo":{"status":"ok","timestamp":1723204751656,"user_tz":-120,"elapsed":720967,"user":{"displayName":"Marija","userId":"12050775141700439362"}},"outputId":"bea27b73-e49e-402d-dfc0-6ec2387768a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.5494, Val Loss: 0.4948\n","Epoch 2, Train Loss: 0.4178, Val Loss: 0.4276\n","Epoch 3, Train Loss: 0.3669, Val Loss: 0.3920\n","Epoch 4, Train Loss: 0.3266, Val Loss: 0.3632\n","Epoch 5, Train Loss: 0.2968, Val Loss: 0.3502\n","Epoch 6, Train Loss: 0.2746, Val Loss: 0.3341\n","Epoch 7, Train Loss: 0.2524, Val Loss: 0.3270\n","Epoch 8, Train Loss: 0.2405, Val Loss: 0.3238\n","Epoch 9, Train Loss: 0.2267, Val Loss: 0.3175\n","Epoch 10, Train Loss: 0.2176, Val Loss: 0.3152\n","Test Accuracy: 25.50\n","Test F1-macro: 49.97\n","Test F1-micro: 67.15\n","Confusion Matrix:\n","Label: anticipation\n","[[2787   47]\n"," [ 382   43]]\n","\n","Label: optimism\n","[[1850  266]\n"," [ 408  735]]\n","\n","Label: trust\n","[[3090   16]\n"," [ 145    8]]\n","\n","Label: joy\n","[[1668  149]\n"," [ 340 1102]]\n","\n","Label: love\n","[[2604  139]\n"," [ 257  259]]\n","\n","Label: anger\n","[[1937  221]\n"," [ 273  828]]\n","\n","Label: disgust\n","[[1885  275]\n"," [ 273  826]]\n","\n","Label: pessimism\n","[[2833   51]\n"," [ 340   35]]\n","\n","Label: sadness\n","[[2118  181]\n"," [ 393  567]]\n","\n","Label: fear\n","[[2660  114]\n"," [ 183  302]]\n","\n","Label: surprise\n","[[3076   13]\n"," [ 154   16]]\n","\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1ffec70c17ae440a93d4958c9483f599":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9884bd2a6fd14a1fafcbdd19e9c61eb4","IPY_MODEL_605455bfec134bb7bd665e60d63705b1","IPY_MODEL_ae455097ab29484697d9f52a9750d925"],"layout":"IPY_MODEL_f984811d6eea4771be638803644102b0"}},"9884bd2a6fd14a1fafcbdd19e9c61eb4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea6bf797f9344ac789b32491a9b94dbf","placeholder":"​","style":"IPY_MODEL_451ea0d4da3540d888ddde67a9a0b2f2","value":"tokenizer_config.json: 100%"}},"605455bfec134bb7bd665e60d63705b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1aa46fa1ad874ff0a900a60086a5a88f","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e9338ac654ad47eab728f3c8e75e7f9d","value":25}},"ae455097ab29484697d9f52a9750d925":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fa9470f8fd54b8aac0963c445a83511","placeholder":"​","style":"IPY_MODEL_0d328f1d995643efbf009430ffc4e063","value":" 25.0/25.0 [00:00&lt;00:00, 835B/s]"}},"f984811d6eea4771be638803644102b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea6bf797f9344ac789b32491a9b94dbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"451ea0d4da3540d888ddde67a9a0b2f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1aa46fa1ad874ff0a900a60086a5a88f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9338ac654ad47eab728f3c8e75e7f9d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fa9470f8fd54b8aac0963c445a83511":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d328f1d995643efbf009430ffc4e063":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f5233f7a85b4cc5b694c808228da264":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7285d68e9da345a487fcd6202a1746b7","IPY_MODEL_cf5d8f5123d4456087ce5cb2573c0c24","IPY_MODEL_19ab87b9349c4015b78fdb3ed73e1110"],"layout":"IPY_MODEL_887698bb0bf84241bb5988cc6bf24f05"}},"7285d68e9da345a487fcd6202a1746b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a97495399adc4589a090f6263dec8919","placeholder":"​","style":"IPY_MODEL_9202ee81c3084a0a99b8b505a5181fd7","value":"config.json: 100%"}},"cf5d8f5123d4456087ce5cb2573c0c24":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8725afcbc3d4f2c953431ee1faee1c6","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c1a446ac8fe4fb8acbf94e5a8f5575e","value":481}},"19ab87b9349c4015b78fdb3ed73e1110":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e2cd25a56d0447cb8e1480b022f96a0","placeholder":"​","style":"IPY_MODEL_fa73950001a24716a9f3a295f38de8fb","value":" 481/481 [00:00&lt;00:00, 19.3kB/s]"}},"887698bb0bf84241bb5988cc6bf24f05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a97495399adc4589a090f6263dec8919":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9202ee81c3084a0a99b8b505a5181fd7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8725afcbc3d4f2c953431ee1faee1c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c1a446ac8fe4fb8acbf94e5a8f5575e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e2cd25a56d0447cb8e1480b022f96a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa73950001a24716a9f3a295f38de8fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e268a01a2444bc696a0f53bcc2867e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b020b789533e4c748129e1e83001c99c","IPY_MODEL_a4d9c62eb6764f08b5ab6c096d8dbd60","IPY_MODEL_751b05f75a214b5ea69f682e3fd94576"],"layout":"IPY_MODEL_26d50baa9a63492cabddb50db4624ac4"}},"b020b789533e4c748129e1e83001c99c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fa412d9b8d7490ea7618292fe34746f","placeholder":"​","style":"IPY_MODEL_e0b96835ed4f47cb912e851d4e3c3402","value":"vocab.json: 100%"}},"a4d9c62eb6764f08b5ab6c096d8dbd60":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de7ef3c29ae14c92a1fbe0cad66f1731","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_12771d84b0b6482c8c4b91004d30df2e","value":898823}},"751b05f75a214b5ea69f682e3fd94576":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3d9bc1074144e24a1a0766a88517514","placeholder":"​","style":"IPY_MODEL_ff4a02c5640e424b83dad460e52ca58a","value":" 899k/899k [00:00&lt;00:00, 2.76MB/s]"}},"26d50baa9a63492cabddb50db4624ac4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fa412d9b8d7490ea7618292fe34746f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0b96835ed4f47cb912e851d4e3c3402":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de7ef3c29ae14c92a1fbe0cad66f1731":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12771d84b0b6482c8c4b91004d30df2e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3d9bc1074144e24a1a0766a88517514":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff4a02c5640e424b83dad460e52ca58a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1969b2ffa0fa412d944a807d8c3e93c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a4614bf86ca4130a3f0faa23ae8c6d1","IPY_MODEL_86c3120a0b9c4955b2a51743150cf260","IPY_MODEL_d2872075fc0242c5a0c436ce451af7c4"],"layout":"IPY_MODEL_74e1f84077eb44559ebeb96500b6770d"}},"7a4614bf86ca4130a3f0faa23ae8c6d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_856d320562244d0e9979074676600521","placeholder":"​","style":"IPY_MODEL_d2f9d2f5a7514b3fa6e9211a5b9c397b","value":"merges.txt: 100%"}},"86c3120a0b9c4955b2a51743150cf260":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f005824fef4849acac5979dec9f39e96","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_233ed0774f3e45c9a3f2382ae74f833d","value":456318}},"d2872075fc0242c5a0c436ce451af7c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b17c6e7900054fee808f6bd1db201b8b","placeholder":"​","style":"IPY_MODEL_f09db17c39524b0eac1ccda8ad078dd9","value":" 456k/456k [00:00&lt;00:00, 2.83MB/s]"}},"74e1f84077eb44559ebeb96500b6770d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"856d320562244d0e9979074676600521":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2f9d2f5a7514b3fa6e9211a5b9c397b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f005824fef4849acac5979dec9f39e96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"233ed0774f3e45c9a3f2382ae74f833d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b17c6e7900054fee808f6bd1db201b8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f09db17c39524b0eac1ccda8ad078dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f777898e3af46dda86978a3391f6f4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6b975d730a443139c1001b89a66ade2","IPY_MODEL_adabe25462b248ad9463aadba525be1b","IPY_MODEL_8786bf2e12134a8a8748b1d62ee73743"],"layout":"IPY_MODEL_c86fa111794c4d96b2d73032c938477f"}},"b6b975d730a443139c1001b89a66ade2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07920d19fc6e4458a492338865db8b7d","placeholder":"​","style":"IPY_MODEL_37f29c16ff6a4ddab96b0dfbeaef9d2b","value":"tokenizer.json: 100%"}},"adabe25462b248ad9463aadba525be1b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c811c4a3bc8d472baacd767236bdbd74","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57d96ce946644547af55196f8ae2930d","value":1355863}},"8786bf2e12134a8a8748b1d62ee73743":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8aa4fb7ac6e64fb296037f15db811751","placeholder":"​","style":"IPY_MODEL_77acfe065816484789ee2b787dc121ad","value":" 1.36M/1.36M [00:00&lt;00:00, 15.7MB/s]"}},"c86fa111794c4d96b2d73032c938477f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07920d19fc6e4458a492338865db8b7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37f29c16ff6a4ddab96b0dfbeaef9d2b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c811c4a3bc8d472baacd767236bdbd74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57d96ce946644547af55196f8ae2930d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8aa4fb7ac6e64fb296037f15db811751":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77acfe065816484789ee2b787dc121ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f1254d9a5284a4483e71610d8fca2af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10331b025fb449978944f17b68a5b1e6","IPY_MODEL_46c3c9b6a124459fbf4480be58e0e20d","IPY_MODEL_d3d4fab526fe4a69a9471c4c688c855b"],"layout":"IPY_MODEL_3c94f76efeb04b748233f65e26875c9c"}},"10331b025fb449978944f17b68a5b1e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa06dae160ef48238e771f9c8dd96094","placeholder":"​","style":"IPY_MODEL_6312dd219f544cc3bfbb1bb684bbec64","value":"model.safetensors: 100%"}},"46c3c9b6a124459fbf4480be58e0e20d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7023865968340249e47dcd126104781","max":498818054,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e027c9e909064d109d43ca6443461867","value":498818054}},"d3d4fab526fe4a69a9471c4c688c855b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45d252ab38d64b3097f2cfd1a4deeb86","placeholder":"​","style":"IPY_MODEL_201b511579b949dd95c667a26238f52d","value":" 499M/499M [00:03&lt;00:00, 190MB/s]"}},"3c94f76efeb04b748233f65e26875c9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa06dae160ef48238e771f9c8dd96094":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6312dd219f544cc3bfbb1bb684bbec64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7023865968340249e47dcd126104781":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e027c9e909064d109d43ca6443461867":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45d252ab38d64b3097f2cfd1a4deeb86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"201b511579b949dd95c667a26238f52d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7bddee1aec0f4132887202c62e7262cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6c288df58d9e4d9a805ce3c88a9c9d52","IPY_MODEL_405f649dea22401fa1d82ca4dd080e1c","IPY_MODEL_77aa5ca91fb74a23934866e76e6bc46f"],"layout":"IPY_MODEL_cc6ceaacedf44731b21bdfeee8d4dd5a"}},"6c288df58d9e4d9a805ce3c88a9c9d52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_deaecb17f6bc4d5f912ba64406e735a3","placeholder":"​","style":"IPY_MODEL_3ba526087e8f474fbb0c09b207af30e1","value":"tokenizer_config.json: 100%"}},"405f649dea22401fa1d82ca4dd080e1c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe7c46f25255423495247896ce8c2d2e","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_199c46024a8f4ba59d288635425b85ba","value":48}},"77aa5ca91fb74a23934866e76e6bc46f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc7df7259a47473e95c5429573b47970","placeholder":"​","style":"IPY_MODEL_97221faa12dc49cfb0ebb8de84cd63a1","value":" 48.0/48.0 [00:00&lt;00:00, 1.56kB/s]"}},"cc6ceaacedf44731b21bdfeee8d4dd5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"deaecb17f6bc4d5f912ba64406e735a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ba526087e8f474fbb0c09b207af30e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe7c46f25255423495247896ce8c2d2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"199c46024a8f4ba59d288635425b85ba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc7df7259a47473e95c5429573b47970":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97221faa12dc49cfb0ebb8de84cd63a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6db730becba4f9296ef19b26feb1e9e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae1c03fca0fb40d1bb3f8aaa839ef1dc","IPY_MODEL_0e2decaac6f54816a5f8b9b752cbbf6d","IPY_MODEL_182bfc0fa50b4531a8611361a3849e94"],"layout":"IPY_MODEL_095d200f1dcf43a5aca87e236fbc159e"}},"ae1c03fca0fb40d1bb3f8aaa839ef1dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0cac7ca2cbc400d8ce1636781968e13","placeholder":"​","style":"IPY_MODEL_a2ebc7246d9d4f0d9dad1852024a17bc","value":"vocab.txt: 100%"}},"0e2decaac6f54816a5f8b9b752cbbf6d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_887d61a568c740e69987bbb83cc6dc5b","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_967073b9809a441198d1a4125f5ff82b","value":231508}},"182bfc0fa50b4531a8611361a3849e94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbd6744c051c4e648fb6ac559f655ddb","placeholder":"​","style":"IPY_MODEL_0ffc082574474a3b8fdf2c2bded46145","value":" 232k/232k [00:00&lt;00:00, 3.70MB/s]"}},"095d200f1dcf43a5aca87e236fbc159e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0cac7ca2cbc400d8ce1636781968e13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2ebc7246d9d4f0d9dad1852024a17bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"887d61a568c740e69987bbb83cc6dc5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"967073b9809a441198d1a4125f5ff82b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cbd6744c051c4e648fb6ac559f655ddb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ffc082574474a3b8fdf2c2bded46145":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f69455d5d6894c6983b0507a04d86319":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9f05598d6f64ca9b0a83d7703bae362","IPY_MODEL_a51c0322bf3d4269842db154624d1e81","IPY_MODEL_dfdffde72ed844bb879107546e43e763"],"layout":"IPY_MODEL_fe9d07b1a8044dfeb72de5f2141a6608"}},"d9f05598d6f64ca9b0a83d7703bae362":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b354d55a51804907a2b03f2af0748840","placeholder":"​","style":"IPY_MODEL_4e20908a793f40929e78a1fb067bfdc4","value":"tokenizer.json: 100%"}},"a51c0322bf3d4269842db154624d1e81":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5245109373a1459999d84b37777d52e4","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15efca04ef8443a48c4eeb3ec377f092","value":466062}},"dfdffde72ed844bb879107546e43e763":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f578a03190d14343bfb177ed5d3497ab","placeholder":"​","style":"IPY_MODEL_1014b271b4544c8da6dc192edb5a1acc","value":" 466k/466k [00:00&lt;00:00, 6.04MB/s]"}},"fe9d07b1a8044dfeb72de5f2141a6608":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b354d55a51804907a2b03f2af0748840":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e20908a793f40929e78a1fb067bfdc4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5245109373a1459999d84b37777d52e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15efca04ef8443a48c4eeb3ec377f092":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f578a03190d14343bfb177ed5d3497ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1014b271b4544c8da6dc192edb5a1acc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46239bf76d6c4e66bc2c6ad233052a7d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_56c032bded6443979780a48a8c5afa7a","IPY_MODEL_ee9196323eb644a9a3cb157655e245d8","IPY_MODEL_ecdbcd379a6945fab99273f4db2c7127"],"layout":"IPY_MODEL_98f42cdc44c14afd9f4b71145585d048"}},"56c032bded6443979780a48a8c5afa7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5adf0bf75f864f399cc23392506ba709","placeholder":"​","style":"IPY_MODEL_69d95ca8f8404e65a30a6dd1618f5267","value":"config.json: 100%"}},"ee9196323eb644a9a3cb157655e245d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_422e24e03a4d408393a7ff774ec61bfb","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f120d79121844bb9afed3d907f5c8a78","value":570}},"ecdbcd379a6945fab99273f4db2c7127":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f399adad53143289ad6ec792b07206f","placeholder":"​","style":"IPY_MODEL_83e685a4519c43e59946ba6716d4372f","value":" 570/570 [00:00&lt;00:00, 9.97kB/s]"}},"98f42cdc44c14afd9f4b71145585d048":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5adf0bf75f864f399cc23392506ba709":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69d95ca8f8404e65a30a6dd1618f5267":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"422e24e03a4d408393a7ff774ec61bfb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f120d79121844bb9afed3d907f5c8a78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f399adad53143289ad6ec792b07206f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83e685a4519c43e59946ba6716d4372f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b64271812de440629b5fdbb42c4bfc4a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be64d9b530ed463b957db50a3d27c41b","IPY_MODEL_fc3d963c777d49a289293c749ae45180","IPY_MODEL_466c8daa6dcd4da99021ec5d9819588c"],"layout":"IPY_MODEL_c4997318b53f4a6caf54e0e5395b0c75"}},"be64d9b530ed463b957db50a3d27c41b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a8514269f7746c2979f171c50baf55d","placeholder":"​","style":"IPY_MODEL_09579506dfb14967b681256651b0529f","value":"model.safetensors: 100%"}},"fc3d963c777d49a289293c749ae45180":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9d3bb56d67f418697ef9f0bb91cb5ef","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ea63dd2beb194fae986ee7bc7ad1d458","value":440449768}},"466c8daa6dcd4da99021ec5d9819588c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0168e3e81c9a4324bbdc40c57312a820","placeholder":"​","style":"IPY_MODEL_f8a8c7588be24f2289cd2cae947ce6a8","value":" 440M/440M [00:04&lt;00:00, 115MB/s]"}},"c4997318b53f4a6caf54e0e5395b0c75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a8514269f7746c2979f171c50baf55d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09579506dfb14967b681256651b0529f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9d3bb56d67f418697ef9f0bb91cb5ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea63dd2beb194fae986ee7bc7ad1d458":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0168e3e81c9a4324bbdc40c57312a820":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8a8c7588be24f2289cd2cae947ce6a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b54e2e8e9fe4cb09e31cffa598b3083":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0da0af466498429891335a33c051c6be","IPY_MODEL_44d3675ae77e482c9e42785c67a07804","IPY_MODEL_0303055eec1d495381833a4fd1956c9a"],"layout":"IPY_MODEL_84c43b090cad4a2798156602145be7cd"}},"0da0af466498429891335a33c051c6be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8439dd8e228040ee89cf41f064a7c294","placeholder":"​","style":"IPY_MODEL_555697e85ccd4931bd6fde37e7bbeecc","value":"tokenizer_config.json: 100%"}},"44d3675ae77e482c9e42785c67a07804":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d363f44f8174d9e87186b3f385dc30b","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f28b4bd4dbce475e9c2f70a5d20070ba","value":25}},"0303055eec1d495381833a4fd1956c9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1e0d271141b4207a77d115b130c9b57","placeholder":"​","style":"IPY_MODEL_f4011992fa78497bb03f2dd597662a90","value":" 25.0/25.0 [00:00&lt;00:00, 1.89kB/s]"}},"84c43b090cad4a2798156602145be7cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8439dd8e228040ee89cf41f064a7c294":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"555697e85ccd4931bd6fde37e7bbeecc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d363f44f8174d9e87186b3f385dc30b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f28b4bd4dbce475e9c2f70a5d20070ba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1e0d271141b4207a77d115b130c9b57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4011992fa78497bb03f2dd597662a90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ca55695d3c44faaacfdb2abdc49773f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5d8586e66c5242aeb63b5ed41625925f","IPY_MODEL_0a8a10ac097d47c0bccc10d0f8ea28b5","IPY_MODEL_2b5b4cc3a88f413db5cc18344f08297d"],"layout":"IPY_MODEL_21ffdf271c954a97a2fa7740e0e44d98"}},"5d8586e66c5242aeb63b5ed41625925f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18604f8e3c014841b916f4a7f4c48175","placeholder":"​","style":"IPY_MODEL_300e1899ad9b4603a3a741bc235859a1","value":"config.json: 100%"}},"0a8a10ac097d47c0bccc10d0f8ea28b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_031804f4031048b9af3dc25468c04b66","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_511c3aca2b11405fb7282a6f433163ad","value":481}},"2b5b4cc3a88f413db5cc18344f08297d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5f78f5fde6644fe9a945ffb663613b4","placeholder":"​","style":"IPY_MODEL_283ccdf03c334339af8ba60c8adf546a","value":" 481/481 [00:00&lt;00:00, 36.0kB/s]"}},"21ffdf271c954a97a2fa7740e0e44d98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18604f8e3c014841b916f4a7f4c48175":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"300e1899ad9b4603a3a741bc235859a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"031804f4031048b9af3dc25468c04b66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"511c3aca2b11405fb7282a6f433163ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b5f78f5fde6644fe9a945ffb663613b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"283ccdf03c334339af8ba60c8adf546a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6afe62bc97044592931a83d07f837486":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_50bd0b64789a477f9e44a175f14992b1","IPY_MODEL_d3bd84410c4d486bbd11af404209b240","IPY_MODEL_05019c0eef7c4c7782d13e0c1a53e8f4"],"layout":"IPY_MODEL_744614491461405c84ca4380c9cdc1d3"}},"50bd0b64789a477f9e44a175f14992b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efd98c9a9b3c4eb3acf0693a5ac25984","placeholder":"​","style":"IPY_MODEL_7dd79836cfb04f1cbf7533c9052bfdd7","value":"vocab.json: 100%"}},"d3bd84410c4d486bbd11af404209b240":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8299af08714f40769725ccf680db8d6e","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ef719f89673450eb110715c3a5ec13e","value":898823}},"05019c0eef7c4c7782d13e0c1a53e8f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e95b5a10ed9430ab4532085a1d807d2","placeholder":"​","style":"IPY_MODEL_39b9faee3d314c1f8af015e700bc9002","value":" 899k/899k [00:00&lt;00:00, 3.48MB/s]"}},"744614491461405c84ca4380c9cdc1d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efd98c9a9b3c4eb3acf0693a5ac25984":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dd79836cfb04f1cbf7533c9052bfdd7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8299af08714f40769725ccf680db8d6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ef719f89673450eb110715c3a5ec13e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e95b5a10ed9430ab4532085a1d807d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39b9faee3d314c1f8af015e700bc9002":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c2a5fb577c3448f8509b9a53ffaf279":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c6159beaf04c4889b193f2a641bc65ac","IPY_MODEL_a7c74ee1c0d847ec82479cee32e29950","IPY_MODEL_de69cabeec4a47e481634cf3230bdc8d"],"layout":"IPY_MODEL_f5c829e3dab3474cbfc8156682d20e35"}},"c6159beaf04c4889b193f2a641bc65ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_182bd70bd3894374906e68883892717f","placeholder":"​","style":"IPY_MODEL_ffacab2372144df899cc1efe8023f73e","value":"merges.txt: 100%"}},"a7c74ee1c0d847ec82479cee32e29950":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0eb5498a28ad4006b3b3abfad9e5fa0f","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43f4ddfdb6484ea895c0cff6ee40f2c6","value":456318}},"de69cabeec4a47e481634cf3230bdc8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab23d57777d540d8a52d62c9cda51246","placeholder":"​","style":"IPY_MODEL_dfc4cd5c48be428d817964beecce4432","value":" 456k/456k [00:00&lt;00:00, 2.37MB/s]"}},"f5c829e3dab3474cbfc8156682d20e35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"182bd70bd3894374906e68883892717f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffacab2372144df899cc1efe8023f73e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0eb5498a28ad4006b3b3abfad9e5fa0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43f4ddfdb6484ea895c0cff6ee40f2c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab23d57777d540d8a52d62c9cda51246":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfc4cd5c48be428d817964beecce4432":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9057b7919114b09b64c76ab163528c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f38a0ff569b475c93832986e02ebc8d","IPY_MODEL_e625def06f304fc2b75a0cb154b88970","IPY_MODEL_70318ae86cfa431489dbc2403aac9730"],"layout":"IPY_MODEL_9e46422db1d34d9199cbd4328c1961d5"}},"8f38a0ff569b475c93832986e02ebc8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f78b7ae6991455198e589d2a5bbd174","placeholder":"​","style":"IPY_MODEL_2e230e1dcd42461f847dbdc7a60be60e","value":"tokenizer.json: 100%"}},"e625def06f304fc2b75a0cb154b88970":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f05db8e097448faacda27d27bc990f7","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96462350f53a4f749e6c020b65951ee6","value":1355863}},"70318ae86cfa431489dbc2403aac9730":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ece8ed0b580a4fe587f5adfec031b2e7","placeholder":"​","style":"IPY_MODEL_31e7a5aa455a4b2997292578e213acb7","value":" 1.36M/1.36M [00:00&lt;00:00, 6.66MB/s]"}},"9e46422db1d34d9199cbd4328c1961d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f78b7ae6991455198e589d2a5bbd174":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e230e1dcd42461f847dbdc7a60be60e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f05db8e097448faacda27d27bc990f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96462350f53a4f749e6c020b65951ee6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ece8ed0b580a4fe587f5adfec031b2e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31e7a5aa455a4b2997292578e213acb7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd2588bfbb9845b8a8a6fd1b8173dbf5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_68f574f032ed4f58947177518078f913","IPY_MODEL_3ae07d733ea64116accd8dbca713101f","IPY_MODEL_f79afb706af44093b4527399c51035e5"],"layout":"IPY_MODEL_aca7920e2cbe464d8c5cb3f4d9e216ea"}},"68f574f032ed4f58947177518078f913":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0d153d800a948a092c29eb01848fb5a","placeholder":"​","style":"IPY_MODEL_0f8f12878593409ebf6ed3ccc92df012","value":"model.safetensors: 100%"}},"3ae07d733ea64116accd8dbca713101f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b1e68ddbaa44de89e89b5a12540602b","max":498818054,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3799cbe4f2b943298572a118f0764ae2","value":498818054}},"f79afb706af44093b4527399c51035e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47c4b282eaac45b58dcb06a82df86106","placeholder":"​","style":"IPY_MODEL_ef50f581d00b43afbc344c521de1be54","value":" 499M/499M [00:04&lt;00:00, 149MB/s]"}},"aca7920e2cbe464d8c5cb3f4d9e216ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0d153d800a948a092c29eb01848fb5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f8f12878593409ebf6ed3ccc92df012":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b1e68ddbaa44de89e89b5a12540602b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3799cbe4f2b943298572a118f0764ae2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"47c4b282eaac45b58dcb06a82df86106":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef50f581d00b43afbc344c521de1be54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91a82e1b70714e29a77a1611655ef5a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_65fb8157b96d43ba994c030696190571","IPY_MODEL_c1204ce91caa453a981a77e896ba3db4","IPY_MODEL_0c00497aba114c818c24ee354feae1f1"],"layout":"IPY_MODEL_0dbdf7616cba40a39ae8c3978e961257"}},"65fb8157b96d43ba994c030696190571":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73ce93af19284581b73d078cdc0f5f81","placeholder":"​","style":"IPY_MODEL_f8792981ec574e62bb8ae05e744bd7f2","value":"tokenizer_config.json: 100%"}},"c1204ce91caa453a981a77e896ba3db4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6a29dfcc1bf4296b784ea3c38ce83f4","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb1166e0bb514f7eb76fa9ac50de6600","value":25}},"0c00497aba114c818c24ee354feae1f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_554ecfda6a734e7a9cf99c465e0c3499","placeholder":"​","style":"IPY_MODEL_8490b32431574356ae7fc35c97039a66","value":" 25.0/25.0 [00:00&lt;00:00, 318B/s]"}},"0dbdf7616cba40a39ae8c3978e961257":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73ce93af19284581b73d078cdc0f5f81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8792981ec574e62bb8ae05e744bd7f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6a29dfcc1bf4296b784ea3c38ce83f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb1166e0bb514f7eb76fa9ac50de6600":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"554ecfda6a734e7a9cf99c465e0c3499":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8490b32431574356ae7fc35c97039a66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f46224ead6f147279b9f88a2be97f52b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0cabd5a963454dacab1446f25862a795","IPY_MODEL_16e1fbaebafe4683a9733601decbb1b6","IPY_MODEL_6f19626c11334ba684922cc75230e141"],"layout":"IPY_MODEL_0d9068a0f83d4b00a42d7e283573de24"}},"0cabd5a963454dacab1446f25862a795":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46fe8041e8e547c7a9ba2894757f3d04","placeholder":"​","style":"IPY_MODEL_cc42f4975bf846c2850ab2740a536a5d","value":"config.json: 100%"}},"16e1fbaebafe4683a9733601decbb1b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5226143687024cfc8f52b5998cdbce42","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c840bb8e4004417c9e22ddf023585f6e","value":481}},"6f19626c11334ba684922cc75230e141":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcc98c0e80b744699c05f245c38e10b8","placeholder":"​","style":"IPY_MODEL_39d972d6666c48c6a4e003cc0a2a8e6b","value":" 481/481 [00:00&lt;00:00, 6.24kB/s]"}},"0d9068a0f83d4b00a42d7e283573de24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46fe8041e8e547c7a9ba2894757f3d04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc42f4975bf846c2850ab2740a536a5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5226143687024cfc8f52b5998cdbce42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c840bb8e4004417c9e22ddf023585f6e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dcc98c0e80b744699c05f245c38e10b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39d972d6666c48c6a4e003cc0a2a8e6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"790a390d36e74bed84d957ef61278860":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_641cdfe0a3134dd896db9c091827c7d3","IPY_MODEL_8bfbdf9e31574aad82acbe472c8ac2c5","IPY_MODEL_0ab60fcadf0c409bbd37df509bcfbd7c"],"layout":"IPY_MODEL_6d57f97cf4c046b78e8e2ce57e4a2bc9"}},"641cdfe0a3134dd896db9c091827c7d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f741992852d14c96a757261995101f8d","placeholder":"​","style":"IPY_MODEL_b95ab8021d0844579ffc978414a5163e","value":"vocab.json: 100%"}},"8bfbdf9e31574aad82acbe472c8ac2c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_43cf4a40910b422a802047820c52fe78","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_721daa733066437d95c8521cf2021cc7","value":898823}},"0ab60fcadf0c409bbd37df509bcfbd7c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_814bb2caa4b64e5dbbfb17029030c9a8","placeholder":"​","style":"IPY_MODEL_bb44ca8b866f43f1bb49d6d6c7f3d50c","value":" 899k/899k [00:00&lt;00:00, 7.87MB/s]"}},"6d57f97cf4c046b78e8e2ce57e4a2bc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f741992852d14c96a757261995101f8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b95ab8021d0844579ffc978414a5163e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43cf4a40910b422a802047820c52fe78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"721daa733066437d95c8521cf2021cc7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"814bb2caa4b64e5dbbfb17029030c9a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb44ca8b866f43f1bb49d6d6c7f3d50c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff4a1fe1707f4a2d958cbb49c81f0077":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_24b256d587f647d9a855f18760baa4f3","IPY_MODEL_831a548017294d33855057e8dddd631f","IPY_MODEL_f45cafdaa73f4edd9b257b6093717709"],"layout":"IPY_MODEL_aec4d22bdd584758b5c56f9c9beac363"}},"24b256d587f647d9a855f18760baa4f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9b6a5a3f83d4314aa1b2a1c20449a6d","placeholder":"​","style":"IPY_MODEL_92dacb3f38eb4643954dddb0b001d720","value":"merges.txt: 100%"}},"831a548017294d33855057e8dddd631f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_edf4a08b555b4e33938faf3b99b0a940","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_592905c32e804d40835ef62865af9265","value":456318}},"f45cafdaa73f4edd9b257b6093717709":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33ae7d12d6c54681b2a1d886c5912b10","placeholder":"​","style":"IPY_MODEL_b0759dcdfe3046f09e0ecb8148681645","value":" 456k/456k [00:00&lt;00:00, 3.09MB/s]"}},"aec4d22bdd584758b5c56f9c9beac363":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9b6a5a3f83d4314aa1b2a1c20449a6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92dacb3f38eb4643954dddb0b001d720":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"edf4a08b555b4e33938faf3b99b0a940":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"592905c32e804d40835ef62865af9265":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33ae7d12d6c54681b2a1d886c5912b10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0759dcdfe3046f09e0ecb8148681645":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38931569ec8d4304bc296062b4340df7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5517ce7f22d84909ab8dafcd5feff1e7","IPY_MODEL_e88463f146544d3693d2482913af6503","IPY_MODEL_4086a48615b74850be0c50f4b5856f89"],"layout":"IPY_MODEL_ce204ed931784055bb9784189b5c2f33"}},"5517ce7f22d84909ab8dafcd5feff1e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_634f59ad8b1847af942866520268d9a0","placeholder":"​","style":"IPY_MODEL_984c68c3cda14dbdbfcdb98702d68d3f","value":"tokenizer.json: 100%"}},"e88463f146544d3693d2482913af6503":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_affd217187b045bf82391bc1a9b79621","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e9efecb54d644251b597cf17b22d5f1b","value":1355863}},"4086a48615b74850be0c50f4b5856f89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab78609dc9514818a293787ff308cdbe","placeholder":"​","style":"IPY_MODEL_09b70881944d41c19f880bc72e2d9f84","value":" 1.36M/1.36M [00:00&lt;00:00, 11.9MB/s]"}},"ce204ed931784055bb9784189b5c2f33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"634f59ad8b1847af942866520268d9a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"984c68c3cda14dbdbfcdb98702d68d3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"affd217187b045bf82391bc1a9b79621":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9efecb54d644251b597cf17b22d5f1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab78609dc9514818a293787ff308cdbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09b70881944d41c19f880bc72e2d9f84":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6399928716449c2ad78e8a71e40545b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9917fe1d8c954fba88ca8fede7c19ac0","IPY_MODEL_7909112b5cb444adb9a9e8c5d46b7743","IPY_MODEL_486a5402ab8c48c1b5c36e0a65833580"],"layout":"IPY_MODEL_de0e21c186704735936e7efa14ec4e3b"}},"9917fe1d8c954fba88ca8fede7c19ac0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2854116fbc54233b134080c8eccee34","placeholder":"​","style":"IPY_MODEL_3f54e4d272d84b89b5d27cc12fcebcf4","value":"model.safetensors: 100%"}},"7909112b5cb444adb9a9e8c5d46b7743":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b96e13426313497bbf83dee7dc778f4c","max":498818054,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ca3e9b8d9d448f59da073ec184d205b","value":498818054}},"486a5402ab8c48c1b5c36e0a65833580":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1aa0363fd289404eb454aa30f0917e46","placeholder":"​","style":"IPY_MODEL_ede7bb2f06164cf78df342bdc3f7f475","value":" 499M/499M [00:09&lt;00:00, 47.4MB/s]"}},"de0e21c186704735936e7efa14ec4e3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2854116fbc54233b134080c8eccee34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f54e4d272d84b89b5d27cc12fcebcf4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b96e13426313497bbf83dee7dc778f4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ca3e9b8d9d448f59da073ec184d205b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1aa0363fd289404eb454aa30f0917e46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ede7bb2f06164cf78df342bdc3f7f475":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d6deea706164b9e9b713601885d5b3a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_855683555af249bfa7876ab80b990114","IPY_MODEL_67f77135f51f4ae794e2fd8c353ff430","IPY_MODEL_e021df59e25c4f79af322ff69a43385d"],"layout":"IPY_MODEL_9ef5272104fc43298ef1ca027522b971"}},"855683555af249bfa7876ab80b990114":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c49e3b8e6d104db8ac3f6218e15c73e8","placeholder":"​","style":"IPY_MODEL_2f66a7a685cb4ede836593a7df183093","value":"Downloading builder script: 100%"}},"67f77135f51f4ae794e2fd8c353ff430":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e43d23a83c4840dd93dfdd289bb3a048","max":6288,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75dc1f2eaa1e4505b86f4b319a0da6ef","value":6288}},"e021df59e25c4f79af322ff69a43385d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cf6b200f0764672b5380ee6b0b025b3","placeholder":"​","style":"IPY_MODEL_58bbe52516574330ac4989c269794bb6","value":" 6.29k/6.29k [00:00&lt;00:00, 25.8kB/s]"}},"9ef5272104fc43298ef1ca027522b971":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c49e3b8e6d104db8ac3f6218e15c73e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f66a7a685cb4ede836593a7df183093":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e43d23a83c4840dd93dfdd289bb3a048":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75dc1f2eaa1e4505b86f4b319a0da6ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5cf6b200f0764672b5380ee6b0b025b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58bbe52516574330ac4989c269794bb6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d9490c215e042d7bc807b0ebd7ab32c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21db14007bfb4d778563ccdf6f0043a0","IPY_MODEL_b7a87550f22d485098dfd1ad14a62a12","IPY_MODEL_9e2662a65e7642dfae4fe523d918a84e"],"layout":"IPY_MODEL_4475e30e6f004dfcb655bf24ebc95057"}},"21db14007bfb4d778563ccdf6f0043a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e2b1386296f45e48efe793a9c7ae7d4","placeholder":"​","style":"IPY_MODEL_4615712c78a44efc9eb0e897752d7613","value":"Downloading readme: 100%"}},"b7a87550f22d485098dfd1ad14a62a12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c0d118e438644fca0073b384e6e38b4","max":10566,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c66d45e0e0104668890fc2c576d40328","value":10566}},"9e2662a65e7642dfae4fe523d918a84e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a9b7cf127f1447abd79e2dec9d905cb","placeholder":"​","style":"IPY_MODEL_8158df217bf44588846bb3aaffb8e1b6","value":" 10.6k/10.6k [00:00&lt;00:00, 47.6kB/s]"}},"4475e30e6f004dfcb655bf24ebc95057":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e2b1386296f45e48efe793a9c7ae7d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4615712c78a44efc9eb0e897752d7613":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c0d118e438644fca0073b384e6e38b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c66d45e0e0104668890fc2c576d40328":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a9b7cf127f1447abd79e2dec9d905cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8158df217bf44588846bb3aaffb8e1b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78c65744dc99401c8274691aba0da24d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2b0f55d2d3f24045a3e91325ad0453c0","IPY_MODEL_481bd1eeec534d1cb0d0e3952b313d04","IPY_MODEL_7ff14e0197e349c99bd9c6538a4e3d11"],"layout":"IPY_MODEL_b85af78814cf4aa79b17e876a1c26002"}},"2b0f55d2d3f24045a3e91325ad0453c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca8cf606c2a74e1b995ac9ccbaae7f8a","placeholder":"​","style":"IPY_MODEL_300f8da996944ec29868272b50959f92","value":"Downloading data: 100%"}},"481bd1eeec534d1cb0d0e3952b313d04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f32e64005f6490db9ccdedfa9f6ac3c","max":5975590,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1f4ada9eae81495daf7f9019650dad62","value":5975590}},"7ff14e0197e349c99bd9c6538a4e3d11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_243fb5975d664783ac727e7519bda8eb","placeholder":"​","style":"IPY_MODEL_d0dba3c70dfa46bd9552c13e52043410","value":" 5.98M/5.98M [00:00&lt;00:00, 14.0MB/s]"}},"b85af78814cf4aa79b17e876a1c26002":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca8cf606c2a74e1b995ac9ccbaae7f8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"300f8da996944ec29868272b50959f92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f32e64005f6490db9ccdedfa9f6ac3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f4ada9eae81495daf7f9019650dad62":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"243fb5975d664783ac727e7519bda8eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0dba3c70dfa46bd9552c13e52043410":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7b6f52913dd40fe9fbf6c064f72af03":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3937e1f2d6714d64985be9abb684d7b4","IPY_MODEL_0e49533d3cca4ca897e9379f3cafbb90","IPY_MODEL_12a01607f87e4b999e4e239d450be841"],"layout":"IPY_MODEL_ae3016f50d1c410884d15fc5f39f17ef"}},"3937e1f2d6714d64985be9abb684d7b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cef22989322849fda4215bec758ead24","placeholder":"​","style":"IPY_MODEL_75cd2c5ae7754bfaaaa559e388c5003f","value":"Generating train split: 100%"}},"0e49533d3cca4ca897e9379f3cafbb90":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4289399d5804a3aadca3cec90101304","max":6838,"min":0,"orientation":"horizontal","style":"IPY_MODEL_316da883b4444f6f9be384933877e899","value":6838}},"12a01607f87e4b999e4e239d450be841":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d906c24360c043c888a6843ab57936f2","placeholder":"​","style":"IPY_MODEL_7d414696c96c4c6f826f04c3dc97e94e","value":" 6838/6838 [00:01&lt;00:00, 4461.35 examples/s]"}},"ae3016f50d1c410884d15fc5f39f17ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cef22989322849fda4215bec758ead24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75cd2c5ae7754bfaaaa559e388c5003f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a4289399d5804a3aadca3cec90101304":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"316da883b4444f6f9be384933877e899":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d906c24360c043c888a6843ab57936f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d414696c96c4c6f826f04c3dc97e94e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3177845a32fa453fa7d94636c55a9bc8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0eb84b8b3c01410d982726090ffba339","IPY_MODEL_7f20295853dc449fb938c39f93b9fc4d","IPY_MODEL_5ff26d969d90421caf090509b67eaf18"],"layout":"IPY_MODEL_7650f4b5ee9441ac8b21f291e05bccea"}},"0eb84b8b3c01410d982726090ffba339":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b7d746b85dd4379bb549c445a816227","placeholder":"​","style":"IPY_MODEL_ada36259e603424a875d7e72e5c1df47","value":"Generating test split: 100%"}},"7f20295853dc449fb938c39f93b9fc4d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9cb8c122dba42daba9881aa348a5a5a","max":3259,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99d76bcf7fea4b5c84a2239c2f56c726","value":3259}},"5ff26d969d90421caf090509b67eaf18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a979b376285b40188a6f20f0c8ff9efb","placeholder":"​","style":"IPY_MODEL_8a83ecd420dd4eb2b6b16e433e65079f","value":" 3259/3259 [00:00&lt;00:00, 5498.77 examples/s]"}},"7650f4b5ee9441ac8b21f291e05bccea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b7d746b85dd4379bb549c445a816227":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ada36259e603424a875d7e72e5c1df47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9cb8c122dba42daba9881aa348a5a5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99d76bcf7fea4b5c84a2239c2f56c726":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a979b376285b40188a6f20f0c8ff9efb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a83ecd420dd4eb2b6b16e433e65079f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72454818b4e64f87aeaba8e7934c4929":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_221f429041f54a42ba16505174903c7d","IPY_MODEL_e7216360fdef454997b6939b446c8f9b","IPY_MODEL_07a6c8764f58401bbd690da1d8bb9e00"],"layout":"IPY_MODEL_460fd7ca8f0e4039846346b8d8b2cd04"}},"221f429041f54a42ba16505174903c7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9eeb232ed3c34c1cae664fe61cff094f","placeholder":"​","style":"IPY_MODEL_c798567b2034459597025c7640317d76","value":"Generating validation split: 100%"}},"e7216360fdef454997b6939b446c8f9b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c26f601d16ae47bf96f2c8f295e12db9","max":886,"min":0,"orientation":"horizontal","style":"IPY_MODEL_03ba2083ad4e441d9f42b8dbc293c381","value":886}},"07a6c8764f58401bbd690da1d8bb9e00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d4d08b708704d5c8de1e8a15f93c302","placeholder":"​","style":"IPY_MODEL_d57b26c1ce5d415da5f5baded87b87bc","value":" 886/886 [00:00&lt;00:00, 4613.64 examples/s]"}},"460fd7ca8f0e4039846346b8d8b2cd04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9eeb232ed3c34c1cae664fe61cff094f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c798567b2034459597025c7640317d76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c26f601d16ae47bf96f2c8f295e12db9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03ba2083ad4e441d9f42b8dbc293c381":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d4d08b708704d5c8de1e8a15f93c302":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d57b26c1ce5d415da5f5baded87b87bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}