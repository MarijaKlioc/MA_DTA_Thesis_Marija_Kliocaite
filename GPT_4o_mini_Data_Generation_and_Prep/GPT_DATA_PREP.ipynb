{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"cQSb4y2-FUSP"},"outputs":[],"source":["%pip install datasets transformers --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKPJ94b6ApvN"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","import json\n","\n","seed = 42\n","\n","np.random.seed(seed)\n","random.seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"6Vzmlkc0eFrO"},"source":["Getting the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qlj0cokYFeEY","executionInfo":{"status":"ok","timestamp":1723988250166,"user_tz":-120,"elapsed":10869,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"colab":{"base_uri":"https://localhost:8080/","height":322,"referenced_widgets":["a19c6d0410124f39b814cc68b5cae5da","4e81083bf7b54c88b4173b6be6cdb265","b13d7494b47249148368eed06ffa120e","5a06861debb449adb04fefb5db27b832","3ad244d966124a0d9430551cfb3f8f12","4a29433e3b4949f5878f079d15b9d3b4","12189ba1754f4dd9bd1a734915e57356","edb917fc5f87434c9d5376d687711bd8","c571ace2751540ecb3426e3866eb496b","13ef495b4a444d29a142cc65f7fef627","17987d72fce241d0941ab354daf3554c","67e487fbd9c34f9da8f2d3edd89e7339","4c9e4bf1195b4015a14d5948c6ac3dfc","62822a41015047828e337993bb85f1a8","c9f9e34143cc45d4970c5f1909294dda","04fc887d4f9d4b5f97923501094aea85","fac1380a938c4a268a7e3b869ef547ba","0f0b24d9e30d46e785d5b1cb7b9a3044","8de0788b6ed2415f8ac1434233e940e0","a12e0d1562f04eec8aac36f54e6ccf4e","47ab899bd2de4988a0cf6641668c3b0a","5f37e797250243c780032afcd91e60c0","5e0bfc1dfb9a4a97bbe6e15876a40760","c5aee9ea9d73411cb59a036080c80369","79b4a8ce01bf41478fb9ea777a7bb22c","f8a6028bf5744ee98dae498e012ef144","27fcba03c94249cf8d048c0d986f3317","08b7a56a358c4476a56ef4d5c86f569d","db1652dcdc5a4665aa89f698d8fa802e","edbb9956a16748f4aedced996a53b188","4ef2c3f8a5a446a69b12fb1a6c7ef315","b2b32eedae1a40fea7de0bf25ab4483e","c106d7c45989483ba2c0c1ee8bb1a1d4","c8c09926ad4c494f95b77c0d66325391","159c6e4ff5714c0eb6a824afd1bff3a9","4c63cd6eba8842f2a49538b990f80f08","d5e90431f54f46f192a279218db44eff","1426760556f14e45a1cabaff2090408c","dc0b0f5147c7416196372dd13aabecea","89220695ebbc4657ae558fa84ca331c6","600d9102280f4c4e9cd55083675dd17c","908d03e86ca14b40af1f87537114e4cc","43a7ac6f07584cd0b19598e8eb5eb45f","f02ec2d73fb14ada9764bb5a4ef9826d","9c27824e3985419a895c18043a4913ad","7e0df0133b0541358ba439f82a45e02e","1db84918d7484cc08c80e248dece8f70","f66777a85f9d4b5a9a2da6dab10dc9eb","5844e2a0d14b4ef0b4cd8292a0ae74bb","2a21ca2cbd9e484fa2003e9a63954b05","45c02d2b1a9c44b485dada27dac9ad90","4814dfee3e0e46f6b77184f363cb8529","70664cb000434bf8a22a701c2dda091e","4190f9ac862640818cc35d96283c1d6a","ba434ae538e145eaa21509ab93fbf628","434090d0f7c74bc48236b568e36fc1ba","f309c879112c49a4a087a49306916ec4","0d6aa2d984a24460b0f7e998813eedd7","bb2c1e09cad0447298351e8443de0d2b","5473a7e444ea4fbaa47f39d31be7bf9d","1eb9aef3283b4f088d81ed4795cd9e0a","5fdddbb86cdc4cd6a16883f538f70ecc","6d0e7b2d9766410e92c311d6838d4009","e58bd675863b4d3aac1e8d32257c56e4","115c748cb9d94a1ca7c234b8c4babb72","bd5d8c78160e44f09e653d21b6202115"]},"outputId":"b93a0c12-9be2-490f-beee-99a6aef5ae8e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.29k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a19c6d0410124f39b814cc68b5cae5da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67e487fbd9c34f9da8f2d3edd89e7339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/5.98M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e0bfc1dfb9a4a97bbe6e15876a40760"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/6838 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8c09926ad4c494f95b77c0d66325391"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/3259 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c27824e3985419a895c18043a4913ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/886 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"434090d0f7c74bc48236b568e36fc1ba"}},"metadata":{}}],"source":["from datasets import load_dataset\n","sem_eval_2018_task_1 = load_dataset('sem_eval_2018_task_1', 'subtask5.english', trust_remote_code=True)"]},{"cell_type":"markdown","source":["For each of the files with generated GPT data, we parse the JSON file, drop the missing values and, if necessary, convert the label columns to boolean values. The data is combined with half of the gold standard data. Additionally, the distribution of labels can be verified."],"metadata":{"id":"iHXyDjCuRg5E"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_no_role.csv')"],"metadata":{"id":"N-aLMTpuutLP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data.drop(columns=[\"Tweet\"]).sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":429},"id":"MykWkb1Mut9W","executionInfo":{"status":"ok","timestamp":1723810624130,"user_tz":-120,"elapsed":4,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"bf09454d-5529-4189-d301-0e965104b049"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["anger           104\n","anticipation    723\n","disgust         158\n","fear            256\n","joy             600\n","love            399\n","optimism        610\n","pessimism       495\n","sadness         296\n","surprise        387\n","trust           608\n","dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>anger</th>\n","      <td>104</td>\n","    </tr>\n","    <tr>\n","      <th>anticipation</th>\n","      <td>723</td>\n","    </tr>\n","    <tr>\n","      <th>disgust</th>\n","      <td>158</td>\n","    </tr>\n","    <tr>\n","      <th>fear</th>\n","      <td>256</td>\n","    </tr>\n","    <tr>\n","      <th>joy</th>\n","      <td>600</td>\n","    </tr>\n","    <tr>\n","      <th>love</th>\n","      <td>399</td>\n","    </tr>\n","    <tr>\n","      <th>optimism</th>\n","      <td>610</td>\n","    </tr>\n","    <tr>\n","      <th>pessimism</th>\n","      <td>495</td>\n","    </tr>\n","    <tr>\n","      <th>sadness</th>\n","      <td>296</td>\n","    </tr>\n","    <tr>\n","      <th>surprise</th>\n","      <td>387</td>\n","    </tr>\n","    <tr>\n","      <th>trust</th>\n","      <td>608</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Half GSD"],"metadata":{"id":"Op0lOPdeH7j3"}},{"cell_type":"code","source":["# Load original dataset\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])\n","\n","# Select half of the original DataFrame and shuffle\n","df_shuffled = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n","half_index = len(df_shuffled) // 2\n","half_df = df_shuffled.iloc[:half_index].copy()\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([half_df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('half_gsd_half_gpt_data.csv', index=False)"],"metadata":{"id":"Z3tp5PDcKzd1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full GSD data"],"metadata":{"id":"9EJz_G6PHrn_"}},{"cell_type":"code","source":["# Load original dataset\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('full_gsd_multi_gpt_data.csv', index=False)"],"metadata":{"id":"ahyLtLFxHt7z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Half GSD"],"metadata":{"id":"ZsQuPWKxH6-7"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_no_role_news.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data.drop(columns=[\"Tweet\"]).sum()\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([half_df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('half_gsd_half_gpt_data_no_role_news.csv', index=False)"],"metadata":{"id":"DjbKmEABJi5z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full GSD"],"metadata":{"id":"rL8QTU0ZH-cD"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_no_role_news.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","\n","# Concatenate df and augmented_df to create mixed_df\n","mixed_df = pd.concat([df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('full_gsd_multi_gpt_data_no_role_news.csv', index=False)"],"metadata":{"id":"80TG0725IA_3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Half GSD"],"metadata":{"id":"oT3tM8acIQNt"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_role_1.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","generated_data.drop(columns=[\"Tweet\"]).sum()\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([half_df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('half_gsd_half_gpt_data_role_1.csv', index=False)"],"metadata":{"id":"LiMiOgapJnq7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full GSD"],"metadata":{"id":"Ba4jIVUuIVZ3"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_role_1.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('full_gsd_multi_gpt_data_role_1.csv', index=False)"],"metadata":{"id":"C0XybBDMQkWJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Half GSD"],"metadata":{"id":"JYqJEU9wIrrM"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_role_2.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([half_df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('half_gsd_half_gpt_data_role_2.csv', index=False)"],"metadata":{"id":"eAMMyZDdJvqu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full GSD"],"metadata":{"id":"INDSqlSkItFI"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_role_2.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('full_gsd_multi_gpt_data_role_2.csv', index=False)"],"metadata":{"id":"j9G3Ng6MIvBc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Half GSD"],"metadata":{"id":"E2yBkorCI0Gg"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_role_3.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([half_df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('half_gsd_half_gpt_data_role_3.csv', index=False)"],"metadata":{"id":"hNXDK8DGM4S8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full GSD"],"metadata":{"id":"YIo1ftDyI1T8"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_role_3.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('full_gsd_multi_gpt_data_role_3.csv', index=False)"],"metadata":{"id":"WtDOmwb8I2Yz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Half GSD"],"metadata":{"id":"BxFZl663I6-0"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_role_4.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([half_df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('half_gsd_half_gpt_data_role_4.csv', index=False)"],"metadata":{"id":"S-LBTl-QMsT5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full GSD"],"metadata":{"id":"LA4Wo6ZNI99r"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_role_4.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('full_gsd_multi_gpt_data_role_4.csv', index=False)"],"metadata":{"id":"f2qNKRIqI8aA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Half GSD"],"metadata":{"id":"t62RktrDJCzY"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_role_5.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([half_df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('half_gsd_half_gpt_data_role_5.csv', index=False)"],"metadata":{"id":"oFPfm-KOMuZI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full GSD"],"metadata":{"id":"pddCXg0oJEAF"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_role_5.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('full_gsd_multi_gpt_data_role_5.csv', index=False)"],"metadata":{"id":"1eq9v62TJFA0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Single-label Data"],"metadata":{"id":"NSRKfZpjdo7v"}},{"cell_type":"markdown","source":["Half GSD"],"metadata":{"id":"VylBEx2dJIA8"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_single_label_no_role.csv')\n","\n","# Load original dataset\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])\n","\n","# Select half of the original DataFrame and shuffle\n","df_shuffled = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n","half_index = len(df_shuffled) // 2\n","half_df = df_shuffled.iloc[:half_index].copy()\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","generated_data.drop(columns=[\"Tweet\"]).sum()\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([half_df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('gsd_gpt_single_label_no_role.csv', index=False)"],"metadata":{"id":"SXAS5OMMdy-q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full GSD"],"metadata":{"id":"xRD0zl4sJKLl"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_single_label_no_role.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('full_gsd_single_gpt_data_no_role.csv', index=False)"],"metadata":{"id":"8gIA0Ye-JJWE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Half GSD"],"metadata":{"id":"bUPd9NGXJSF7"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_single_label_role_1.csv')\n","\n","# Load original dataset\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])\n","\n","# Select half of the original DataFrame and shuffle\n","df_shuffled = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n","half_index = len(df_shuffled) // 2\n","half_df = df_shuffled.iloc[:half_index].copy()\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","generated_data.drop(columns=[\"Tweet\"]).sum()\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([half_df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('gsd_gpt_single_label_role_1.csv', index=False)"],"metadata":{"id":"ttwcGd9fdrm3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full GSD"],"metadata":{"id":"j4bfrN2tJT80"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_single_label_role_1.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('full_gsd_single_gpt_data_role_1.csv', index=False)"],"metadata":{"id":"Lz75czImJTeD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Half GSD"],"metadata":{"id":"RaWW9BnxJZkD"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_single_label_role_1_news.csv')\n","\n","# Load original dataset\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])\n","\n","# Select half of the original DataFrame and shuffle\n","df_shuffled = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n","half_index = len(df_shuffled) // 2\n","half_df = df_shuffled.iloc[:half_index].copy()\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([half_df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('gsd_gpt_single_label_role_1_news.csv', index=False)"],"metadata":{"id":"_w4DdfSueSY0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full GSD"],"metadata":{"id":"L-z2GHF_JawL"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_single_label_role_1_news.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('full_gsd_single_gpt_data_role_1_news.csv', index=False)"],"metadata":{"id":"qzYAzsWAJcEH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Increased size data"],"metadata":{"id":"IsbkwVO8t_-n"}},{"cell_type":"code","source":["# Load original dataset\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])\n","\n","# Select half of the original DataFrame and shuffle\n","df_shuffled = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n","half_index = len(df_shuffled) // 2\n","half_df = df_shuffled.iloc[:half_index].copy()"],"metadata":{"id":"j5oRojbeuddR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Half GSD"],"metadata":{"id":"gUcRzermJhtr"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_increased_multi.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","generated_data.drop(columns=[\"Tweet\"]).sum()\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([half_df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('gsd_gpt_increased_multi.csv', index=False)"],"metadata":{"id":"ePmNvQB6uDIP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full GSD"],"metadata":{"id":"mNxX60_1JjLj"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_increased_multi.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('full_gsd_increased_gpt_data_multi.csv', index=False)"],"metadata":{"id":"_6f9dLvEJkHT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Half GSD"],"metadata":{"id":"SEkUZCC6Jt0x"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_increased_single.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","generated_data.drop(columns=[\"Tweet\"]).sum()\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([half_df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('gsd_gpt_increased_single.csv', index=False)"],"metadata":{"id":"Z2Z06CoIukjM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full GSD"],"metadata":{"id":"m1A8JFNAJvgS"}},{"cell_type":"code","source":["generated_df = pd.read_csv('generated_tweets_increased_single.csv')\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Turning the data into boolean values\n","label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love', 'anger', 'disgust', 'pessimism', 'sadness', 'fear', 'surprise']\n","generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","# Concatenate half_df and half_augmented_df to create mixed_df\n","mixed_df = pd.concat([df, generated_data], ignore_index=True)\n","\n","# Save the mixed dataset to a CSV file\n","mixed_df.to_csv('full_gsd_increased_gpt_data_single.csv', index=False)"],"metadata":{"id":"yiPHq5t9JvMt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Full GSD + GPT Data\n","Generating data for multi-label data, single-label data and increased size data."],"metadata":{"id":"NP9tm8yFGE7J"}},{"cell_type":"code","source":["# Load original dataset\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])"],"metadata":{"id":"EJyThVasGRYO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DATA Typicality\n","\n","Inter-dataset typicality.\n","\n","We are looking at how similar the tweets from the generated data are to the original gold standard data tweets."],"metadata":{"id":"IR8TjWepe7EH"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Sample data\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])  # Original df\n","generated_data = pd.read_csv('generated_tweets_no_role.csv')  # Generated data\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_data.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","\n","# Vectorize the tweets\n","vectorizer = TfidfVectorizer()\n","original_vectors = vectorizer.fit_transform(df['Tweet'])\n","generated_vectors = vectorizer.transform(generated_data['Tweet'])\n","\n","# Calculate cosine similarity\n","similarity_matrix = cosine_similarity(generated_vectors, original_vectors)\n","\n","def calculate_typicality(similarities, labels, target_label):\n","    target_indices = np.where(labels[:, target_label] == True)[0]\n","    non_target_indices = np.where(labels[:, target_label] == False)[0]\n","\n","    sim_target = similarities[:, target_indices]\n","    sim_non_target = similarities[:, non_target_indices]\n","\n","    avg_sim_target = np.mean(sim_target, axis=1)\n","    avg_sim_non_target = np.mean(sim_non_target, axis=1)\n","\n","    typicality = avg_sim_target / avg_sim_non_target\n","    return typicality\n","\n","# Convert labels to numpy array for easier indexing\n","labels = df.drop(columns=['Tweet', 'ID']).to_numpy()\n","\n","# Calculate typicality for each emotion\n","typicality_scores = {}\n","for i, emotion in enumerate(df.columns[2:]):  # Skipping 'Tweet' and 'ID'\n","    typicality_scores[emotion] = calculate_typicality(similarity_matrix, labels, i)\n","\n","# Calculate average typicality for the entire dataset for each emotion\n","average_typicality = {emotion: np.mean(scores) for emotion, scores in typicality_scores.items()}\n","std_deviation_typicality = {emotion: np.std(scores) for emotion, scores in typicality_scores.items()}\n","\n","# Print results with formatting\n","for emotion in average_typicality:\n","    avg = average_typicality[emotion]\n","    std_dev = std_deviation_typicality[emotion]\n","    print(f\"Typicality for {emotion}: {avg:.2f} with standard deviation of {std_dev:.2f}\")\n","\n","# Combine all typicality scores across all emotions\n","all_typicality_scores = np.concatenate(list(typicality_scores.values()))\n","\n","# Print overall results\n","print(f\"\\nOverall Typicality: {np.mean(all_typicality_scores):.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NXUjfegae9HY","executionInfo":{"status":"ok","timestamp":1723046753460,"user_tz":-120,"elapsed":2931,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"01b33751-d979-4c2b-9006-cb9b94c149cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality for anger: 0.96 with standard deviation of 0.10\n","Typicality for anticipation: 0.96 with standard deviation of 0.10\n","Typicality for disgust: 0.96 with standard deviation of 0.11\n","Typicality for fear: 0.98 with standard deviation of 0.12\n","Typicality for joy: 1.03 with standard deviation of 0.14\n","Typicality for love: 1.08 with standard deviation of 0.23\n","Typicality for optimism: 1.13 with standard deviation of 0.16\n","Typicality for pessimism: 1.13 with standard deviation of 0.13\n","Typicality for sadness: 1.12 with standard deviation of 0.15\n","Typicality for surprise: 0.92 with standard deviation of 0.13\n","Typicality for trust: 1.08 with standard deviation of 0.13\n","\n","Overall Typicality: 1.03\n"]}]},{"cell_type":"code","source":["# Sample data\n","generated_data = pd.read_csv('generated_tweets_no_role_news.csv')  # Generated data\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_data.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","\n","# Vectorize the tweets\n","vectorizer = TfidfVectorizer()\n","original_vectors = vectorizer.fit_transform(df['Tweet'])\n","generated_vectors = vectorizer.transform(generated_data['Tweet'])\n","\n","# Calculate cosine similarity\n","similarity_matrix = cosine_similarity(generated_vectors, original_vectors)\n","\n","# Convert labels to numpy array for easier indexing\n","labels = df.drop(columns=['Tweet', 'ID']).to_numpy()\n","\n","# Calculate typicality for each emotion\n","typicality_scores = {}\n","for i, emotion in enumerate(df.columns[2:]):  # Skipping 'Tweet' and 'ID'\n","    typicality_scores[emotion] = calculate_typicality(similarity_matrix, labels, i)\n","\n","# Calculate average typicality for the entire dataset for each emotion\n","average_typicality = {emotion: np.mean(scores) for emotion, scores in typicality_scores.items()}\n","std_deviation_typicality = {emotion: np.std(scores) for emotion, scores in typicality_scores.items()}\n","\n","# Print results with formatting\n","for emotion in average_typicality:\n","    avg = average_typicality[emotion]\n","    std_dev = std_deviation_typicality[emotion]\n","    print(f\"Typicality for {emotion}: {avg:.2f} with standard deviation of {std_dev:.2f}\")\n","\n","# Combine all typicality scores across all emotions\n","all_typicality_scores = np.concatenate(list(typicality_scores.values()))\n","\n","# Print overall results\n","print(f\"\\nOverall Typicality: {np.mean(all_typicality_scores):.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53yYmWP4h3rO","executionInfo":{"status":"ok","timestamp":1723046766128,"user_tz":-120,"elapsed":1562,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"ee21ec3f-11a8-4392-be2d-b9be9b99d53c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality for anger: 1.00 with standard deviation of 0.08\n","Typicality for anticipation: 1.01 with standard deviation of 0.09\n","Typicality for disgust: 1.01 with standard deviation of 0.08\n","Typicality for fear: 1.03 with standard deviation of 0.08\n","Typicality for joy: 0.97 with standard deviation of 0.09\n","Typicality for love: 0.96 with standard deviation of 0.19\n","Typicality for optimism: 1.09 with standard deviation of 0.12\n","Typicality for pessimism: 1.10 with standard deviation of 0.10\n","Typicality for sadness: 1.07 with standard deviation of 0.11\n","Typicality for surprise: 0.98 with standard deviation of 0.20\n","Typicality for trust: 1.06 with standard deviation of 0.12\n","\n","Overall Typicality: 1.02\n"]}]},{"cell_type":"code","source":["# Sample data\n","generated_data = pd.read_csv('generated_tweets_role_1.csv')  # Generated data\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_data.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Vectorize the tweets\n","vectorizer = TfidfVectorizer()\n","original_vectors = vectorizer.fit_transform(df['Tweet'])\n","generated_vectors = vectorizer.transform(generated_data['Tweet'])\n","\n","# Calculate cosine similarity\n","similarity_matrix = cosine_similarity(generated_vectors, original_vectors)\n","\n","# Convert labels to numpy array for easier indexing\n","labels = df.drop(columns=['Tweet', 'ID']).to_numpy()\n","\n","# Calculate typicality for each emotion\n","typicality_scores = {}\n","for i, emotion in enumerate(df.columns[2:]):  # Skipping 'Tweet' and 'ID'\n","    typicality_scores[emotion] = calculate_typicality(similarity_matrix, labels, i)\n","\n","# Calculate average typicality for the entire dataset for each emotion\n","average_typicality = {emotion: np.mean(scores) for emotion, scores in typicality_scores.items()}\n","std_deviation_typicality = {emotion: np.std(scores) for emotion, scores in typicality_scores.items()}\n","\n","# Print results with formatting\n","for emotion in average_typicality:\n","    avg = average_typicality[emotion]\n","    std_dev = std_deviation_typicality[emotion]\n","    print(f\"Typicality for {emotion}: {avg:.2f} with standard deviation of {std_dev:.2f}\")\n","\n","# Combine all typicality scores across all emotions\n","all_typicality_scores = np.concatenate(list(typicality_scores.values()))\n","\n","# Print overall results\n","print(f\"\\nOverall Typicality: {np.mean(all_typicality_scores):.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I05SWUi-jbJA","executionInfo":{"status":"ok","timestamp":1723046773802,"user_tz":-120,"elapsed":2622,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"521eba80-1b7f-41ec-f85d-6cfea8b0c725"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality for anger: 0.98 with standard deviation of 0.12\n","Typicality for anticipation: 0.98 with standard deviation of 0.11\n","Typicality for disgust: 0.98 with standard deviation of 0.11\n","Typicality for fear: 1.01 with standard deviation of 0.13\n","Typicality for joy: 0.99 with standard deviation of 0.12\n","Typicality for love: 1.01 with standard deviation of 0.21\n","Typicality for optimism: 1.12 with standard deviation of 0.15\n","Typicality for pessimism: 1.14 with standard deviation of 0.13\n","Typicality for sadness: 1.11 with standard deviation of 0.14\n","Typicality for surprise: 0.91 with standard deviation of 0.12\n","Typicality for trust: 1.07 with standard deviation of 0.15\n","\n","Overall Typicality: 1.03\n"]}]},{"cell_type":"code","source":["# Sample data\n","generated_data = pd.read_csv('generated_tweets_role_2.csv')  # Generated data\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_data.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Vectorize the tweets\n","vectorizer = TfidfVectorizer()\n","original_vectors = vectorizer.fit_transform(df['Tweet'])\n","generated_vectors = vectorizer.transform(generated_data['Tweet'])\n","\n","# Calculate cosine similarity\n","similarity_matrix = cosine_similarity(generated_vectors, original_vectors)\n","\n","# Convert labels to numpy array for easier indexing\n","labels = df.drop(columns=['Tweet', 'ID']).to_numpy()\n","\n","# Calculate typicality for each emotion\n","typicality_scores = {}\n","for i, emotion in enumerate(df.columns[2:]):  # Skipping 'Tweet' and 'ID'\n","    typicality_scores[emotion] = calculate_typicality(similarity_matrix, labels, i)\n","\n","# Calculate average typicality for the entire dataset for each emotion\n","average_typicality = {emotion: np.mean(scores) for emotion, scores in typicality_scores.items()}\n","std_deviation_typicality = {emotion: np.std(scores) for emotion, scores in typicality_scores.items()}\n","\n","# Print results with formatting\n","for emotion in average_typicality:\n","    avg = average_typicality[emotion]\n","    std_dev = std_deviation_typicality[emotion]\n","    print(f\"Typicality for {emotion}: {avg:.2f} with standard deviation of {std_dev:.2f}\")\n","\n","# Combine all typicality scores across all emotions\n","all_typicality_scores = np.concatenate(list(typicality_scores.values()))\n","\n","# Print overall results\n","print(f\"\\nOverall Typicality: {np.mean(all_typicality_scores):.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kKq9yVgSjtaV","executionInfo":{"status":"ok","timestamp":1723046779545,"user_tz":-120,"elapsed":1865,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"2209d5b1-93f7-42c5-f066-2cda30efd014"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality for anger: 0.95 with standard deviation of 0.12\n","Typicality for anticipation: 0.98 with standard deviation of 0.12\n","Typicality for disgust: 0.95 with standard deviation of 0.12\n","Typicality for fear: 0.98 with standard deviation of 0.12\n","Typicality for joy: 1.03 with standard deviation of 0.16\n","Typicality for love: 1.08 with standard deviation of 0.27\n","Typicality for optimism: 1.16 with standard deviation of 0.18\n","Typicality for pessimism: 1.13 with standard deviation of 0.13\n","Typicality for sadness: 1.10 with standard deviation of 0.14\n","Typicality for surprise: 0.91 with standard deviation of 0.14\n","Typicality for trust: 1.09 with standard deviation of 0.15\n","\n","Overall Typicality: 1.03\n"]}]},{"cell_type":"code","source":["# Sample data\n","generated_data = pd.read_csv('generated_tweets_role_3.csv')  # Generated data\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_data.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Vectorize the tweets\n","vectorizer = TfidfVectorizer()\n","original_vectors = vectorizer.fit_transform(df['Tweet'])\n","generated_vectors = vectorizer.transform(generated_data['Tweet'])\n","\n","# Calculate cosine similarity\n","similarity_matrix = cosine_similarity(generated_vectors, original_vectors)\n","\n","# Convert labels to numpy array for easier indexing\n","labels = df.drop(columns=['Tweet', 'ID']).to_numpy()\n","\n","# Calculate typicality for each emotion\n","typicality_scores = {}\n","for i, emotion in enumerate(df.columns[2:]):  # Skipping 'Tweet' and 'ID'\n","    typicality_scores[emotion] = calculate_typicality(similarity_matrix, labels, i)\n","\n","# Calculate average typicality for the entire dataset for each emotion\n","average_typicality = {emotion: np.mean(scores) for emotion, scores in typicality_scores.items()}\n","std_deviation_typicality = {emotion: np.std(scores) for emotion, scores in typicality_scores.items()}\n","\n","# Print results with formatting\n","for emotion in average_typicality:\n","    avg = average_typicality[emotion]\n","    std_dev = std_deviation_typicality[emotion]\n","    print(f\"Typicality for {emotion}: {avg:.2f} with standard deviation of {std_dev:.2f}\")\n","\n","# Combine all typicality scores across all emotions\n","all_typicality_scores = np.concatenate(list(typicality_scores.values()))\n","\n","# Print overall results\n","print(f\"\\nOverall Typicality: {np.mean(all_typicality_scores):.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6KL0-2TjxO7","executionInfo":{"status":"ok","timestamp":1723046785941,"user_tz":-120,"elapsed":2003,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"02780a35-4d46-41d7-8d4b-67f150bb66a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality for anger: 0.95 with standard deviation of 0.11\n","Typicality for anticipation: 0.98 with standard deviation of 0.11\n","Typicality for disgust: 0.95 with standard deviation of 0.12\n","Typicality for fear: 0.99 with standard deviation of 0.13\n","Typicality for joy: 1.03 with standard deviation of 0.15\n","Typicality for love: 1.08 with standard deviation of 0.25\n","Typicality for optimism: 1.16 with standard deviation of 0.17\n","Typicality for pessimism: 1.14 with standard deviation of 0.15\n","Typicality for sadness: 1.11 with standard deviation of 0.17\n","Typicality for surprise: 0.91 with standard deviation of 0.14\n","Typicality for trust: 1.09 with standard deviation of 0.15\n","\n","Overall Typicality: 1.03\n"]}]},{"cell_type":"code","source":["# Sample data\n","generated_data = pd.read_csv('generated_tweets_role_4.csv')  # Generated data\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_data.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Vectorize the tweets\n","vectorizer = TfidfVectorizer()\n","original_vectors = vectorizer.fit_transform(df['Tweet'])\n","generated_vectors = vectorizer.transform(generated_data['Tweet'])\n","\n","# Calculate cosine similarity\n","similarity_matrix = cosine_similarity(generated_vectors, original_vectors)\n","\n","# Convert labels to numpy array for easier indexing\n","labels = df.drop(columns=['Tweet', 'ID']).to_numpy()\n","\n","# Calculate typicality for each emotion\n","typicality_scores = {}\n","for i, emotion in enumerate(df.columns[2:]):  # Skipping 'Tweet' and 'ID'\n","    typicality_scores[emotion] = calculate_typicality(similarity_matrix, labels, i)\n","\n","# Calculate average typicality for the entire dataset for each emotion\n","average_typicality = {emotion: np.mean(scores) for emotion, scores in typicality_scores.items()}\n","std_deviation_typicality = {emotion: np.std(scores) for emotion, scores in typicality_scores.items()}\n","\n","# Print results with formatting\n","for emotion in average_typicality:\n","    avg = average_typicality[emotion]\n","    std_dev = std_deviation_typicality[emotion]\n","    print(f\"Typicality for {emotion}: {avg:.2f} with standard deviation of {std_dev:.2f}\")\n","\n","# Combine all typicality scores across all emotions\n","all_typicality_scores = np.concatenate(list(typicality_scores.values()))\n","\n","# Print overall results\n","print(f\"\\nOverall Typicality: {np.mean(all_typicality_scores):.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zP4nAe6Rj0Ng","executionInfo":{"status":"ok","timestamp":1723046794100,"user_tz":-120,"elapsed":3046,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"b14f895e-a682-47f1-b89d-cf0af4e06c1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality for anger: 0.98 with standard deviation of 0.11\n","Typicality for anticipation: 0.97 with standard deviation of 0.11\n","Typicality for disgust: 0.99 with standard deviation of 0.11\n","Typicality for fear: 1.01 with standard deviation of 0.12\n","Typicality for joy: 0.96 with standard deviation of 0.10\n","Typicality for love: 0.97 with standard deviation of 0.18\n","Typicality for optimism: 1.10 with standard deviation of 0.13\n","Typicality for pessimism: 1.16 with standard deviation of 0.13\n","Typicality for sadness: 1.14 with standard deviation of 0.14\n","Typicality for surprise: 0.91 with standard deviation of 0.16\n","Typicality for trust: 1.02 with standard deviation of 0.13\n","\n","Overall Typicality: 1.02\n"]}]},{"cell_type":"code","source":["# Sample data\n","generated_data = pd.read_csv('generated_tweets_role_5.csv')  # Generated data\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_data.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","generated_data = generated_data.dropna()\n","\n","# Vectorize the tweets\n","vectorizer = TfidfVectorizer()\n","original_vectors = vectorizer.fit_transform(df['Tweet'])\n","generated_vectors = vectorizer.transform(generated_data['Tweet'])\n","\n","# Calculate cosine similarity\n","similarity_matrix = cosine_similarity(generated_vectors, original_vectors)\n","\n","# Convert labels to numpy array for easier indexing\n","labels = df.drop(columns=['Tweet', 'ID']).to_numpy()\n","\n","# Calculate typicality for each emotion\n","typicality_scores = {}\n","for i, emotion in enumerate(df.columns[2:]):  # Skipping 'Tweet' and 'ID'\n","    typicality_scores[emotion] = calculate_typicality(similarity_matrix, labels, i)\n","\n","# Calculate average typicality for the entire dataset for each emotion\n","average_typicality = {emotion: np.mean(scores) for emotion, scores in typicality_scores.items()}\n","std_deviation_typicality = {emotion: np.std(scores) for emotion, scores in typicality_scores.items()}\n","\n","# Print results with formatting\n","for emotion in average_typicality:\n","    avg = average_typicality[emotion]\n","    std_dev = std_deviation_typicality[emotion]\n","    print(f\"Typicality for {emotion}: {avg:.2f} with standard deviation of {std_dev:.2f}\")\n","\n","# Combine all typicality scores across all emotions\n","all_typicality_scores = np.concatenate(list(typicality_scores.values()))\n","\n","# Print overall results\n","print(f\"\\nOverall Typicality: {np.mean(all_typicality_scores):.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRzfrPfHj3T5","executionInfo":{"status":"ok","timestamp":1723046800345,"user_tz":-120,"elapsed":2252,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"43ca7712-7a38-4a36-87d5-f304d8a6da04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality for anger: 1.00 with standard deviation of 0.09\n","Typicality for anticipation: 1.01 with standard deviation of 0.09\n","Typicality for disgust: 1.01 with standard deviation of 0.09\n","Typicality for fear: 1.03 with standard deviation of 0.09\n","Typicality for joy: 0.96 with standard deviation of 0.09\n","Typicality for love: 0.95 with standard deviation of 0.18\n","Typicality for optimism: 1.08 with standard deviation of 0.13\n","Typicality for pessimism: 1.11 with standard deviation of 0.11\n","Typicality for sadness: 1.07 with standard deviation of 0.12\n","Typicality for surprise: 0.98 with standard deviation of 0.21\n","Typicality for trust: 1.05 with standard deviation of 0.12\n","\n","Overall Typicality: 1.02\n"]}]},{"cell_type":"markdown","source":["Single-label"],"metadata":{"id":"X7apkOB1f9Jt"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Sample data\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])  # Original df\n","generated_data = pd.read_csv('generated_tweets_single_label_no_role.csv')  # Generated data\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_data.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","\n","# Vectorize the tweets\n","vectorizer = TfidfVectorizer()\n","original_vectors = vectorizer.fit_transform(df['Tweet'])\n","generated_vectors = vectorizer.transform(generated_data['Tweet'])\n","\n","# Calculate cosine similarity\n","similarity_matrix = cosine_similarity(generated_vectors, original_vectors)\n","\n","def calculate_typicality(similarities, labels, target_label):\n","    target_indices = np.where(labels[:, target_label] == True)[0]\n","    non_target_indices = np.where(labels[:, target_label] == False)[0]\n","\n","    sim_target = similarities[:, target_indices]\n","    sim_non_target = similarities[:, non_target_indices]\n","\n","    avg_sim_target = np.mean(sim_target, axis=1)\n","    avg_sim_non_target = np.mean(sim_non_target, axis=1)\n","\n","    typicality = avg_sim_target / avg_sim_non_target\n","    return typicality\n","\n","# Convert labels to numpy array for easier indexing\n","labels = df.drop(columns=['Tweet', 'ID']).to_numpy()\n","\n","# Calculate typicality for each emotion\n","typicality_scores = {}\n","for i, emotion in enumerate(df.columns[2:]):  # Skipping 'Tweet' and 'ID'\n","    typicality_scores[emotion] = calculate_typicality(similarity_matrix, labels, i)\n","\n","# Calculate average typicality for the entire dataset for each emotion\n","average_typicality = {emotion: np.mean(scores) for emotion, scores in typicality_scores.items()}\n","std_deviation_typicality = {emotion: np.std(scores) for emotion, scores in typicality_scores.items()}\n","\n","# Print results with formatting\n","for emotion in average_typicality:\n","    avg = average_typicality[emotion]\n","    std_dev = std_deviation_typicality[emotion]\n","    print(f\"Typicality for {emotion}: {avg:.2f} with standard deviation of {std_dev:.2f}\")\n","\n","# Combine all typicality scores across all emotions\n","all_typicality_scores = np.concatenate(list(typicality_scores.values()))\n","\n","# Print overall results\n","print(f\"\\nOverall Typicality: {np.mean(all_typicality_scores):.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gsTcQOt3ffie","executionInfo":{"status":"ok","timestamp":1723129024188,"user_tz":-120,"elapsed":2868,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"fcbd7954-32d6-4ecd-ecb2-032772e13f4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality for anger: 0.95 with standard deviation of 0.11\n","Typicality for anticipation: 0.97 with standard deviation of 0.11\n","Typicality for disgust: 0.96 with standard deviation of 0.10\n","Typicality for fear: 0.98 with standard deviation of 0.11\n","Typicality for joy: 1.03 with standard deviation of 0.14\n","Typicality for love: 1.07 with standard deviation of 0.23\n","Typicality for optimism: 1.13 with standard deviation of 0.17\n","Typicality for pessimism: 1.14 with standard deviation of 0.17\n","Typicality for sadness: 1.13 with standard deviation of 0.19\n","Typicality for surprise: 0.91 with standard deviation of 0.12\n","Typicality for trust: 1.08 with standard deviation of 0.13\n","\n","Overall Typicality: 1.03\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Sample data\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])  # Original df\n","generated_data = pd.read_csv('generated_tweets_single_label_role_1.csv')  # Generated data\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_data.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","\n","# Vectorize the tweets\n","vectorizer = TfidfVectorizer()\n","original_vectors = vectorizer.fit_transform(df['Tweet'])\n","generated_vectors = vectorizer.transform(generated_data['Tweet'])\n","\n","# Calculate cosine similarity\n","similarity_matrix = cosine_similarity(generated_vectors, original_vectors)\n","\n","def calculate_typicality(similarities, labels, target_label):\n","    target_indices = np.where(labels[:, target_label] == True)[0]\n","    non_target_indices = np.where(labels[:, target_label] == False)[0]\n","\n","    sim_target = similarities[:, target_indices]\n","    sim_non_target = similarities[:, non_target_indices]\n","\n","    avg_sim_target = np.mean(sim_target, axis=1)\n","    avg_sim_non_target = np.mean(sim_non_target, axis=1)\n","\n","    typicality = avg_sim_target / avg_sim_non_target\n","    return typicality\n","\n","# Convert labels to numpy array for easier indexing\n","labels = df.drop(columns=['Tweet', 'ID']).to_numpy()\n","\n","# Calculate typicality for each emotion\n","typicality_scores = {}\n","for i, emotion in enumerate(df.columns[2:]):  # Skipping 'Tweet' and 'ID'\n","    typicality_scores[emotion] = calculate_typicality(similarity_matrix, labels, i)\n","\n","# Calculate average typicality for the entire dataset for each emotion\n","average_typicality = {emotion: np.mean(scores) for emotion, scores in typicality_scores.items()}\n","std_deviation_typicality = {emotion: np.std(scores) for emotion, scores in typicality_scores.items()}\n","\n","# Print results with formatting\n","for emotion in average_typicality:\n","    avg = average_typicality[emotion]\n","    std_dev = std_deviation_typicality[emotion]\n","    print(f\"Typicality for {emotion}: {avg:.2f} with standard deviation of {std_dev:.2f}\")\n","\n","# Combine all typicality scores across all emotions\n","all_typicality_scores = np.concatenate(list(typicality_scores.values()))\n","\n","# Print overall results\n","print(f\"\\nOverall Typicality: {np.mean(all_typicality_scores):.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVOYXOXAfnqR","executionInfo":{"status":"ok","timestamp":1723129059927,"user_tz":-120,"elapsed":6037,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"c567ebc0-f8d3-4bc7-f98c-107c147eecf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality for anger: 0.99 with standard deviation of 0.12\n","Typicality for anticipation: 0.98 with standard deviation of 0.12\n","Typicality for disgust: 0.99 with standard deviation of 0.12\n","Typicality for fear: 0.99 with standard deviation of 0.12\n","Typicality for joy: 0.99 with standard deviation of 0.12\n","Typicality for love: 1.01 with standard deviation of 0.21\n","Typicality for optimism: 1.11 with standard deviation of 0.15\n","Typicality for pessimism: 1.12 with standard deviation of 0.12\n","Typicality for sadness: 1.10 with standard deviation of 0.14\n","Typicality for surprise: 0.92 with standard deviation of 0.12\n","Typicality for trust: 1.07 with standard deviation of 0.14\n","\n","Overall Typicality: 1.03\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Sample data\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])  # Original df\n","generated_data = pd.read_csv('generated_tweets_single_label_role_1_news.csv')  # Generated data\n","\n","# Convert the JSON strings in the single column to dictionaries\n","parsed_data = [json.loads(entry) for entry in generated_data.iloc[:, 0]]\n","\n","# Create a new DataFrame from the parsed data\n","generated_data = pd.DataFrame(parsed_data)\n","\n","# Vectorize the tweets\n","vectorizer = TfidfVectorizer()\n","original_vectors = vectorizer.fit_transform(df['Tweet'])\n","generated_vectors = vectorizer.transform(generated_data['Tweet'])\n","\n","# Calculate cosine similarity\n","similarity_matrix = cosine_similarity(generated_vectors, original_vectors)\n","\n","def calculate_typicality(similarities, labels, target_label):\n","    target_indices = np.where(labels[:, target_label] == True)[0]\n","    non_target_indices = np.where(labels[:, target_label] == False)[0]\n","\n","    sim_target = similarities[:, target_indices]\n","    sim_non_target = similarities[:, non_target_indices]\n","\n","    avg_sim_target = np.mean(sim_target, axis=1)\n","    avg_sim_non_target = np.mean(sim_non_target, axis=1)\n","\n","    typicality = avg_sim_target / avg_sim_non_target\n","    return typicality\n","\n","# Convert labels to numpy array for easier indexing\n","labels = df.drop(columns=['Tweet', 'ID']).to_numpy()\n","\n","# Calculate typicality for each emotion\n","typicality_scores = {}\n","for i, emotion in enumerate(df.columns[2:]):  # Skipping 'Tweet' and 'ID'\n","    typicality_scores[emotion] = calculate_typicality(similarity_matrix, labels, i)\n","\n","# Calculate average typicality for the entire dataset for each emotion\n","average_typicality = {emotion: np.mean(scores) for emotion, scores in typicality_scores.items()}\n","std_deviation_typicality = {emotion: np.std(scores) for emotion, scores in typicality_scores.items()}\n","\n","# Print results with formatting\n","for emotion in average_typicality:\n","    avg = average_typicality[emotion]\n","    std_dev = std_deviation_typicality[emotion]\n","    print(f\"Typicality for {emotion}: {avg:.2f} with standard deviation of {std_dev:.2f}\")\n","\n","# Combine all typicality scores across all emotions\n","all_typicality_scores = np.concatenate(list(typicality_scores.values()))\n","\n","# Print overall results\n","print(f\"\\nOverall Typicality: {np.mean(all_typicality_scores):.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUWKwmdPfsFZ","executionInfo":{"status":"ok","timestamp":1723129090151,"user_tz":-120,"elapsed":3206,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"bcc18093-7b0b-4b68-dfcf-2ac27c0f92a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality for anger: 1.00 with standard deviation of 0.09\n","Typicality for anticipation: 1.01 with standard deviation of 0.10\n","Typicality for disgust: 1.01 with standard deviation of 0.10\n","Typicality for fear: 1.02 with standard deviation of 0.09\n","Typicality for joy: 0.97 with standard deviation of 0.10\n","Typicality for love: 0.95 with standard deviation of 0.20\n","Typicality for optimism: 1.08 with standard deviation of 0.13\n","Typicality for pessimism: 1.10 with standard deviation of 0.13\n","Typicality for sadness: 1.07 with standard deviation of 0.16\n","Typicality for surprise: 0.98 with standard deviation of 0.20\n","Typicality for trust: 1.05 with standard deviation of 0.14\n","\n","Overall Typicality: 1.02\n"]}]},{"cell_type":"markdown","source":["Additionally, we compare the typocality of labels in the first generated dataset, as well as comparing generated datasets to each other."],"metadata":{"id":"cQChwLC3vmhr"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import pandas as pd\n","import json\n","\n","# Specify the dataset to compare\n","dataset_name = 'role_1'\n","filepath = 'generated_tweets_role_1.csv'\n","\n","# Load the dataset\n","generated_data = pd.read_csv(filepath)\n","\n","# Convert JSON strings to dictionaries if needed\n","if isinstance(generated_data.iloc[0, 0], str) and generated_data.iloc[0, 0].startswith('{'):\n","    parsed_data = [json.loads(entry) for entry in generated_data.iloc[:, 0]]\n","    generated_data = pd.DataFrame(parsed_data)\n","    generated_data = generated_data.dropna()\n","\n","# Extract tweets and emotion labels\n","tweets = generated_data['Tweet'].tolist()\n","emotion_labels = generated_data.drop(columns=['Tweet'])\n","\n","# Fit vectorizer on this dataset's text only\n","vectorizer = TfidfVectorizer()\n","vec = vectorizer.fit_transform(tweets)\n","\n","# Calculate cosine similarity of the dataset with itself\n","similarity_matrix = cosine_similarity(vec, vec)\n","\n","# Function to calculate typicality for each emotion\n","def calculate_typicality(similarities, labels, target_label):\n","    target_indices = np.where(labels[:, target_label] == True)[0]\n","    non_target_indices = np.where(labels[:, target_label] == False)[0]\n","\n","    sim_target = similarities[:, target_indices]\n","    sim_non_target = similarities[:, non_target_indices]\n","\n","    avg_sim_target = np.mean(sim_target, axis=1)\n","    avg_sim_non_target = np.mean(sim_non_target, axis=1)\n","\n","    typicality = avg_sim_target / avg_sim_non_target\n","    return typicality\n","\n","# Convert labels to numpy array for easier indexing\n","labels = emotion_labels.to_numpy()\n","\n","# Dictionary to store typicality scores for each emotion\n","emotion_typicality_scores = {}\n","\n","# Calculate typicality for each emotion\n","for i, emotion in enumerate(emotion_labels.columns):\n","    typicality_scores = calculate_typicality(similarity_matrix, labels, i)\n","    emotion_typicality_scores[emotion] = np.mean(typicality_scores)\n","\n","# Print typicality scores for each emotion\n","for emotion, score in emotion_typicality_scores.items():\n","    print(f\"Typicality for {emotion} in {dataset_name}: {score:.2f}\")\n","\n","# Print overall typicality score\n","overall_typicality = np.mean(list(emotion_typicality_scores.values()))\n","print(f\"Overall Typicality for {dataset_name}: {overall_typicality:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YuARo8itxAsp","executionInfo":{"status":"ok","timestamp":1723201722760,"user_tz":-120,"elapsed":658,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"f881d811-0e93-4731-f743-f8b559f8c2e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality for anger in role_1: 1.12\n","Typicality for anticipation in role_1: 1.56\n","Typicality for disgust in role_1: 1.01\n","Typicality for fear in role_1: 1.36\n","Typicality for joy in role_1: 1.59\n","Typicality for love in role_1: 1.22\n","Typicality for optimism in role_1: 1.61\n","Typicality for pessimism in role_1: 1.66\n","Typicality for sadness in role_1: 1.69\n","Typicality for surprise in role_1: 1.24\n","Typicality for trust in role_1: 1.44\n","Overall Typicality for role_1: 1.41\n"]}]},{"cell_type":"markdown","source":["Ad"],"metadata":{"id":"pLuXnI-3SA_X"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import pandas as pd\n","import json\n","\n","# List of generated datasets\n","generated_datasets = {\n","    'no_role': 'generated_tweets_no_role.csv',\n","    'no_role_news': 'generated_tweets_no_role_news.csv',\n","    'role_1': 'generated_tweets_role_1.csv',\n","    'role_2': 'generated_tweets_role_2.csv',\n","    'role_3': 'generated_tweets_role_3.csv',\n","    'role_4': 'generated_tweets_role_4.csv',\n","    'role_5': 'generated_tweets_role_5.csv',\n","}\n","\n","# Load all generated datasets and combine text for vectorizer fitting\n","all_texts = []\n","dataset_texts = {}\n","dataset_labels = {}\n","\n","for name, filepath in generated_datasets.items():\n","    generated_data = pd.read_csv(filepath)\n","\n","    # Convert JSON strings to dictionaries if needed\n","    if isinstance(generated_data.iloc[0, 0], str) and generated_data.iloc[0, 0].startswith('{'):\n","        parsed_data = [json.loads(entry) for entry in generated_data.iloc[:, 0]]\n","        generated_data = pd.DataFrame(parsed_data)\n","        generated_data = generated_data.dropna()\n","\n","    tweets = generated_data['Tweet'].tolist()\n","    labels = generated_data.drop(columns=['Tweet'])  # Emotion labels\n","\n","    all_texts.extend(tweets)  # Collect all texts for fitting vectorizer\n","    dataset_texts[name] = tweets  # Store each dataset's texts separately\n","    dataset_labels[name] = labels.to_numpy()  # Store emotion labels separately\n","\n","# Fit vectorizer on combined text\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit(all_texts)\n","\n","# Vectorize each dataset using the common vectorizer\n","vectorized_data = {name: vectorizer.transform(texts) for name, texts in dataset_texts.items()}\n","\n","# Dictionary to store typicality results between datasets\n","typicality_results = {}\n","\n","# Function to calculate typicality for each emotion\n","def calculate_typicality(similarities, labels, target_label):\n","    if target_label >= labels.shape[1]:  # Ensure target_label is within bounds\n","        return np.array([])\n","\n","    target_indices = np.where(labels[:, target_label] == True)[0]\n","    non_target_indices = np.where(labels[:, target_label] == False)[0]\n","\n","    if len(target_indices) == 0 or len(non_target_indices) == 0:\n","        return np.array([])  # Return empty array if no indices for target or non-target\n","\n","    sim_target = similarities[:, target_indices]\n","    sim_non_target = similarities[:, non_target_indices]\n","\n","    avg_sim_target = np.mean(sim_target, axis=1)\n","    avg_sim_non_target = np.mean(sim_non_target, axis=1)\n","\n","    typicality = avg_sim_target / avg_sim_non_target\n","    return typicality\n","\n","# Compare each pair of datasets\n","for name1, vec1 in vectorized_data.items():\n","    for name2, vec2 in vectorized_data.items():\n","        if name1 != name2:\n","            # Align sample sizes\n","            min_samples = min(vec1.shape[0], vec2.shape[0])\n","            vec1_aligned = vec1[:min_samples]\n","            vec2_aligned = vec2[:min_samples]\n","\n","            # Calculate cosine similarity between the two datasets\n","            similarity_matrix = cosine_similarity(vec1_aligned, vec2_aligned)\n","\n","            # Align labels to the smallest number of samples\n","            labels1 = dataset_labels[name1][:min_samples]\n","            labels2 = dataset_labels[name2][:min_samples]\n","\n","            # Calculate typicality scores for each emotion\n","            num_emotions = min(labels1.shape[1], labels2.shape[1])\n","            emotion_typicality_scores = {}\n","            for i in range(num_emotions):  # Iterate over common number of emotions\n","                typicality_scores = calculate_typicality(similarity_matrix, labels1, i)\n","                emotion_typicality_scores[f'{name1}_vs_{name2}_{i}'] = np.mean(typicality_scores) if len(typicality_scores) > 0 else np.nan\n","\n","            # Store the average typicality score for all emotions\n","            valid_scores = [score for score in emotion_typicality_scores.values() if not np.isnan(score)]\n","            overall_typicality = np.mean(valid_scores) if len(valid_scores) > 0 else np.nan\n","            typicality_results[(name1, name2)] = overall_typicality\n","\n","# Print overall typicality scores between each pair of generated datasets\n","for (name1, name2), score in typicality_results.items():\n","    print(f\"Typicality between {name1} and {name2}: {score:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jSyKbLQbz_9y","executionInfo":{"status":"ok","timestamp":1723202154808,"user_tz":-120,"elapsed":4649,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"6860164b-07d7-40e3-80af-139d7e96a06e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality between no_role and no_role_news: 1.05\n","Typicality between no_role and role_1: 1.26\n","Typicality between no_role and role_2: 1.35\n","Typicality between no_role and role_3: 1.36\n","Typicality between no_role and role_4: 1.13\n","Typicality between no_role and role_5: 1.05\n","Typicality between no_role_news and no_role: 1.08\n","Typicality between no_role_news and role_1: 1.06\n","Typicality between no_role_news and role_2: 1.07\n","Typicality between no_role_news and role_3: 1.08\n","Typicality between no_role_news and role_4: 1.05\n","Typicality between no_role_news and role_5: 1.05\n","Typicality between role_1 and no_role: 1.23\n","Typicality between role_1 and no_role_news: 1.04\n","Typicality between role_1 and role_2: 1.35\n","Typicality between role_1 and role_3: 1.35\n","Typicality between role_1 and role_4: 1.15\n","Typicality between role_1 and role_5: 1.05\n","Typicality between role_2 and no_role: 1.25\n","Typicality between role_2 and no_role_news: 1.04\n","Typicality between role_2 and role_1: 1.29\n","Typicality between role_2 and role_3: 1.45\n","Typicality between role_2 and role_4: 1.12\n","Typicality between role_2 and role_5: 1.05\n","Typicality between role_3 and no_role: 1.31\n","Typicality between role_3 and no_role_news: 1.05\n","Typicality between role_3 and role_1: 1.32\n","Typicality between role_3 and role_2: 1.51\n","Typicality between role_3 and role_4: 1.12\n","Typicality between role_3 and role_5: 1.04\n","Typicality between role_4 and no_role: 1.11\n","Typicality between role_4 and no_role_news: 1.03\n","Typicality between role_4 and role_1: 1.15\n","Typicality between role_4 and role_2: 1.14\n","Typicality between role_4 and role_3: 1.13\n","Typicality between role_4 and role_5: 1.04\n","Typicality between role_5 and no_role: 1.05\n","Typicality between role_5 and no_role_news: 1.04\n","Typicality between role_5 and role_1: 1.06\n","Typicality between role_5 and role_2: 1.04\n","Typicality between role_5 and role_3: 1.04\n","Typicality between role_5 and role_4: 1.04\n"]}]},{"cell_type":"markdown","source":["Gold standard data typicality"],"metadata":{"id":"kNM0D063tPxb"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Load the original dataset\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])  # Load training data\n","\n","# Extract tweets and emotion labels\n","tweets = df['Tweet'].tolist()\n","emotion_labels = df.drop(columns=['Tweet', 'ID'])  # Emotion labels only\n","\n","# Vectorize the tweets\n","vectorizer = TfidfVectorizer()\n","vec = vectorizer.fit_transform(tweets)\n","\n","# Calculate the cosine similarity matrix\n","similarity_matrix = cosine_similarity(vec, vec)\n","\n","# Function to calculate typicality for each emotion label\n","def calculate_typicality(similarity_matrix, labels, target_label_index):\n","    # Indices of tweets with and without the target emotion\n","    target_indices = np.where(labels[:, target_label_index] == 1)[0]\n","    non_target_indices = np.where(labels[:, target_label_index] == 0)[0]\n","\n","    if len(target_indices) == 0 or len(non_target_indices) == 0:\n","        # If there are no tweets with or without the emotion, return NaN\n","        return np.nan\n","\n","    # Initialize lists to store average similarities\n","    avg_sim_target = []\n","    avg_sim_non_target = []\n","\n","    # Calculate average similarity for tweets with the target label\n","    for idx in target_indices:\n","        sim_with_target = similarity_matrix[idx, target_indices]  # Similarities with other target tweets\n","        avg_sim_target.append(np.mean(sim_with_target))\n","\n","    # Calculate average similarity for tweets without the target label\n","    for idx in target_indices:\n","        sim_with_non_target = similarity_matrix[idx, non_target_indices]  # Similarities with non-target tweets\n","        avg_sim_non_target.append(np.mean(sim_with_non_target))\n","\n","    # Calculate typicality score as the ratio of average similarities\n","    avg_sim_target_all = np.mean(avg_sim_target)\n","    avg_sim_non_target_all = np.mean(avg_sim_non_target)\n","\n","    # Avoid division by zero\n","    avg_sim_non_target_all += 1e-10  # Small constant to prevent division by zero\n","\n","    typicality = avg_sim_target_all / avg_sim_non_target_all\n","    return typicality\n","\n","# Convert labels to a NumPy array for processing\n","labels = emotion_labels.to_numpy()\n","\n","# Dictionary to store typicality scores for each emotion\n","emotion_typicality_scores = {}\n","\n","# Calculate typicality for each emotion label\n","for i, emotion in enumerate(emotion_labels.columns):\n","    typicality_score = calculate_typicality(similarity_matrix, labels, i)\n","    emotion_typicality_scores[emotion] = typicality_score\n","\n","# Print typicality scores for each emotion\n","for emotion, score in emotion_typicality_scores.items():\n","    print(f\"Typicality for {emotion} in original dataset: {score:.2f}\")\n","\n","# Print overall typicality score\n","overall_typicality = np.mean(list(emotion_typicality_scores.values()))\n","print(f\"Overall Typicality for original dataset: {overall_typicality:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwcq_ZI6tR7i","executionInfo":{"status":"ok","timestamp":1723988614124,"user_tz":-120,"elapsed":8255,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"21bd6272-0b3c-469e-f7f8-38a44e180f14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typicality for anger in original dataset: 1.13\n","Typicality for anticipation in original dataset: 1.13\n","Typicality for disgust in original dataset: 1.11\n","Typicality for fear in original dataset: 1.25\n","Typicality for joy in original dataset: 1.13\n","Typicality for love in original dataset: 1.45\n","Typicality for optimism in original dataset: 1.22\n","Typicality for pessimism in original dataset: 1.25\n","Typicality for sadness in original dataset: 1.17\n","Typicality for surprise in original dataset: 1.37\n","Typicality for trust in original dataset: 1.37\n","Overall Typicality for original dataset: 1.23\n"]}]},{"cell_type":"markdown","source":["## Selecting examples of generated tweets"],"metadata":{"id":"C3jJ8PfTKYS1"}},{"cell_type":"code","source":["def process_generated_data(generated_df):\n","    \"\"\"\n","    Processes the DataFrame containing JSON strings in the first column.\n","\n","    Parameters:\n","    generated_df (pd.DataFrame): DataFrame with JSON strings in the first column.\n","\n","    Returns:\n","    pd.DataFrame: Processed DataFrame with boolean values for label columns.\n","    \"\"\"\n","    # Convert the JSON strings in the first column to dictionaries\n","    parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","    # Create a new DataFrame from the parsed data\n","    generated_data = pd.DataFrame(parsed_data)\n","\n","    # Drop rows with missing values\n","    generated_data = generated_data.dropna()\n","    generated_data = generated_data.drop_duplicates()\n","\n","    # Define the list of label columns\n","    label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love',\n","                     'anger', 'disgust', 'pessimism', 'sadness', 'fear',\n","                     'surprise']\n","\n","    # Convert label columns to boolean values\n","    generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","    return generated_data"],"metadata":{"id":"vUzh3539KbMe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example of loading data and using the function\n","generated_df = pd.read_csv('generated_tweets_role_5.csv')\n","processed_df = process_generated_data(generated_df)"],"metadata":{"id":"8W4sDiQ-LEmL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Getting the multi-label examples"],"metadata":{"id":"CsjhnuWeLmuA"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def get_best_emotion_tweet(dataset, emotions):\n","    \"\"\"\n","    This function takes a multi-label dataset and returns the best tweet for each specified emotion\n","    using cosine similarity, along with the average similarity score and associated labels.\n","\n","    Parameters:\n","    dataset (pd.DataFrame): The loaded DataFrame.\n","    emotions (list of str): List of emotions to find the best tweet for.\n","\n","    Returns:\n","    dict: A dictionary where keys are emotions and values are tuples containing\n","          the best tweet, its average similarity score, and its labels.\n","    \"\"\"\n","    results = {}\n","\n","    for emotion in emotions:\n","        # Initialize list to store tweets for the specified emotion\n","        emotion_tweets = []\n","        labels_list = []\n","\n","        # Iterate through the dataset\n","        for _, row in dataset.iterrows():\n","            tweet = row['Tweet']\n","\n","            # Add tweet to the list if it is labeled with the specified emotion\n","            if row[emotion] == 1:\n","                emotion_tweets.append(tweet)\n","                labels = {label: row[label] for label in emotions if row[label] == 1}\n","                labels_list.append(labels)\n","\n","        if not emotion_tweets:\n","            results[emotion] = (None, None, None)\n","            continue\n","\n","        # Vectorize the tweets using TF-IDF\n","        vectorizer = TfidfVectorizer(stop_words='english')\n","        tweet_vectors = vectorizer.fit_transform(emotion_tweets)\n","\n","        # Calculate cosine similarity matrix\n","        cosine_sim = cosine_similarity(tweet_vectors)\n","\n","        # Average similarity score for each tweet\n","        avg_sim_scores = cosine_sim.mean(axis=1)\n","\n","        # Get the index of the tweet with the highest average similarity\n","        best_idx = np.argmax(avg_sim_scores)\n","        best_tweet = emotion_tweets[best_idx]\n","        best_score = avg_sim_scores[best_idx]\n","        best_labels = labels_list[best_idx]\n","\n","        results[emotion] = (best_tweet, best_score, best_labels)\n","\n","    return results\n","\n","# Define the list of emotions\n","emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n","\n","# Get the best example for each specified emotion\n","best_tweets = get_best_emotion_tweet(processed_df, emotions)\n","\n","# Output the best example with its cosine similarity score and associated labels for each emotion\n","for emotion, (best_tweet, best_score, best_labels) in best_tweets.items():\n","    if best_tweet:\n","        labels_str = ', '.join([f\"{label}: {int(value)}\" for label, value in best_labels.items()])\n","        print(f\"Best example for {emotion}: {best_tweet} (Average Cosine Similarity: {best_score:.4f}) with labels: {labels_str}\")\n","    else:\n","        print(f\"No tweets found for emotion: {emotion}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YeTyCDQ7LRs1","executionInfo":{"status":"ok","timestamp":1723916859249,"user_tz":-120,"elapsed":1408,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"af51e44e-bd38-44d2-9836-9b953e8ac05d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best example for anger: Wow, Russia's media really called it 'the Olympics of Hell' after the Paris ban! Just shocking! 😲😡 (Average Cosine Similarity: 0.0570) with labels: anger: 1, surprise: 1\n","Best example for anticipation: Wow, a dead bear in RFK Jr's campaign? This is just wild! Can't believe the twists we see in politics. 😳 (Average Cosine Similarity: 0.0445) with labels: anticipation: 1, pessimism: 1, surprise: 1\n","Best example for disgust: Hearing about the US funeral home fined $950m for decaying bodies is terrifying. How can people trust our systems? (Average Cosine Similarity: 0.0683) with labels: anger: 1, disgust: 1, fear: 1, pessimism: 1, sadness: 1, surprise: 1\n","Best example for fear: Just heard about that small plane crash on the golf course! How terrifying! 😱 (Average Cosine Similarity: 0.0560) with labels: anticipation: 1, fear: 1, pessimism: 1, surprise: 1\n","Best example for joy: Exciting times in football! Atletico's £81.5m deal for Alvarez shows ambition and hope. Can't wait to see how this unfolds! #optimism (Average Cosine Similarity: 0.0641) with labels: anticipation: 1, joy: 1, optimism: 1, surprise: 1, trust: 1\n","Best example for love: It's heartbreaking to see the impact of Tropical Storm Debby. Let's come together to support each other during this tough time. Trust in our community! (Average Cosine Similarity: 0.0820) with labels: anticipation: 1, fear: 1, love: 1, optimism: 1, sadness: 1, trust: 1\n","Best example for optimism: Excited to see Kamala Harris announce her running mate! This brings hope for a stronger future. Change is coming! #optimism #politics (Average Cosine Similarity: 0.0540) with labels: anticipation: 1, joy: 1, optimism: 1, surprise: 1, trust: 1\n","Best example for pessimism: Wow, is the US really heading for a recession? This is just shocking! 🤯 Can't believe it! (Average Cosine Similarity: 0.0502) with labels: anticipation: 1, fear: 1, pessimism: 1, surprise: 1\n","Best example for sadness: Hearing Flintoff's nightmares from the Top Gear crash is heartbreaking. How could this happen? #sadness (Average Cosine Similarity: 0.0619) with labels: fear: 1, pessimism: 1, sadness: 1\n","Best example for surprise: Wow, a dead bear in RFK Jr's campaign? This is just wild! Can't believe the twists we see in politics. 😳 (Average Cosine Similarity: 0.0614) with labels: anticipation: 1, pessimism: 1, surprise: 1\n","Best example for trust: Excited to see Kamala Harris announce her running mate! This brings hope for a stronger future. Change is coming! #optimism #politics (Average Cosine Similarity: 0.0531) with labels: anticipation: 1, joy: 1, optimism: 1, surprise: 1, trust: 1\n"]}]},{"cell_type":"markdown","source":["For single-label"],"metadata":{"id":"cBpyL2gqLkzm"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def get_best_emotion_tweet(dataset, emotions):\n","    \"\"\"\n","    This function takes a single-label dataset and returns the best tweet for each specified emotion\n","    using cosine similarity, along with the average similarity score.\n","\n","    Parameters:\n","    dataset (pd.DataFrame): The loaded DataFrame.\n","    emotions (list of str): List of emotions to find the best tweet for.\n","\n","    Returns:\n","    dict: A dictionary where keys are emotions and values are tuples containing\n","          the best tweet and its average similarity score.\n","    \"\"\"\n","    results = {}\n","\n","    for emotion in emotions:\n","        # Initialize list to store tweets for the specified emotion\n","        emotion_tweets = []\n","\n","        # Iterate through the dataset\n","        for _, row in dataset.iterrows():\n","            tweet = row['Tweet']\n","\n","            # Add tweet to the list if it is labeled with the specified emotion\n","            if row[emotion] == 1:\n","                emotion_tweets.append(tweet)\n","\n","        if not emotion_tweets:\n","            results[emotion] = (None, None)\n","            continue\n","\n","        # Vectorize the tweets using TF-IDF\n","        vectorizer = TfidfVectorizer(stop_words='english')\n","        tweet_vectors = vectorizer.fit_transform(emotion_tweets)\n","\n","        # Calculate cosine similarity matrix\n","        cosine_sim = cosine_similarity(tweet_vectors)\n","\n","        # Average similarity score for each tweet\n","        avg_sim_scores = cosine_sim.mean(axis=1)\n","\n","        # Get the index of the tweet with the highest average similarity\n","        best_idx = np.argmax(avg_sim_scores)\n","        best_tweet = emotion_tweets[best_idx]\n","        best_score = avg_sim_scores[best_idx]\n","\n","        results[emotion] = (best_tweet, best_score)\n","\n","    return results\n","\n","# Define the list of emotions\n","emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n","\n","# Load and process the data\n","generated_df = pd.read_csv('generated_tweets_single_label_role_1_news.csv')\n","processed_df = process_generated_data(generated_df)\n","\n","# Get the best example for each specified emotion\n","best_tweets = get_best_emotion_tweet(processed_df, emotions)\n","\n","# Output the best example with its cosine similarity score for each emotion\n","for emotion, (best_tweet, best_score) in best_tweets.items():\n","    if best_tweet:\n","        print(f\"Best example for {emotion}: {best_tweet} (Average Cosine Similarity: {best_score:.4f})\")\n","    else:\n","        print(f\"No tweets found for emotion: {emotion}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBjPr20-LmTZ","executionInfo":{"status":"ok","timestamp":1723917895825,"user_tz":-120,"elapsed":1236,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"0146acea-ae81-446b-86f0-d1a9fb596146"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best example for anger: Can you believe the parliament dissolved just like that? This is beyond frustrating! (Average Cosine Similarity: 0.0784)\n","Best example for anticipation: Flintoff's 'nightmares' from the Top Gear crash sound intense. I can't wait to hear more about this! (Average Cosine Similarity: 0.0599)\n","Best example for disgust: Protests show some anger, but honestly, this situation is just frustrating. It feels like a lot of noise for nothing. (Average Cosine Similarity: 0.0556)\n","Best example for fear: This news about the funeral home is terrifying. How could this happen? (Average Cosine Similarity: 0.0744)\n","Best example for joy: Exciting times ahead as President Biden gathers his national security team! Let's hope for a peaceful resolution. #Hope #Peace (Average Cosine Similarity: 0.0552)\n","Best example for love: @economicinsights I know times are tough, but let’s remember to support each other. Love and unity can guide us through! (Average Cosine Similarity: 0.0705)\n","Best example for optimism: Even in troubling times, we can find strength and resilience. Let's unite for a brighter future! #optimism #hope (Average Cosine Similarity: 0.0966)\n","Best example for pessimism: It feels like no matter what we do, things will just get worse. Can we really trust the leaders to handle this? (Average Cosine Similarity: 0.0637)\n","Best example for sadness: Feeling a heavy weight in my heart as tensions rise. #sadness (Average Cosine Similarity: 0.0855)\n","Best example for surprise: Wow, I can't believe the devastation from Tropical Storm Debby! Five lives lost is just shocking. (Average Cosine Similarity: 0.0851)\n","Best example for trust: In times like these, it's crucial to trust the experts and stay safe. Let's support each other through this storm. (Average Cosine Similarity: 0.0794)\n"]}]},{"cell_type":"markdown","source":["## Type-Token Ratio"],"metadata":{"id":"q7HDrChRyyOW"}},{"cell_type":"code","source":["def process_generated_data(generated_df):\n","    \"\"\"\n","    Processes the DataFrame containing JSON strings in the first column.\n","\n","    Parameters:\n","    generated_df (pd.DataFrame): DataFrame with JSON strings in the first column.\n","\n","    Returns:\n","    pd.DataFrame: Processed DataFrame with boolean values for label columns.\n","    \"\"\"\n","    # Convert the JSON strings in the first column to dictionaries\n","    parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","    # Create a new DataFrame from the parsed data\n","    generated_data = pd.DataFrame(parsed_data)\n","\n","    # Drop rows with missing values\n","    generated_data = generated_data.dropna()\n","    generated_data = generated_data.drop_duplicates()\n","\n","    # Define the list of label columns\n","    label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love',\n","                     'anger', 'disgust', 'pessimism', 'sadness', 'fear',\n","                     'surprise']\n","\n","    # Convert label columns to boolean values\n","    generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","    return generated_data"],"metadata":{"id":"aCJbN__Yy2cF","executionInfo":{"status":"ok","timestamp":1723989852414,"user_tz":-120,"elapsed":289,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","\n","# Download the tokenizer models\n","nltk.download('punkt')\n","\n","# Define function to calculate TTR\n","def calculate_overall_ttr(tweets):\n","    all_tokens = []\n","    unique_tokens = set()\n","\n","    # Tokenize each tweet and collect tokens\n","    for tweet in tweets:\n","        tokens = word_tokenize(tweet.lower())\n","        all_tokens.extend(tokens)\n","        unique_tokens.update(tokens)\n","\n","    # Calculate total tokens and unique types\n","    total_tokens = len(all_tokens)\n","    unique_types = len(unique_tokens)\n","\n","    # Calculate and return the overall TTR\n","    return unique_types / total_tokens if total_tokens > 0 else 0\n","\n","# Average length of a tweet\n","def calculate_average_length_nltk(tweets):\n","    lengths = []\n","\n","    for tweet in tweets:\n","      tokens = word_tokenize(tweet)  # Tokenize tweet\n","      length = len(tokens)  # Word count\n","\n","      lengths.append(length)\n","\n","    # Calculate average length\n","    average_length = sum(lengths) / len(lengths)\n","    return average_length\n","\n","# datasets\n","df = pd.DataFrame(sem_eval_2018_task_1['train'])\n","\n","df_no_role = pd.read_csv('generated_tweets_no_role.csv')\n","df_no_role = process_generated_data(df_no_role)\n","\n","df_no_role_news = pd.read_csv('generated_tweets_no_role_news.csv')\n","df_no_role_news = process_generated_data(df_no_role_news)\n","\n","df_role_1 = pd.read_csv('generated_tweets_role_1.csv')\n","df_role_1 = process_generated_data(df_role_1)\n","\n","df_role_2 = pd.read_csv('generated_tweets_role_2.csv')\n","df_role_2 = process_generated_data(df_role_2)\n","\n","df_role_3 = pd.read_csv('generated_tweets_role_3.csv')\n","df_role_3 = process_generated_data(df_role_3)\n","\n","df_role_4 = pd.read_csv('generated_tweets_role_4.csv')\n","df_role_4 = process_generated_data(df_role_4)\n","\n","df_role_5 = pd.read_csv('generated_tweets_role_5.csv')\n","df_role_5 = process_generated_data(df_role_5)\n","\n","df_single_no_role = pd.read_csv('generated_tweets_single_label_no_role.csv')\n","df_single_no_role = process_generated_data(df_single_no_role)\n","\n","df_single_role_1 = pd.read_csv('generated_tweets_single_label_role_1.csv')\n","df_single_role_1 = process_generated_data(df_single_role_1)\n","\n","df_single_role_1_news = pd.read_csv('generated_tweets_single_label_role_1_news.csv')\n","df_single_role_1_news = process_generated_data(df_single_role_1_news)\n","\n","# Apply the function for TTR\n","df_ttr = calculate_overall_ttr(df['Tweet'])\n","df_no_role_ttr = calculate_overall_ttr(df_no_role['Tweet'])\n","df_no_role_news_ttr = calculate_overall_ttr(df_no_role_news['Tweet'])\n","df_role_1_ttr = calculate_overall_ttr(df_role_1['Tweet'])\n","df_role_2_ttr = calculate_overall_ttr(df_role_2['Tweet'])\n","df_role_3_ttr = calculate_overall_ttr(df_role_3['Tweet'])\n","df_role_4_ttr = calculate_overall_ttr(df_role_4['Tweet'])\n","df_role_5_ttr = calculate_overall_ttr(df_role_5['Tweet'])\n","df_single_no_role_ttr = calculate_overall_ttr(df_single_no_role['Tweet'])\n","df_single_role_1_ttr = calculate_overall_ttr(df_single_role_1['Tweet'])\n","df_single_role_1_news_ttr = calculate_overall_ttr(df_single_role_1_news['Tweet'])\n","\n","# Getting the average length of tweets\n","df_average_length = calculate_average_length_nltk(df['Tweet'])\n","df_no_role_average_length = calculate_average_length_nltk(df_no_role['Tweet'])\n","df_no_role_news_average_length = calculate_average_length_nltk(df_no_role_news['Tweet'])\n","df_role_1_average_length = calculate_average_length_nltk(df_role_1['Tweet'])\n","df_role_2_average_length = calculate_average_length_nltk(df_role_2['Tweet'])\n","df_role_3_average_length = calculate_average_length_nltk(df_role_3['Tweet'])\n","df_role_4_average_length = calculate_average_length_nltk(df_role_4['Tweet'])\n","df_role_5_average_length = calculate_average_length_nltk(df_role_5['Tweet'])\n","df_single_no_role_average_length = calculate_average_length_nltk(df_single_no_role['Tweet'])\n","df_single_role_1_average_length = calculate_average_length_nltk(df_single_role_1['Tweet'])\n","df_single_role_1_news_average_length = calculate_average_length_nltk(df_single_role_1_news['Tweet'])\n","\n","# Print the DataFrame with TTR values\n","print(\"Overall TTR\")\n","print(f\"Original: {df_ttr}\")\n","print(f\"No role: {df_no_role_ttr}\")\n","print(f\"No role news: {df_no_role_news_ttr}\")\n","print(f\"Role 1: {df_role_1_ttr}\")\n","print(f\"Role 2: {df_role_2_ttr}\")\n","print(f\"Role 3: {df_role_3_ttr}\")\n","print(f\"Role 4: {df_role_4_ttr}\")\n","print(f\"Role 5: {df_role_5_ttr}\")\n","print(f\"Single label no role: {df_single_no_role_ttr}\")\n","print(f\"Single label role 1: {df_single_role_1_ttr}\")\n","print(f\"Single label role 1 news: {df_single_role_1_news_ttr}\")\n","\n","# Print the average lengths\n","print(\"\\nAverage Lengths\")\n","print(f\"Original: {df_average_length}\")\n","print(f\"No role: {df_no_role_average_length}\")\n","print(f\"No role news: {df_no_role_news_average_length}\")\n","print(f\"Role 1: {df_role_1_average_length}\")\n","print(f\"Role 2: {df_role_2_average_length}\")\n","print(f\"Role 3: {df_role_3_average_length}\")\n","print(f\"Role 4: {df_role_4_average_length}\")\n","print(f\"Role 5: {df_role_5_average_length}\")\n","print(f\"Single label no role: {df_single_no_role_average_length}\")\n","print(f\"Single label role 1: {df_single_role_1_average_length}\")\n","print(f\"Single label role 1 news: {df_single_role_1_news_average_length}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6nMsL5zy0ZN","executionInfo":{"status":"ok","timestamp":1723990718194,"user_tz":-120,"elapsed":14708,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"73be38a2-5e17-4d4d-9841-a2abba111d24"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Overall TTR\n","Original: 0.12422432544503532\n","No role: 0.05734749795702543\n","No role news: 0.06918589529223795\n","Role 1: 0.06678235002478929\n","Role 2: 0.0549834950977977\n","Role 3: 0.05440989099145864\n","Role 4: 0.06536685479733197\n","Role 5: 0.0766762913057427\n","Single label no role: 0.06015611244791361\n","Single label role 1: 0.06689404775002751\n","Single label role 1 news: 0.08661057252606548\n","\n","Average Lengths\n","Original: 20.105586428780345\n","No role: 19.40578358208955\n","No role news: 24.488626023657872\n","Role 1: 18.72794800371402\n","Role 2: 19.094073377234242\n","Role 3: 19.566943674976915\n","Role 4: 18.147113594040967\n","Role 5: 22.940202391904325\n","Single label no role: 17.879328436516264\n","Single label role 1: 17.58027079303675\n","Single label role 1 news: 19.88\n"]}]},{"cell_type":"markdown","source":["## Average number of labels per tweet"],"metadata":{"id":"NKns6va_br2s"}},{"cell_type":"code","source":["def process_generated_data(generated_df):\n","    \"\"\"\n","    Processes the DataFrame containing JSON strings in the first column.\n","\n","    Parameters:\n","    generated_df (pd.DataFrame): DataFrame with JSON strings in the first column.\n","\n","    Returns:\n","    pd.DataFrame: Processed DataFrame with boolean values for label columns.\n","    \"\"\"\n","    # Convert the JSON strings in the first column to dictionaries\n","    parsed_data = [json.loads(entry) for entry in generated_df.iloc[:, 0]]\n","\n","    # Create a new DataFrame from the parsed data\n","    generated_data = pd.DataFrame(parsed_data)\n","\n","    # Drop rows with missing values\n","    generated_data = generated_data.dropna()\n","    generated_data = generated_data.drop_duplicates()\n","\n","    # Define the list of label columns\n","    label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love',\n","                     'anger', 'disgust', 'pessimism', 'sadness', 'fear',\n","                     'surprise']\n","\n","    # Convert label columns to boolean values\n","    generated_data[label_columns] = generated_data[label_columns].astype(bool)\n","\n","    return generated_data"],"metadata":{"id":"LI8lmE90bxBR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def average_labels_per_tweet(df: pd.DataFrame) -> float:\n","    \"\"\"\n","    Calculate the average number of labels per tweet in a multi-label classification dataframe.\n","\n","    Parameters:\n","    df (pd.DataFrame): DataFrame where columns represent labels and values are 0 or 1.\n","\n","    Returns:\n","    float: Average number of labels per tweet.\n","    \"\"\"\n","\n","    label_columns = ['anticipation', 'optimism', 'trust', 'joy', 'love',\n","                     'anger', 'disgust', 'pessimism', 'sadness', 'fear',\n","                     'surprise']\n","\n","    # Count the number of labels per tweet\n","    labels_count_per_tweet = df[label_columns].sum(axis=1)\n","\n","    # Calculate the average number of labels per tweet\n","    average_labels = labels_count_per_tweet.mean()\n","\n","    return average_labels\n","\n","df_no_role = pd.read_csv('generated_tweets_no_role.csv')\n","df_no_role = process_generated_data(df_no_role)\n","\n","df_no_role_news = pd.read_csv('generated_tweets_no_role_news.csv')\n","df_no_role_news = process_generated_data(df_no_role_news)\n","\n","df_role_1 = pd.read_csv('generated_tweets_role_1.csv')\n","df_role_1 = process_generated_data(df_role_1)\n","\n","df_role_2 = pd.read_csv('generated_tweets_role_2.csv')\n","df_role_2 = process_generated_data(df_role_2)\n","\n","df_role_3 = pd.read_csv('generated_tweets_role_3.csv')\n","df_role_3 = process_generated_data(df_role_3)\n","\n","df_role_4 = pd.read_csv('generated_tweets_role_4.csv')\n","df_role_4 = process_generated_data(df_role_4)\n","\n","df_role_5 = pd.read_csv('generated_tweets_role_5.csv')\n","df_role_5 = process_generated_data(df_role_5)\n","\n","df_increased_multi = pd.read_csv('generated_tweets_increased_multi.csv')\n","df_increased_multi = process_generated_data(df_increased_multi)\n","\n","print(f\"No role: {average_labels_per_tweet(df_no_role)}\")\n","print(f\"No role news: {average_labels_per_tweet(df_no_role_news)}\")\n","print(f\"Role 1: {average_labels_per_tweet(df_role_1)}\")\n","print(f\"Role 2: {average_labels_per_tweet(df_role_2)}\")\n","print(f\"Role 3: {average_labels_per_tweet(df_role_3)}\")\n","print(f\"Role 4: {average_labels_per_tweet(df_role_4)}\")\n","print(f\"Role 5: {average_labels_per_tweet(df_role_5)}\")\n","print(f\"Increased multi: {average_labels_per_tweet(df_increased_multi)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BvrkuhqLcLcV","executionInfo":{"status":"ok","timestamp":1723984224938,"user_tz":-120,"elapsed":779,"user":{"displayName":"Marija Kliocaite","userId":"08439538792170122037"}},"outputId":"924ad2dd-6ba1-4fdc-e81b-c5d438fb5d9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["No role: 4.229477611940299\n","No role news: 4.592356687898089\n","Role 1: 4.109563602599814\n","Role 2: 4.333960489181561\n","Role 3: 4.7276084949215145\n","Role 4: 3.211359404096834\n","Role 5: 4.632014719411224\n","Increased multi: 4.231643770724775\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a19c6d0410124f39b814cc68b5cae5da":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e81083bf7b54c88b4173b6be6cdb265","IPY_MODEL_b13d7494b47249148368eed06ffa120e","IPY_MODEL_5a06861debb449adb04fefb5db27b832"],"layout":"IPY_MODEL_3ad244d966124a0d9430551cfb3f8f12"}},"4e81083bf7b54c88b4173b6be6cdb265":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a29433e3b4949f5878f079d15b9d3b4","placeholder":"​","style":"IPY_MODEL_12189ba1754f4dd9bd1a734915e57356","value":"Downloading builder script: 100%"}},"b13d7494b47249148368eed06ffa120e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_edb917fc5f87434c9d5376d687711bd8","max":6288,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c571ace2751540ecb3426e3866eb496b","value":6288}},"5a06861debb449adb04fefb5db27b832":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13ef495b4a444d29a142cc65f7fef627","placeholder":"​","style":"IPY_MODEL_17987d72fce241d0941ab354daf3554c","value":" 6.29k/6.29k [00:00&lt;00:00, 19.7kB/s]"}},"3ad244d966124a0d9430551cfb3f8f12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a29433e3b4949f5878f079d15b9d3b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12189ba1754f4dd9bd1a734915e57356":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"edb917fc5f87434c9d5376d687711bd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c571ace2751540ecb3426e3866eb496b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13ef495b4a444d29a142cc65f7fef627":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17987d72fce241d0941ab354daf3554c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67e487fbd9c34f9da8f2d3edd89e7339":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c9e4bf1195b4015a14d5948c6ac3dfc","IPY_MODEL_62822a41015047828e337993bb85f1a8","IPY_MODEL_c9f9e34143cc45d4970c5f1909294dda"],"layout":"IPY_MODEL_04fc887d4f9d4b5f97923501094aea85"}},"4c9e4bf1195b4015a14d5948c6ac3dfc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fac1380a938c4a268a7e3b869ef547ba","placeholder":"​","style":"IPY_MODEL_0f0b24d9e30d46e785d5b1cb7b9a3044","value":"Downloading readme: 100%"}},"62822a41015047828e337993bb85f1a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8de0788b6ed2415f8ac1434233e940e0","max":10566,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a12e0d1562f04eec8aac36f54e6ccf4e","value":10566}},"c9f9e34143cc45d4970c5f1909294dda":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47ab899bd2de4988a0cf6641668c3b0a","placeholder":"​","style":"IPY_MODEL_5f37e797250243c780032afcd91e60c0","value":" 10.6k/10.6k [00:00&lt;00:00, 29.1kB/s]"}},"04fc887d4f9d4b5f97923501094aea85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fac1380a938c4a268a7e3b869ef547ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f0b24d9e30d46e785d5b1cb7b9a3044":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8de0788b6ed2415f8ac1434233e940e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a12e0d1562f04eec8aac36f54e6ccf4e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"47ab899bd2de4988a0cf6641668c3b0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f37e797250243c780032afcd91e60c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e0bfc1dfb9a4a97bbe6e15876a40760":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c5aee9ea9d73411cb59a036080c80369","IPY_MODEL_79b4a8ce01bf41478fb9ea777a7bb22c","IPY_MODEL_f8a6028bf5744ee98dae498e012ef144"],"layout":"IPY_MODEL_27fcba03c94249cf8d048c0d986f3317"}},"c5aee9ea9d73411cb59a036080c80369":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08b7a56a358c4476a56ef4d5c86f569d","placeholder":"​","style":"IPY_MODEL_db1652dcdc5a4665aa89f698d8fa802e","value":"Downloading data: 100%"}},"79b4a8ce01bf41478fb9ea777a7bb22c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_edbb9956a16748f4aedced996a53b188","max":5975590,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ef2c3f8a5a446a69b12fb1a6c7ef315","value":5975590}},"f8a6028bf5744ee98dae498e012ef144":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2b32eedae1a40fea7de0bf25ab4483e","placeholder":"​","style":"IPY_MODEL_c106d7c45989483ba2c0c1ee8bb1a1d4","value":" 5.98M/5.98M [00:00&lt;00:00, 13.6MB/s]"}},"27fcba03c94249cf8d048c0d986f3317":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08b7a56a358c4476a56ef4d5c86f569d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db1652dcdc5a4665aa89f698d8fa802e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"edbb9956a16748f4aedced996a53b188":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ef2c3f8a5a446a69b12fb1a6c7ef315":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b2b32eedae1a40fea7de0bf25ab4483e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c106d7c45989483ba2c0c1ee8bb1a1d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8c09926ad4c494f95b77c0d66325391":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_159c6e4ff5714c0eb6a824afd1bff3a9","IPY_MODEL_4c63cd6eba8842f2a49538b990f80f08","IPY_MODEL_d5e90431f54f46f192a279218db44eff"],"layout":"IPY_MODEL_1426760556f14e45a1cabaff2090408c"}},"159c6e4ff5714c0eb6a824afd1bff3a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc0b0f5147c7416196372dd13aabecea","placeholder":"​","style":"IPY_MODEL_89220695ebbc4657ae558fa84ca331c6","value":"Generating train split: 100%"}},"4c63cd6eba8842f2a49538b990f80f08":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_600d9102280f4c4e9cd55083675dd17c","max":6838,"min":0,"orientation":"horizontal","style":"IPY_MODEL_908d03e86ca14b40af1f87537114e4cc","value":6838}},"d5e90431f54f46f192a279218db44eff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43a7ac6f07584cd0b19598e8eb5eb45f","placeholder":"​","style":"IPY_MODEL_f02ec2d73fb14ada9764bb5a4ef9826d","value":" 6838/6838 [00:01&lt;00:00, 5157.41 examples/s]"}},"1426760556f14e45a1cabaff2090408c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc0b0f5147c7416196372dd13aabecea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89220695ebbc4657ae558fa84ca331c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"600d9102280f4c4e9cd55083675dd17c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"908d03e86ca14b40af1f87537114e4cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"43a7ac6f07584cd0b19598e8eb5eb45f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f02ec2d73fb14ada9764bb5a4ef9826d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c27824e3985419a895c18043a4913ad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e0df0133b0541358ba439f82a45e02e","IPY_MODEL_1db84918d7484cc08c80e248dece8f70","IPY_MODEL_f66777a85f9d4b5a9a2da6dab10dc9eb"],"layout":"IPY_MODEL_5844e2a0d14b4ef0b4cd8292a0ae74bb"}},"7e0df0133b0541358ba439f82a45e02e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a21ca2cbd9e484fa2003e9a63954b05","placeholder":"​","style":"IPY_MODEL_45c02d2b1a9c44b485dada27dac9ad90","value":"Generating test split: 100%"}},"1db84918d7484cc08c80e248dece8f70":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4814dfee3e0e46f6b77184f363cb8529","max":3259,"min":0,"orientation":"horizontal","style":"IPY_MODEL_70664cb000434bf8a22a701c2dda091e","value":3259}},"f66777a85f9d4b5a9a2da6dab10dc9eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4190f9ac862640818cc35d96283c1d6a","placeholder":"​","style":"IPY_MODEL_ba434ae538e145eaa21509ab93fbf628","value":" 3259/3259 [00:00&lt;00:00, 7020.14 examples/s]"}},"5844e2a0d14b4ef0b4cd8292a0ae74bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a21ca2cbd9e484fa2003e9a63954b05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45c02d2b1a9c44b485dada27dac9ad90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4814dfee3e0e46f6b77184f363cb8529":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70664cb000434bf8a22a701c2dda091e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4190f9ac862640818cc35d96283c1d6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba434ae538e145eaa21509ab93fbf628":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"434090d0f7c74bc48236b568e36fc1ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f309c879112c49a4a087a49306916ec4","IPY_MODEL_0d6aa2d984a24460b0f7e998813eedd7","IPY_MODEL_bb2c1e09cad0447298351e8443de0d2b"],"layout":"IPY_MODEL_5473a7e444ea4fbaa47f39d31be7bf9d"}},"f309c879112c49a4a087a49306916ec4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1eb9aef3283b4f088d81ed4795cd9e0a","placeholder":"​","style":"IPY_MODEL_5fdddbb86cdc4cd6a16883f538f70ecc","value":"Generating validation split: 100%"}},"0d6aa2d984a24460b0f7e998813eedd7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d0e7b2d9766410e92c311d6838d4009","max":886,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e58bd675863b4d3aac1e8d32257c56e4","value":886}},"bb2c1e09cad0447298351e8443de0d2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_115c748cb9d94a1ca7c234b8c4babb72","placeholder":"​","style":"IPY_MODEL_bd5d8c78160e44f09e653d21b6202115","value":" 886/886 [00:00&lt;00:00, 4644.57 examples/s]"}},"5473a7e444ea4fbaa47f39d31be7bf9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1eb9aef3283b4f088d81ed4795cd9e0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fdddbb86cdc4cd6a16883f538f70ecc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d0e7b2d9766410e92c311d6838d4009":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e58bd675863b4d3aac1e8d32257c56e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"115c748cb9d94a1ca7c234b8c4babb72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd5d8c78160e44f09e653d21b6202115":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}